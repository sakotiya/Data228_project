#!/usr/bin/env python3
import os
import sys
import json
import argparse
from io import BytesIO

import boto3
import pandas as pd
import numpy as np
import pyarrow.parquet as pq

# =========================
# Defaults (overridable via CLI/env)
# =========================
DEFAULT_BUCKET = os.getenv("S3_BUCKET", "data228-bigdata-nyc")
DEFAULT_RAW_PREFIX = os.getenv("RAW_PREFIX", "raw/")
DEFAULT_STAGING_PREFIX = os.getenv("STAGING_PREFIX", "staging/")

CLEANING_MODE = os.getenv("CLEANING_MODE", "SMART")
CLEANING_CONFIG = {
    'mode': CLEANING_MODE,
    'drop_completely_empty_columns': True,
    'impute_locations_with_unknown': True,
    'remove_invalid_records': True,
    'max_trip_distance': 100,
    'max_fare_amount': 500,
    'max_passenger_count': 8,
}
session = boto3.Session(profile_name="data228", region_name="us-east-1")
s3_client = session.client("s3")
#s3_client = boto3.client("s3")

# =========================
# Helpers
# =========================
def safe_read_parquet_from_s3(bucket: str, key: str) -> pd.DataFrame:
    """
    Read Parquet from S3; if ArrowInvalid / timestamp overflow occurs,
    fall back to reading as table and casting likely timestamp columns.
    """
    try:
        obj = s3_client.get_object(Bucket=bucket, Key=key)
        return pd.read_parquet(BytesIO(obj['Body'].read()))
    except Exception as e:
        error_type = type(e).__name__
        error_msg = str(e) if e else ""
        is_timestamp_error = ('ArrowInvalid' in error_type
                              or 'out of bounds' in error_msg.lower()
                              or 'timestamp' in error_msg.lower())
        if is_timestamp_error:
            print(f"[warn] Timestamp overflow detected for {key}; using safe mode...")
            obj = s3_client.get_object(Bucket=bucket, Key=key)
            table = pq.read_table(BytesIO(obj['Body'].read()))
            df = table.to_pandas(timestamp_as_object=True)

            # Best-effort parse of likely time columns
            for col in df.columns:
                if df[col].dtype == 'object' and any(x in col.lower() for x in ['time', 'datetime', 'date', 'tstamp']):
                    try:
                        df[col] = pd.to_datetime(df[col], errors='coerce')
                    except Exception:
                        pass
            return df
        else:
            raise

def get_file_type(file_name: str) -> str:
    if file_name.startswith('fhvhv_'): return 'fhvhv'
    if file_name.startswith('fhv_'): return 'fhv'
    if file_name.startswith('green_'): return 'green'
    if file_name.startswith('yellow_'): return 'yellow'
    return 'unknown'

def clean_dataframe_smart(df: pd.DataFrame, file_name: str, file_type: str):
    df_clean = df.copy()
    original_rows = len(df_clean)
    original_cols = len(df_clean.columns)

    cleaning_log = {
        'file_name': file_name,
        'file_type': file_type,
        'cleaning_mode': CLEANING_CONFIG['mode'],
        'original_rows': original_rows,
        'original_cols': original_cols,
        'actions': []
    }

    # Drop 100% empty columns
    if CLEANING_CONFIG['drop_completely_empty_columns']:
        empty_cols = [col for col in df_clean.columns if df_clean[col].isnull().sum() == original_rows]
        if empty_cols:
            df_clean = df_clean.drop(columns=empty_cols)
            cleaning_log['actions'].append({'action': 'DROP_EMPTY_COLUMNS', 'columns': empty_cols})

    # Impute location columns with -1
    if CLEANING_CONFIG['impute_locations_with_unknown']:
        location_cols = [col for col in df_clean.columns if 'location' in col.lower()]
        changed = []
        for col in location_cols:
            if df_clean[col].isnull().sum() > 0:
                df_clean[col] = df_clean[col].fillna(-1)
                changed.append(col)
        if changed:
            cleaning_log['actions'].append({'action': 'IMPUTE_LOCATIONS', 'columns': changed, 'value': -1})

    # passenger_count → median
    if 'passenger_count' in df_clean.columns and df_clean['passenger_count'].isnull().sum() > 0:
        if CLEANING_MODE == "SMART":
            med = df_clean['passenger_count'].median()
            df_clean['passenger_count'] = df_clean['passenger_count'].fillna(med)
            cleaning_log['actions'].append({'action': 'IMPUTE_PASSENGER_COUNT', 'value': float(med)})

    # RatecodeID → mode (or 1)
    if 'RatecodeID' in df_clean.columns and df_clean['RatecodeID'].isnull().sum() > 0:
        mode_val = df_clean['RatecodeID'].mode()
        fill_val = mode_val[0] if len(mode_val) > 0 else 1
        df_clean['RatecodeID'] = df_clean['RatecodeID'].fillna(fill_val)
        cleaning_log['actions'].append({'action': 'IMPUTE_RATECODEID', 'value': fill_val})

    # store_and_fwd_flag → mode (or 'N')
    if 'store_and_fwd_flag' in df_clean.columns and df_clean['store_and_fwd_flag'].isnull().sum() > 0:
        mode_val = df_clean['store_and_fwd_flag'].mode()
        fill_val = mode_val[0] if len(mode_val) > 0 else 'N'
        df_clean['store_and_fwd_flag'] = df_clean['store_and_fwd_flag'].fillna(fill_val)
        cleaning_log['actions'].append({'action': 'IMPUTE_STORE_AND_FWD_FLAG', 'value': fill_val})

    # payment_type → mode (or 1)
    if 'payment_type' in df_clean.columns and df_clean['payment_type'].isnull().sum() > 0:
        mode_val = df_clean['payment_type'].mode()
        fill_val = mode_val[0] if len(mode_val) > 0 else 1
        df_clean['payment_type'] = df_clean['payment_type'].fillna(fill_val)
        cleaning_log['actions'].append({'action': 'IMPUTE_PAYMENT_TYPE', 'value': fill_val})

    # fee/tax/toll → median
    fee_cols = [col for col in df_clean.columns if any(x in col.lower() for x in ['surcharge', 'fee', 'tax', 'toll'])]
    changed = []
    for col in fee_cols:
        if df_clean[col].isnull().sum() > 0 and CLEANING_MODE == "SMART":
            med = df_clean[col].median()
            df_clean[col] = df_clean[col].fillna(med)
            changed.append((col, float(med)))
    if changed:
        cleaning_log['actions'].append({'action': 'IMPUTE_FEES_TAXES_TOLLS', 'columns': changed})

    # trip_type → mode (or 1)
    if 'trip_type' in df_clean.columns and df_clean['trip_type'].isnull().sum() > 0:
        mode_val = df_clean['trip_type'].mode()
        fill_val = mode_val[0] if len(mode_val) > 0 else 1
        df_clean['trip_type'] = df_clean['trip_type'].fillna(fill_val)
        cleaning_log['actions'].append({'action': 'IMPUTE_TRIP_TYPE', 'value': fill_val})

    # Remove invalids
    if CLEANING_CONFIG['remove_invalid_records']:
        rows_before = len(df_clean)
        if 'fare_amount' in df_clean.columns:
            df_clean = df_clean[(df_clean['fare_amount'] >= 0) &
                                (df_clean['fare_amount'] <= CLEANING_CONFIG['max_fare_amount'])]
        if 'trip_distance' in df_clean.columns:
            df_clean = df_clean[(df_clean['trip_distance'] >= 0) &
                                (df_clean['trip_distance'] <= CLEANING_CONFIG['max_trip_distance'])]
        if 'passenger_count' in df_clean.columns:
            df_clean = df_clean[(df_clean['passenger_count'] > 0) &
                                (df_clean['passenger_count'] <= CLEANING_CONFIG['max_passenger_count'])]
        rows_removed = rows_before - len(df_clean)
        if rows_removed > 0:
            cleaning_log['actions'].append({'action': 'DROP_ROWS', 'rows_affected': int(rows_removed)})

    cleaning_log['final_rows'] = len(df_clean)
    cleaning_log['final_cols'] = len(df_clean.columns)
    cleaning_log['retention_rate'] = round(len(df_clean) / original_rows * 100, 2) if original_rows > 0 else 0.0

    return df_clean, cleaning_log

def write_parquet_to_s3(df: pd.DataFrame, bucket: str, key: str):
    buf = BytesIO()
    df.to_parquet(buf, index=False)
    buf.seek(0)
    s3_client.put_object(Bucket=bucket, Key=key, Body=buf.getvalue())

def list_s3_keys(bucket: str, prefix: str, suffix: str = ".parquet", limit: int = None):
    """
    Yield object keys under a prefix, optionally filtered by suffix.
    """
    paginator = s3_client.get_paginator("list_objects_v2")
    count = 0
    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):
        for item in page.get("Contents", []):
            key = item["Key"]
            if suffix and not key.endswith(suffix):
                continue
            yield key
            count += 1
            if limit and count >= limit:
                return

def output_key_for(input_key: str, raw_prefix: str, staging_prefix: str) -> str:
    """
    Mirror raw path to staging path, preserving partition structure if present.
    Example: raw/year=2024/month=01/file.parquet -> staging/year=2024/month=01/file.parquet
    """
    if input_key.startswith(raw_prefix):
        suffix = input_key[len(raw_prefix):]
    else:
        suffix = input_key
    return f"{staging_prefix}{suffix}"

def process_one(bucket: str, key: str, raw_prefix: str, staging_prefix: str) -> dict:
    print(f"[info] Processing s3://{bucket}/{key}")
    file_name = key.split("/")[-1]
    file_type = get_file_type(file_name)

    # Optional: extract year partition if present (for logging)
    year = None
    if "/year=" in key:
        try:
            year = key.split("/year=")[1].split("/")[0]
        except Exception:
            year = None

    # Read
    df_raw = safe_read_parquet_from_s3(bucket, key)
    print(f"[info] Read {len(df_raw):,} rows, {len(df_raw.columns)} columns")

    # Clean
    df_clean, cleaning_log = clean_dataframe_smart(df_raw, file_name, file_type)
    print(f"[info] Cleaned to {len(df_clean):,} rows ({cleaning_log['retention_rate']:.2f}% retained)")

    # Write
    out_key = output_key_for(key, raw_prefix, staging_prefix)
    write_parquet_to_s3(df_clean, bucket, out_key)
    print(f"[ok]   Wrote s3://{bucket}/{out_key}")

    return {
        "input_key": key,
        "output_key": out_key,
        "file_name": file_name,
        "year": year,
        "cleaning_log": cleaning_log
    }

# =========================
# CLI / Main
# =========================
def parse_args():
    ap = argparse.ArgumentParser(description="Clean Parquet files from S3 raw prefix to S3 staging prefix.")
    ap.add_argument("--bucket", default=DEFAULT_BUCKET, help="S3 bucket name (default: env S3_BUCKET or '%(default)s')")
    ap.add_argument("--raw-prefix", default=DEFAULT_RAW_PREFIX, help="Raw prefix (default: env RAW_PREFIX or '%(default)s')")
    ap.add_argument("--staging-prefix", default=DEFAULT_STAGING_PREFIX, help="Staging prefix (default: env STAGING_PREFIX or '%(default)s')")
    ap.add_argument("--single-key", help="Process a single S3 key (overrides listing)")
    ap.add_argument("--suffix", default=".parquet", help="Only process keys with this suffix (default: %(default)s)")
    ap.add_argument("--limit", type=int, help="Limit number of files for a quick test")
    return ap.parse_args()

def main():
    args = parse_args()
    bucket = args.bucket
    raw_prefix = args.raw_prefix
    staging_prefix = args.staging_prefix

    if args.single_key:
        result = process_one(bucket, args.single_key, raw_prefix, staging_prefix)
        print(json.dumps(result, indent=2, default=str))
        return

    processed = 0
    for key in list_s3_keys(bucket, raw_prefix, suffix=args.suffix, limit=args.limit):
        try:
            result = process_one(bucket, key, raw_prefix, staging_prefix)
            processed += 1
        except Exception as e:
            print(f"[error] {key}: {e}")

    print(f"[done] Processed {processed} file(s) from s3://{bucket}/{raw_prefix} -> s3://{bucket}/{staging_prefix}")

if __name__ == "__main__":
    main()
