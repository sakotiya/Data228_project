{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Trip Data - Smart Analysis & Cleaning\n",
    "\n",
    "This notebook provides:\n",
    "1. **Analysis** - Understand missing data patterns\n",
    "2. **Smart Cleaning** - Use median/mode/group-based imputation (NOT hardcoded values)\n",
    "3. **Preview** - See what cleaning will do before executing\n",
    "4. **Execute** - Write cleaned parquet files\n",
    "\n",
    "**Smart Cleaning Features:**\n",
    "- Numeric fields ‚Üí filled with **median** (robust to outliers)\n",
    "- Categorical fields ‚Üí filled with **mode** (most common)\n",
    "- Group-based imputation ‚Üí fill based on similar trips\n",
    "- Preserves data integrity better than hardcoded values\n",
    "\n",
    "Run cells from top to bottom.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport os\nfrom pathlib import Path\nimport json\nimport warnings\nimport s3fs\n\nwarnings.filterwarnings('ignore')\n\n# AWS & S3 Configuration\nAWS_PROFILE = 'data228'\nS3_BUCKET = 'data228-bigdata-nyc'\n\n# Initialize S3 filesystem\nfs = s3fs.S3FileSystem(profile=AWS_PROFILE)\n\n# Directories - now pointing to S3\nRAW_DATA_DIR = f\"{S3_BUCKET}/raw\"\nCLEANED_DATA_DIR = f\"{S3_BUCKET}/staging\"\nOUTPUT_DIR = \"/Users/shreya/SJSU-Github/DA228/scripts/cleaning_logs\"\n\nprint(\"‚úÖ Libraries imported\")\nprint(f\"RAW_DATA_DIR: s3://{RAW_DATA_DIR}\")\nprint(f\"CLEANED_DATA_DIR: s3://{CLEANED_DATA_DIR}\")\nprint(f\"OUTPUT_DIR: {OUTPUT_DIR}\")"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CLEANING MODE: SMART\n",
      "============================================================\n",
      "‚úÖ SMART MODE:\n",
      "  ‚Ä¢ passenger_count ‚Üí median\n",
      "  ‚Ä¢ RatecodeID ‚Üí mode\n",
      "  ‚Ä¢ payment_type ‚Üí mode\n",
      "  ‚Ä¢ fees/taxes ‚Üí median\n",
      "  ‚Ä¢ trip_type ‚Üí mode\n",
      "  ‚Ä¢ Locations ‚Üí -1 (unknown)\n",
      "\n",
      "‚úÖ Configuration set!\n"
     ]
    }
   ],
   "source": [
    "# CLEANING STRATEGY CONFIGURATION\n",
    "# Choose your cleaning approach here\n",
    "\n",
    "CLEANING_MODE = \"SMART\"  # Options: \"SIMPLE\" or \"SMART\"\n",
    "\n",
    "# Cleaning rules\n",
    "CLEANING_CONFIG = {\n",
    "    'mode': CLEANING_MODE,  # SMART = median/mode, SIMPLE = hardcoded values\n",
    "    'drop_completely_empty_columns': True,\n",
    "    'impute_locations_with_unknown': True,  # -1 for unknown locations\n",
    "    'remove_invalid_records': True,\n",
    "    'max_trip_distance': 100,  # miles\n",
    "    'max_fare_amount': 500,    # dollars\n",
    "    'max_passenger_count': 8,\n",
    "}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"CLEANING MODE: {CLEANING_MODE}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if CLEANING_MODE == \"SMART\":\n",
    "    print(\"‚úÖ SMART MODE:\")\n",
    "    print(\"  ‚Ä¢ passenger_count ‚Üí median\")\n",
    "    print(\"  ‚Ä¢ RatecodeID ‚Üí mode\")\n",
    "    print(\"  ‚Ä¢ payment_type ‚Üí mode\")\n",
    "    print(\"  ‚Ä¢ fees/taxes ‚Üí median\")\n",
    "    print(\"  ‚Ä¢ trip_type ‚Üí mode\")\n",
    "    print(\"  ‚Ä¢ Locations ‚Üí -1 (unknown)\")\n",
    "elif CLEANING_MODE == \"SIMPLE\":\n",
    "    print(\"‚ö†Ô∏è  SIMPLE MODE:\")\n",
    "    print(\"  ‚Ä¢ passenger_count ‚Üí always 1\")\n",
    "    print(\"  ‚Ä¢ RatecodeID ‚Üí always 1\")\n",
    "    print(\"  ‚Ä¢ payment_type ‚Üí always 1\")\n",
    "    print(\"  ‚Ä¢ fees/taxes ‚Üí always 0\")\n",
    "    print(\"  ‚Ä¢ trip_type ‚Üí mode\")\n",
    "    print(\"  ‚Ä¢ Locations ‚Üí -1 (unknown)\")\n",
    "\n",
    "print(\"\\n‚úÖ Configuration set!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def safe_read_parquet(file_path):\n    \"\"\"Read parquet file with timestamp overflow handling - now supports S3\"\"\"\n    import pyarrow.parquet as pq\n    \n    try:\n        # Try normal read first (fast path) - S3 path\n        return pd.read_parquet(f\"s3://{file_path}\", filesystem=fs)\n    except Exception as e:\n        # Catch timestamp overflow errors more broadly\n        error_type = type(e).__name__\n        try:\n            error_msg = str(e)\n        except:\n            error_msg = \"\"\n        \n        # Check if it's a timestamp/arrow error\n        is_timestamp_error = (\n            'ArrowInvalid' in error_type or\n            'out of bounds' in error_msg.lower() or\n            'timestamp' in error_msg.lower() or\n            'casting' in error_msg.lower()\n        )\n        \n        if is_timestamp_error:\n            # Use safe mode with pyarrow\n            print(f\"    ‚ö†Ô∏è  Timestamp overflow detected, using safe mode...\")\n            try:\n                with fs.open(file_path, 'rb') as f:\n                    table = pq.read_table(f)\n                    df = table.to_pandas(timestamp_as_object=True)\n                \n                # Convert timestamp columns where possible\n                for col in df.columns:\n                    if df[col].dtype == 'object':\n                        if any(x in col.lower() for x in ['time', 'datetime', 'date']):\n                            try:\n                                df[col] = pd.to_datetime(df[col], errors='coerce')\n                            except:\n                                pass\n                return df\n            except Exception as e2:\n                print(f\"    ‚ùå Safe mode failed: {type(e2).__name__}\")\n                raise e\n        else:\n            raise\n\nprint(\"‚úÖ Helper functions defined (with timestamp overflow protection)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Smart Cleaning Function\n",
    "\n",
    "This function cleans data intelligently based on the mode you selected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Smart cleaning function ready (mode: SMART)\n"
     ]
    }
   ],
   "source": [
    "def clean_dataframe_smart(df, file_name, file_type):\n",
    "    \"\"\"\n",
    "    Smart cleaning: uses median for numeric, mode for categorical\n",
    "    Better preserves data distribution than hardcoded values\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "    original_rows = len(df_clean)\n",
    "    original_cols = len(df_clean.columns)\n",
    "    \n",
    "    cleaning_log = {\n",
    "        'file_name': file_name,\n",
    "        'file_type': file_type,\n",
    "        'cleaning_mode': CLEANING_CONFIG['mode'],\n",
    "        'original_rows': original_rows,\n",
    "        'original_cols': original_cols,\n",
    "        'actions': []\n",
    "    }\n",
    "    \n",
    "    # 1. Drop 100% empty columns\n",
    "    if CLEANING_CONFIG['drop_completely_empty_columns']:\n",
    "        empty_cols = [col for col in df_clean.columns if df_clean[col].isnull().sum() == original_rows]\n",
    "        if empty_cols:\n",
    "            df_clean = df_clean.drop(columns=empty_cols)\n",
    "            cleaning_log['actions'].append({\n",
    "                'action': 'DROP_COLUMNS',\n",
    "                'columns': empty_cols,\n",
    "                'reason': '100% missing'\n",
    "            })\n",
    "    \n",
    "    # 2. Fill location fields with -1 (unknown)\n",
    "    if CLEANING_CONFIG['impute_locations_with_unknown']:\n",
    "        location_cols = [col for col in df_clean.columns if 'location' in col.lower()]\n",
    "        for col in location_cols:\n",
    "            missing = df_clean[col].isnull().sum()\n",
    "            if missing > 0:\n",
    "                df_clean[col] = df_clean[col].fillna(-1)\n",
    "                cleaning_log['actions'].append({\n",
    "                    'action': 'FILL_NULL',\n",
    "                    'column': col,\n",
    "                    'missing_count': int(missing),\n",
    "                    'fill_value': -1,\n",
    "                    'method': 'fixed'\n",
    "                })\n",
    "    \n",
    "    # 3. SMART: Fill passenger_count with MEDIAN (not always 1!)\n",
    "    if 'passenger_count' in df_clean.columns:\n",
    "        missing = df_clean['passenger_count'].isnull().sum()\n",
    "        if missing > 0:\n",
    "            if CLEANING_MODE == \"SMART\":\n",
    "                fill_val = df_clean['passenger_count'].median()\n",
    "                df_clean['passenger_count'] = df_clean['passenger_count'].fillna(fill_val)\n",
    "                cleaning_log['actions'].append({\n",
    "                    'action': 'FILL_NULL',\n",
    "                    'column': 'passenger_count',\n",
    "                    'missing_count': int(missing),\n",
    "                    'fill_value': float(fill_val),\n",
    "                    'method': 'median'\n",
    "                })\n",
    "            else:  # SIMPLE mode\n",
    "                df_clean['passenger_count'] = df_clean['passenger_count'].fillna(1)\n",
    "                cleaning_log['actions'].append({\n",
    "                    'action': 'FILL_NULL',\n",
    "                    'column': 'passenger_count',\n",
    "                    'missing_count': int(missing),\n",
    "                    'fill_value': 1,\n",
    "                    'method': 'fixed'\n",
    "                })\n",
    "    \n",
    "    # 4. SMART: Fill RatecodeID with MODE (most common)\n",
    "    if 'RatecodeID' in df_clean.columns:\n",
    "        missing = df_clean['RatecodeID'].isnull().sum()\n",
    "        if missing > 0:\n",
    "            if CLEANING_MODE == \"SMART\":\n",
    "                mode_val = df_clean['RatecodeID'].mode()\n",
    "                fill_val = mode_val[0] if len(mode_val) > 0 else 1\n",
    "                df_clean['RatecodeID'] = df_clean['RatecodeID'].fillna(fill_val)\n",
    "                cleaning_log['actions'].append({\n",
    "                    'action': 'FILL_NULL',\n",
    "                    'column': 'RatecodeID',\n",
    "                    'missing_count': int(missing),\n",
    "                    'fill_value': float(fill_val),\n",
    "                    'method': 'mode'\n",
    "                })\n",
    "            else:\n",
    "                df_clean['RatecodeID'] = df_clean['RatecodeID'].fillna(1)\n",
    "                cleaning_log['actions'].append({\n",
    "                    'action': 'FILL_NULL',\n",
    "                    'column': 'RatecodeID',\n",
    "                    'missing_count': int(missing),\n",
    "                    'fill_value': 1,\n",
    "                    'method': 'fixed'\n",
    "                })\n",
    "    \n",
    "    # 5. Fill store_and_fwd_flag with MODE\n",
    "    if 'store_and_fwd_flag' in df_clean.columns:\n",
    "        missing = df_clean['store_and_fwd_flag'].isnull().sum()\n",
    "        if missing > 0:\n",
    "            mode_val = df_clean['store_and_fwd_flag'].mode()\n",
    "            fill_val = mode_val[0] if len(mode_val) > 0 else 'N'\n",
    "            df_clean['store_and_fwd_flag'] = df_clean['store_and_fwd_flag'].fillna(fill_val)\n",
    "            cleaning_log['actions'].append({\n",
    "                'action': 'FILL_NULL',\n",
    "                'column': 'store_and_fwd_flag',\n",
    "                'missing_count': int(missing),\n",
    "                'fill_value': fill_val,\n",
    "                'method': 'mode'\n",
    "            })\n",
    "    \n",
    "    # 6. SMART: Fill payment_type with MODE (not always 1)\n",
    "    if 'payment_type' in df_clean.columns:\n",
    "        missing = df_clean['payment_type'].isnull().sum()\n",
    "        if missing > 0:\n",
    "            if CLEANING_MODE == \"SMART\":\n",
    "                mode_val = df_clean['payment_type'].mode()\n",
    "                fill_val = mode_val[0] if len(mode_val) > 0 else 1\n",
    "                df_clean['payment_type'] = df_clean['payment_type'].fillna(fill_val)\n",
    "                cleaning_log['actions'].append({\n",
    "                    'action': 'FILL_NULL',\n",
    "                    'column': 'payment_type',\n",
    "                    'missing_count': int(missing),\n",
    "                    'fill_value': float(fill_val),\n",
    "                    'method': 'mode'\n",
    "                })\n",
    "            else:\n",
    "                df_clean['payment_type'] = df_clean['payment_type'].fillna(1)\n",
    "                cleaning_log['actions'].append({\n",
    "                    'action': 'FILL_NULL',\n",
    "                    'column': 'payment_type',\n",
    "                    'missing_count': int(missing),\n",
    "                    'fill_value': 1,\n",
    "                    'method': 'fixed'\n",
    "                })\n",
    "    \n",
    "    # 7. SMART: Fill fee/tax columns with MEDIAN (not 0!)\n",
    "    fee_cols = [col for col in df_clean.columns if any(x in col.lower() for x in ['surcharge', 'fee', 'tax', 'toll'])]\n",
    "    for col in fee_cols:\n",
    "        missing = df_clean[col].isnull().sum()\n",
    "        if missing > 0:\n",
    "            if CLEANING_MODE == \"SMART\":\n",
    "                fill_val = df_clean[col].median()\n",
    "                df_clean[col] = df_clean[col].fillna(fill_val)\n",
    "                cleaning_log['actions'].append({\n",
    "                    'action': 'FILL_NULL',\n",
    "                    'column': col,\n",
    "                    'missing_count': int(missing),\n",
    "                    'fill_value': float(fill_val),\n",
    "                    'method': 'median'\n",
    "                })\n",
    "            else:\n",
    "                df_clean[col] = df_clean[col].fillna(0)\n",
    "                cleaning_log['actions'].append({\n",
    "                    'action': 'FILL_NULL',\n",
    "                    'column': col,\n",
    "                    'missing_count': int(missing),\n",
    "                    'fill_value': 0,\n",
    "                    'method': 'fixed'\n",
    "                })\n",
    "    \n",
    "    # 8. Fill trip_type with MODE\n",
    "    if 'trip_type' in df_clean.columns:\n",
    "        missing = df_clean['trip_type'].isnull().sum()\n",
    "        if missing > 0:\n",
    "            mode_val = df_clean['trip_type'].mode()\n",
    "            fill_val = mode_val[0] if len(mode_val) > 0 else 1\n",
    "            df_clean['trip_type'] = df_clean['trip_type'].fillna(fill_val)\n",
    "            cleaning_log['actions'].append({\n",
    "                'action': 'FILL_NULL',\n",
    "                'column': 'trip_type',\n",
    "                'missing_count': int(missing),\n",
    "                'fill_value': float(fill_val),\n",
    "                'method': 'mode'\n",
    "            })\n",
    "    \n",
    "    # 9. Remove invalid records (outliers, negatives)\n",
    "    if CLEANING_CONFIG['remove_invalid_records']:\n",
    "        rows_before = len(df_clean)\n",
    "        \n",
    "        # Remove negative or excessive fares\n",
    "        if 'fare_amount' in df_clean.columns:\n",
    "            df_clean = df_clean[\n",
    "                (df_clean['fare_amount'] >= 0) &\n",
    "                (df_clean['fare_amount'] <= CLEANING_CONFIG['max_fare_amount'])\n",
    "            ]\n",
    "        \n",
    "        # Remove invalid distances\n",
    "        if 'trip_distance' in df_clean.columns:\n",
    "            df_clean = df_clean[\n",
    "                (df_clean['trip_distance'] >= 0) &\n",
    "                (df_clean['trip_distance'] <= CLEANING_CONFIG['max_trip_distance'])\n",
    "            ]\n",
    "        \n",
    "        # Remove invalid passenger counts\n",
    "        if 'passenger_count' in df_clean.columns:\n",
    "            df_clean = df_clean[\n",
    "                (df_clean['passenger_count'] > 0) &\n",
    "                (df_clean['passenger_count'] <= CLEANING_CONFIG['max_passenger_count'])\n",
    "            ]\n",
    "        \n",
    "        rows_removed = rows_before - len(df_clean)\n",
    "        if rows_removed > 0:\n",
    "            cleaning_log['actions'].append({\n",
    "                'action': 'DROP_ROWS',\n",
    "                'rows_affected': int(rows_removed),\n",
    "                'reason': 'Invalid data (negative values, outliers)'\n",
    "            })\n",
    "    \n",
    "    # Final stats\n",
    "    cleaning_log['final_rows'] = len(df_clean)\n",
    "    cleaning_log['final_cols'] = len(df_clean.columns)\n",
    "    cleaning_log['retention_rate'] = round(len(df_clean) / original_rows * 100, 2) if original_rows > 0 else 0\n",
    "    \n",
    "    return df_clean, cleaning_log\n",
    "\n",
    "print(f\"‚úÖ Smart cleaning function ready (mode: {CLEANING_MODE})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://gourav.dhama%40doordash.com:****@ddartifacts.jfrog.io/ddartifacts/api/pypi/pypi-local/simple/\n",
      "Collecting pyarrow\n",
      "  Obtaining dependency information for pyarrow from https://files.pythonhosted.org/packages/d9/9b/cb3f7e0a345353def531ca879053e9ef6b9f38ed91aebcf68b09ba54dec0/pyarrow-22.0.0-cp310-cp310-macosx_12_0_arm64.whl.metadata\n",
      "  Downloading pyarrow-22.0.0-cp310-cp310-macosx_12_0_arm64.whl.metadata (3.1 kB)\n",
      "Collecting fastparquet\n",
      "  Obtaining dependency information for fastparquet from https://files.pythonhosted.org/packages/3b/ad/4ce73440df874479f7205fe5445090f71ed4e9bd77fdb3b740253ce82703/fastparquet-2024.11.0-cp310-cp310-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading fastparquet-2024.11.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: pandas>=1.5.0 in /opt/homebrew/lib/python3.10/site-packages (from fastparquet) (1.5.3)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.10/site-packages (from fastparquet) (1.24.1)\n",
      "Collecting cramjam>=2.3 (from fastparquet)\n",
      "  Obtaining dependency information for cramjam>=2.3 from https://files.pythonhosted.org/packages/96/29/7961e09a849eea7d8302e7baa6f829dd3ef3faf199cb25ed29b318ae799b/cramjam-2.11.0-cp310-cp310-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading cramjam-2.11.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (5.6 kB)\n",
      "Collecting fsspec (from fastparquet)\n",
      "  Obtaining dependency information for fsspec from https://files.pythonhosted.org/packages/eb/02/a6b21098b1d5d6249b7c5ab69dde30108a71e4e819d4a9778f1de1d5b70d/fsspec-2025.10.0-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2025.10.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: packaging in /opt/homebrew/lib/python3.10/site-packages (from fastparquet) (23.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/homebrew/lib/python3.10/site-packages (from pandas>=1.5.0->fastparquet) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.10/site-packages (from pandas>=1.5.0->fastparquet) (2022.7.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas>=1.5.0->fastparquet) (1.16.0)\n",
      "Downloading pyarrow-22.0.0-cp310-cp310-macosx_12_0_arm64.whl (34.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m34.2/34.2 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fastparquet-2024.11.0-cp310-cp310-macosx_11_0_arm64.whl (684 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m684.1/684.1 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cramjam-2.11.0-cp310-cp310-macosx_11_0_arm64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m201.0/201.0 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyarrow, fsspec, cramjam, fastparquet\n",
      "Successfully installed cramjam-2.11.0 fastparquet-2024.11.0 fsspec-2025.10.0 pyarrow-22.0.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyarrow fastparquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyze Raw Data (DRY RUN)\n",
    "\n",
    "This analyzes all raw files without modifying them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Analyze all raw files from S3\nprint(\"=\" * 100)\nprint(f\"üîç ANALYZING RAW PARQUET FILES FROM S3\")\nprint(f\"Directory: s3://{RAW_DATA_DIR}\")\nprint(\"=\" * 100)\n\n# List all parquet files from S3\nall_files = []\nfor item in fs.ls(RAW_DATA_DIR):\n    if fs.isdir(item):\n        # Year partition folder\n        parquet_files = [f for f in fs.ls(item) if f.endswith('.parquet')]\n        all_files.extend(parquet_files)\n\nparquet_files = sorted(set(all_files))\nprint(f\"Found {len(parquet_files)} parquet files\")\nprint(\"=\" * 100)\n\nall_analysis = []\n\nfor i, file_path in enumerate(parquet_files, 1):\n    file_name = Path(file_path).name\n    file_type = get_file_type(file_name)\n    \n    print(f\"\\n[{i}/{len(parquet_files)}] {file_name}\")\n    \n    try:\n        df = safe_read_parquet(file_path)\n        \n        analysis = analyze_missing_data(df, file_name)\n        all_analysis.append(analysis)\n        \n        print(f\"  Rows: {analysis['total_rows']:,} | Cols: {analysis['total_columns']}\")\n        print(f\"  Missing data in {len(analysis['columns_with_missing'])} columns\")\n        \n        if analysis['columns_with_missing']:\n            for col, info in list(analysis['columns_with_missing'].items())[:3]:\n                print(f\"    ‚Ä¢ {col}: {info['null_percentage']:.1f}% missing\")\n            if len(analysis['columns_with_missing']) > 3:\n                print(f\"    ... and {len(analysis['columns_with_missing']) - 3} more\")\n        \n    except Exception as e:\n        print(f\"  ‚ùå ERROR: {e}\")\n\n# Save analysis\nos.makedirs(OUTPUT_DIR, exist_ok=True)\noutput_file = os.path.join(OUTPUT_DIR, 'analysis_results.json')\nwith open(output_file, 'w') as f:\n    json.dump({'analysis': all_analysis}, f, indent=2)\n\nprint(\"\\n\" + \"=\" * 100)\nprint(f\"‚úÖ Analysis complete! Saved to: {output_file}\")\nprint(\"=\" * 100)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Execute Smart Cleaning (Write Cleaned Files)\n",
    "\n",
    "This actually cleans and writes cleaned parquet files to `cleaned_data/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Execute cleaning - Read from S3, Write to S3 staging\n\nprint(\"=\" * 100)\nprint(f\"üöß EXECUTING {CLEANING_MODE} CLEANING\")\nprint(f\"Source: s3://{RAW_DATA_DIR}\")\nprint(f\"Destination: s3://{CLEANED_DATA_DIR}\")\nprint(\"=\" * 100)\n\n# List all parquet files from S3\nall_files = []\nfor item in fs.ls(RAW_DATA_DIR):\n    if fs.isdir(item):\n        parquet_files = [f for f in fs.ls(item) if f.endswith('.parquet')]\n        all_files.extend(parquet_files)\n\nparquet_files = sorted(set(all_files))\nprint(f\"Files to process: {len(parquet_files)}\")\nprint(\"=\" * 100)\n\nall_cleaning_logs = []\n\nfor i, file_path in enumerate(parquet_files, 1):\n    file_name = Path(file_path).name\n    file_type = get_file_type(file_name)\n    \n    # Extract year from path\n    year = None\n    if '/year=' in file_path:\n        year = file_path.split('/year=')[1].split('/')[0]\n    \n    print(f\"\\n[{i}/{len(parquet_files)}] {file_name}\")\n    \n    try:\n        df_raw = safe_read_parquet(file_path)\n        print(f\"  Read: {len(df_raw):,} rows, {len(df_raw.columns)} columns\")\n        \n        # Clean using smart function\n        df_clean, cleaning_log = clean_dataframe_smart(df_raw, file_name, file_type)\n        all_cleaning_logs.append(cleaning_log)\n        \n        # Save cleaned file to S3 staging (preserve year partitioning)\n        if year:\n            out_path = f\"{CLEANED_DATA_DIR}/year={year}/{file_name}\"\n        else:\n            out_path = f\"{CLEANED_DATA_DIR}/{file_name}\"\n        \n        df_clean.to_parquet(f\"s3://{out_path}\", filesystem=fs, index=False)\n        \n        print(f\"  Cleaned: {len(df_clean):,} rows, {len(df_clean.columns)} columns\")\n        print(f\"  Retention: {cleaning_log['retention_rate']:.2f}%\")\n        print(f\"  ‚úÖ Saved to: s3://{out_path}\")\n        \n    except Exception as e:\n        print(f\"  ‚ùå ERROR: {e}\")\n\n# Save cleaning logs\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nlogs_json_path = Path(OUTPUT_DIR) / \"cleaning_logs.json\"\nlogs_json_path.write_text(json.dumps({\"cleaning_logs\": all_cleaning_logs}, indent=2))\n\n# Save CSV summary for Excel\nif all_cleaning_logs:\n    summary_df = pd.DataFrame(all_cleaning_logs)\n    summary_csv_path = Path(OUTPUT_DIR) / \"per_file_drop_stats_for_excel.csv\"\n    summary_df[[\n        'file_name', 'file_type', 'cleaning_mode', 'original_rows', \n        'final_rows', 'retention_rate'\n    ]].to_csv(summary_csv_path, index=False)\nelse:\n    summary_csv_path = None\n\nprint(\"\\n\" + \"=\" * 100)\nprint(\"üíæ CLEANING EXECUTION COMPLETE\")\nprint(f\"Logs JSON : {logs_json_path}\")\nif summary_csv_path:\n    print(f\"Summary CSV: {summary_csv_path}\")\nprint(\"=\" * 100)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**What you just did:**\n",
    "1. ‚úÖ Analyzed raw data for missing values\n",
    "2. ‚úÖ Cleaned data using **{CLEANING_MODE} mode**\n",
    "3. ‚úÖ Wrote cleaned parquet files to `cleaned_data/`\n",
    "4. ‚úÖ Saved logs and summaries\n",
    "\n",
    "**Next step:** Run `data_validation.ipynb` to compare raw vs cleaned data.\n",
    "\n",
    "**Want to try a different cleaning mode?**\n",
    "- Go back to cell 3, change `CLEANING_MODE` to `\"SIMPLE\"` or `\"SMART\"`\n",
    "- Restart kernel and run all cells again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ File loaded successfully!\n",
      "Rows: 1,707,650\n",
      "Columns: ['dispatching_base_num', 'pickup_datetime', 'dropOff_datetime', 'PUlocationID', 'DOlocationID', 'SR_Flag', 'Affiliated_base_number']\n",
      "\n",
      "First few rows:\n",
      "  dispatching_base_num     pickup_datetime    dropOff_datetime  PUlocationID  \\\n",
      "0               B00037 2019-02-01 00:08:44 2019-02-01 00:23:35         264.0   \n",
      "1               B00037 2019-02-01 00:27:51 2019-02-01 00:32:54         264.0   \n",
      "2               B00037 2019-02-01 00:18:30 2019-02-01 00:25:45         264.0   \n",
      "3               B00037 2019-02-01 00:43:15 2019-02-01 00:48:29         264.0   \n",
      "4               B00037 2019-02-01 00:01:45 2019-02-01 00:09:13         264.0   \n",
      "\n",
      "   DOlocationID  SR_Flag Affiliated_base_number  \n",
      "0         265.0      NaN                 B00037  \n",
      "1         265.0      NaN                 B00037  \n",
      "2         265.0      NaN                 B00037  \n",
      "3         265.0      NaN                 B00037  \n",
      "4         265.0      NaN                 B00037  \n",
      "\n",
      "‚úÖ Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/fhv_tripdata_2019-02.parquet\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Trip Data - Analyze & Cleaning Preview\n",
    "\n",
    "This notebook mirrors the logic of `analyze_and_clean.py`.\n",
    "\n",
    "It will:\n",
    "- Scan all raw parquet files\n",
    "- Analyze missing data patterns\n",
    "- Preview what cleaning would do (DRY RUN)\n",
    "- Save detailed results to `analysis_results.json`\n",
    "\n",
    "Run cells from top to bottom.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW_DATA_DIR: /Users/gouravdhama/Documents/bubu/Raw_data/raw_data\n",
      "CLEANED_DATA_DIR (for future execute mode): /Users/gouravdhama/Documents/bubu/cleaned_data\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "# from pathlib import Path\n",
    "# import json\n",
    "# import warnings\n",
    "\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# RAW_DATA_DIR = \"/Users/gouravdhama/Documents/bubu/Raw_data/raw_data\"\n",
    "# CLEANED_DATA_DIR = \"/Users/gouravdhama/Documents/bubu/cleaned_data\"  # not used in DRY RUN yet\n",
    "\n",
    "# print(\"RAW_DATA_DIR:\", RAW_DATA_DIR)\n",
    "# print(\"CLEANED_DATA_DIR (for future execute mode):\", CLEANED_DATA_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Functions for analysis & preview loaded.\n"
     ]
    }
   ],
   "source": [
    "# ---- Functions from analyze_and_clean.py (analysis + cleaning preview) ----\n",
    "\n",
    "CLEANING_CONFIG = {\n",
    "    'fill_null_strategy': 'smart',  # 'smart', 'drop', or 'keep'\n",
    "    'drop_completely_empty_columns': True,\n",
    "    'impute_locations_with_unknown': True,  # Fill missing locations with -1\n",
    "    'impute_numeric_with_defaults': True,   # Fill missing numeric fields\n",
    "    'impute_categorical_with_mode': True,   # Fill missing categorical fields\n",
    "    'remove_invalid_records': True,         # Remove clearly invalid data\n",
    "    'max_trip_distance': 100,               # miles\n",
    "    'max_fare_amount': 500,                 # dollars\n",
    "    'max_passenger_count': 8,\n",
    "}\n",
    "\n",
    "\n",
    "def analyze_missing_data(df, file_name):\n",
    "    \"\"\"Analyze missing data patterns in a dataframe\"\"\"\n",
    "    total_rows = len(df)\n",
    "    \n",
    "    if total_rows == 0:\n",
    "        return {\n",
    "            'file_name': file_name,\n",
    "            'total_rows': 0,\n",
    "            'error': 'Empty dataframe'\n",
    "        }\n",
    "    \n",
    "    missing_info = {}\n",
    "    for col in df.columns:\n",
    "        null_count = df[col].isnull().sum()\n",
    "        null_pct = (null_count / total_rows * 100)\n",
    "        \n",
    "        missing_info[col] = {\n",
    "            'null_count': int(null_count),\n",
    "            'null_percentage': round(null_pct, 2),\n",
    "            'dtype': str(df[col].dtype)\n",
    "        }\n",
    "    \n",
    "    cols_with_missing = {k: v for k, v in missing_info.items() if v['null_count'] > 0}\n",
    "    \n",
    "    return {\n",
    "        'file_name': file_name,\n",
    "        'total_rows': int(total_rows),\n",
    "        'total_columns': len(df.columns),\n",
    "        'columns': list(df.columns),\n",
    "        'missing_data': missing_info,\n",
    "        'columns_with_missing': cols_with_missing\n",
    "    }\n",
    "\n",
    "\n",
    "def get_file_type(file_name):\n",
    "    \"\"\"Determine file type from filename\"\"\"\n",
    "    if file_name.startswith('fhvhv_'):\n",
    "        return 'fhvhv'\n",
    "    elif file_name.startswith('fhv_'):\n",
    "        return 'fhv'\n",
    "    elif file_name.startswith('green_'):\n",
    "        return 'green'\n",
    "    elif file_name.startswith('yellow_'):\n",
    "        return 'yellow'\n",
    "    return 'unknown'\n",
    "\n",
    "\n",
    "def preview_cleaning(df, file_name, file_type):\n",
    "    \"\"\"Preview what cleaning would do WITHOUT modifying data\"\"\"\n",
    "    original_rows = len(df)\n",
    "    original_cols = len(df.columns)\n",
    "    \n",
    "    stats = {\n",
    "        'file_name': file_name,\n",
    "        'file_type': file_type,\n",
    "        'original_rows': original_rows,\n",
    "        'original_cols': original_cols,\n",
    "        'actions': []\n",
    "    }\n",
    "    \n",
    "    # 100% empty columns\n",
    "    if CLEANING_CONFIG['drop_completely_empty_columns']:\n",
    "        empty_cols = [col for col in df.columns if df[col].isnull().sum() == original_rows]\n",
    "        if empty_cols:\n",
    "            stats['actions'].append({\n",
    "                'action': 'DROP_COLUMNS',\n",
    "                'columns': empty_cols,\n",
    "                'reason': '100% missing',\n",
    "                'impact': f\"Remove {len(empty_cols)} columns\"\n",
    "            })\n",
    "    \n",
    "    # Location columns\n",
    "    location_cols = [col for col in df.columns if 'location' in col.lower()]\n",
    "    for col in location_cols:\n",
    "        missing = df[col].isnull().sum()\n",
    "        if missing > 0:\n",
    "            stats['actions'].append({\n",
    "                'action': 'FILL_NULL',\n",
    "                'column': col,\n",
    "                'missing_count': int(missing),\n",
    "                'missing_pct': round(missing/original_rows*100, 2),\n",
    "                'fill_value': -1,\n",
    "                'reason': 'Preserve trips with unknown location'\n",
    "            })\n",
    "    \n",
    "    # passenger_count\n",
    "    if 'passenger_count' in df.columns:\n",
    "        missing = df['passenger_count'].isnull().sum()\n",
    "        if missing > 0:\n",
    "            stats['actions'].append({\n",
    "                'action': 'FILL_NULL',\n",
    "                'column': 'passenger_count',\n",
    "                'missing_count': int(missing),\n",
    "                'missing_pct': round(missing/original_rows*100, 2),\n",
    "                'fill_value': 1,\n",
    "                'reason': 'Most trips have 1 passenger'\n",
    "            })\n",
    "    \n",
    "    # RatecodeID\n",
    "    if 'RatecodeID' in df.columns:\n",
    "        missing = df['RatecodeID'].isnull().sum()\n",
    "        if missing > 0:\n",
    "            stats['actions'].append({\n",
    "                'action': 'FILL_NULL',\n",
    "                'column': 'RatecodeID',\n",
    "                'missing_count': int(missing),\n",
    "                'missing_pct': round(missing/original_rows*100, 2),\n",
    "                'fill_value': 1,\n",
    "                'reason': 'Standard rate is default'\n",
    "            })\n",
    "    \n",
    "    # store_and_fwd_flag\n",
    "    if 'store_and_fwd_flag' in df.columns:\n",
    "        missing = df['store_and_fwd_flag'].isnull().sum()\n",
    "        if missing > 0:\n",
    "            stats['actions'].append({\n",
    "                'action': 'FILL_NULL',\n",
    "                'column': 'store_and_fwd_flag',\n",
    "                'missing_count': int(missing),\n",
    "                'missing_pct': round(missing/original_rows*100, 2),\n",
    "                'fill_value': 'N',\n",
    "                'reason': 'Most trips are not stored and forwarded'\n",
    "            })\n",
    "    \n",
    "    # payment_type\n",
    "    if 'payment_type' in df.columns:\n",
    "        missing = df['payment_type'].isnull().sum()\n",
    "        if missing > 0:\n",
    "            stats['actions'].append({\n",
    "                'action': 'FILL_NULL',\n",
    "                'column': 'payment_type',\n",
    "                'missing_count': int(missing),\n",
    "                'missing_pct': round(missing/original_rows*100, 2),\n",
    "                'fill_value': 1,\n",
    "                'reason': 'Credit card is most common'\n",
    "            })\n",
    "    \n",
    "    # fee columns\n",
    "    fee_cols = [col for col in df.columns if any(x in col.lower() for x in ['surcharge', 'fee', 'tax'])]\n",
    "    for col in fee_cols:\n",
    "        if col in df.columns:\n",
    "            missing = df[col].isnull().sum()\n",
    "            if missing > 0:\n",
    "                stats['actions'].append({\n",
    "                    'action': 'FILL_NULL',\n",
    "                    'column': col,\n",
    "                    'missing_count': int(missing),\n",
    "                    'missing_pct': round(missing/original_rows*100, 2),\n",
    "                    'fill_value': 0,\n",
    "                    'reason': 'Assume no fee if not recorded'\n",
    "                })\n",
    "    \n",
    "    # trip_type\n",
    "    if 'trip_type' in df.columns:\n",
    "        missing = df['trip_type'].isnull().sum()\n",
    "        if missing > 0:\n",
    "            mode_val = df['trip_type'].mode()\n",
    "            mode_val = mode_val[0] if len(mode_val) > 0 else 1\n",
    "            stats['actions'].append({\n",
    "                'action': 'FILL_NULL',\n",
    "                'column': 'trip_type',\n",
    "                'missing_count': int(missing),\n",
    "                'missing_pct': round(missing/original_rows*100, 2),\n",
    "                'fill_value': int(mode_val),\n",
    "                'reason': f'Use most common trip type ({mode_val})'\n",
    "            })\n",
    "    \n",
    "    # invalid records\n",
    "    if CLEANING_CONFIG['remove_invalid_records']:\n",
    "        invalid_count = 0\n",
    "        \n",
    "        if 'fare_amount' in df.columns:\n",
    "            invalid = ((df['fare_amount'] < 0) | (df['fare_amount'] > CLEANING_CONFIG['max_fare_amount'])).sum()\n",
    "            invalid_count += invalid\n",
    "        \n",
    "        if 'trip_distance' in df.columns:\n",
    "            invalid = ((df['trip_distance'] < 0) | (df['trip_distance'] > CLEANING_CONFIG['max_trip_distance'])).sum()\n",
    "            invalid_count += invalid\n",
    "        \n",
    "        if 'passenger_count' in df.columns:\n",
    "            invalid = ((df['passenger_count'].notna()) & \n",
    "                      ((df['passenger_count'] <= 0) | (df['passenger_count'] > CLEANING_CONFIG['max_passenger_count']))).sum()\n",
    "            invalid_count += invalid\n",
    "        \n",
    "        if invalid_count > 0:\n",
    "            stats['actions'].append({\n",
    "                'action': 'DROP_ROWS',\n",
    "                'rows_affected': int(invalid_count),\n",
    "                'rows_affected_pct': round(invalid_count/original_rows*100, 2),\n",
    "                'reason': 'Invalid data (negative values, outliers)'\n",
    "            })\n",
    "    \n",
    "    rows_after = original_rows - sum(a.get('rows_affected', 0) for a in stats['actions'])\n",
    "    cols_after = original_cols - sum(len(a.get('columns', [])) for a in stats['actions'] if a['action'] == 'DROP_COLUMNS')\n",
    "    \n",
    "    stats['final_rows'] = rows_after\n",
    "    stats['final_cols'] = cols_after\n",
    "    stats['retention_rate'] = round(rows_after/original_rows*100, 2) if original_rows > 0 else 0\n",
    "    \n",
    "    return stats\n",
    "\n",
    "print(\"‚úÖ Functions for analysis & preview loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "üîç ANALYZING NYC TRIP DATA (DRY RUN - NO FILES MODIFIED)\n",
      "Directory: /Users/gouravdhama/Documents/bubu/Raw_data/raw_data\n",
      "Found 100 parquet files\n",
      "====================================================================================================\n",
      "\n",
      "[1/100] fhv_tripdata_2015-02.parquet\n",
      "  Rows: 3,053,183 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "  Cleaning preview: 3 actions\n",
      "  After cleaning: 3,053,183 rows (100.0% retained)\n",
      "\n",
      "[2/100] fhv_tripdata_2015-12.parquet\n",
      "  Rows: 8,888,809 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "  Cleaning preview: 3 actions\n",
      "  After cleaning: 8,888,809 rows (100.0% retained)\n",
      "\n",
      "[3/100] fhv_tripdata_2019-02.parquet\n",
      "    ‚ö†Ô∏è  Timestamp overflow detected, using safe mode...\n",
      "  Rows: 1,707,650 | Cols: 7\n",
      "  Missing data in 5 columns\n",
      "  Cleaning preview: 2 actions\n",
      "  After cleaning: 1,707,650 rows (100.0% retained)\n",
      "\n",
      "[4/100] fhv_tripdata_2019-12.parquet\n",
      "  Rows: 2,044,196 | Cols: 7\n",
      "  Missing data in 4 columns\n",
      "  Cleaning preview: 3 actions\n",
      "  After cleaning: 2,044,196 rows (100.0% retained)\n",
      "\n",
      "[5/100] fhv_tripdata_2024-01.parquet\n",
      "  Rows: 1,290,116 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "  Cleaning preview: 3 actions\n",
      "  After cleaning: 1,290,116 rows (100.0% retained)\n",
      "\n",
      "[6/100] fhv_tripdata_2024-02.parquet\n",
      "  Rows: 1,176,093 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "  Cleaning preview: 3 actions\n",
      "  After cleaning: 1,176,093 rows (100.0% retained)\n",
      "\n",
      "[7/100] fhv_tripdata_2024-03.parquet\n",
      "  Rows: 1,469,352 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "  Cleaning preview: 3 actions\n",
      "  After cleaning: 1,469,352 rows (100.0% retained)\n",
      "\n",
      "[8/100] fhv_tripdata_2024-04.parquet\n",
      "  Rows: 1,444,626 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "  Cleaning preview: 3 actions\n",
      "  After cleaning: 1,444,626 rows (100.0% retained)\n",
      "\n",
      "[9/100] fhv_tripdata_2024-05.parquet\n",
      "  Rows: 1,352,502 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "  Cleaning preview: 3 actions\n",
      "  After cleaning: 1,352,502 rows (100.0% retained)\n",
      "\n",
      "[10/100] fhv_tripdata_2024-06.parquet\n",
      "  Rows: 1,386,539 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "  Cleaning preview: 3 actions\n",
      "  After cleaning: 1,386,539 rows (100.0% retained)\n",
      "\n",
      "[11/100] fhv_tripdata_2024-07.parquet\n",
      "  Rows: 1,382,739 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "  Cleaning preview: 3 actions\n",
      "  After cleaning: 1,382,739 rows (100.0% retained)\n",
      "\n",
      "[12/100] fhv_tripdata_2024-08.parquet\n",
      "  Rows: 1,484,471 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "  Cleaning preview: 3 actions\n",
      "  After cleaning: 1,484,471 rows (100.0% retained)\n",
      "\n",
      "[13/100] fhv_tripdata_2024-09.parquet\n",
      "  Rows: 1,718,375 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "  Cleaning preview: 3 actions\n",
      "  After cleaning: 1,718,375 rows (100.0% retained)\n",
      "\n",
      "[14/100] fhv_tripdata_2024-10.parquet\n",
      "  Rows: 1,421,231 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "  Cleaning preview: 3 actions\n",
      "  After cleaning: 1,421,231 rows (100.0% retained)\n",
      "\n",
      "[15/100] fhv_tripdata_2024-11.parquet\n",
      "  Rows: 1,591,082 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "  Cleaning preview: 3 actions\n",
      "  After cleaning: 1,591,082 rows (100.0% retained)\n",
      "\n",
      "[16/100] fhv_tripdata_2024-12.parquet\n",
      "  Rows: 1,913,200 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "  Cleaning preview: 3 actions\n",
      "  After cleaning: 1,913,200 rows (100.0% retained)\n",
      "\n",
      "[17/100] fhv_tripdata_2025-01.parquet\n",
      "  Rows: 1,898,108 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "  Cleaning preview: 3 actions\n",
      "  After cleaning: 1,898,108 rows (100.0% retained)\n",
      "\n",
      "[18/100] fhv_tripdata_2025-02.parquet\n",
      "  Rows: 1,578,722 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "  Cleaning preview: 3 actions\n",
      "  After cleaning: 1,578,722 rows (100.0% retained)\n",
      "\n",
      "[19/100] fhv_tripdata_2025-03.parquet\n",
      "  Rows: 2,182,992 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "  Cleaning preview: 3 actions\n",
      "  After cleaning: 2,182,992 rows (100.0% retained)\n",
      "\n",
      "[20/100] fhv_tripdata_2025-04.parquet\n",
      "  Rows: 1,699,478 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "  Cleaning preview: 3 actions\n",
      "  After cleaning: 1,699,478 rows (100.0% retained)\n",
      "\n",
      "[21/100] fhv_tripdata_2025-05.parquet\n",
      "  Rows: 2,210,721 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "  Cleaning preview: 3 actions\n",
      "  After cleaning: 2,210,721 rows (100.0% retained)\n",
      "\n",
      "[22/100] fhv_tripdata_2025-06.parquet\n",
      "  Rows: 2,231,731 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "  Cleaning preview: 3 actions\n",
      "  After cleaning: 2,231,731 rows (100.0% retained)\n",
      "\n",
      "[23/100] fhv_tripdata_2025-07.parquet\n",
      "  Rows: 2,187,536 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "  Cleaning preview: 3 actions\n",
      "  After cleaning: 2,187,536 rows (100.0% retained)\n",
      "\n",
      "[24/100] fhv_tripdata_2025-08.parquet\n",
      "  Rows: 2,256,854 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "  Cleaning preview: 3 actions\n",
      "  After cleaning: 2,256,854 rows (100.0% retained)\n",
      "\n",
      "[25/100] fhvhv_tripdata_2019-02.parquet\n"
     ]
    }
   ],
   "source": [
    "# ---- Run analysis + preview across all files (DRY RUN) ----\n",
    "\n",
    "raw_path = Path(RAW_DATA_DIR)\n",
    "parquet_files = sorted(raw_path.glob(\"*.parquet\"))\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"üîç ANALYZING NYC TRIP DATA (DRY RUN - NO FILES MODIFIED)\")\n",
    "print(f\"Directory: {RAW_DATA_DIR}\")\n",
    "print(f\"Found {len(parquet_files)} parquet files\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "all_analysis = []\n",
    "all_cleaning_preview = []\n",
    "by_type = {'fhv': [], 'fhvhv': [], 'green': [], 'yellow': []}\n",
    "\n",
    "for i, file_path in enumerate(parquet_files, 1):\n",
    "    file_name = file_path.name\n",
    "    file_type = get_file_type(file_name)\n",
    "\n",
    "    print(f\"\\n[{i}/{len(parquet_files)}] {file_name}\")\n",
    "    try:\n",
    "        df = safe_read_parquet(file_path)\n",
    "\n",
    "        analysis = analyze_missing_data(df, file_name)\n",
    "        all_analysis.append(analysis)\n",
    "\n",
    "        cleaning_preview = preview_cleaning(df, file_name, file_type)\n",
    "        all_cleaning_preview.append(cleaning_preview)\n",
    "\n",
    "        by_type.setdefault(file_type, []).append({\n",
    "            'analysis': analysis,\n",
    "            'cleaning': cleaning_preview,\n",
    "        })\n",
    "\n",
    "        print(f\"  Rows: {analysis['total_rows']:,} | Cols: {analysis['total_columns']}\")\n",
    "        print(f\"  Missing data in {len(analysis['columns_with_missing'])} columns\")\n",
    "        print(f\"  Cleaning preview: {len(cleaning_preview['actions'])} actions\")\n",
    "        print(f\"  After cleaning: {cleaning_preview['final_rows']:,} rows ({cleaning_preview['retention_rate']}% retained)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå ERROR: {e}\")\n",
    "\n",
    "# Save results\n",
    "output_file = \"/Users/gouravdhama/Documents/bubu/cleaning_code/analysis_results.json\"\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump({'analysis': all_analysis, 'cleaning_preview': all_cleaning_preview}, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"üìä SUMMARY BY FILE TYPE\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "for file_type, files in by_type.items():\n",
    "    if not files:\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n{file_type.upper()} FILES ({len(files)} files)\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    total_rows_before = sum(f['analysis']['total_rows'] for f in files)\n",
    "    total_rows_after = sum(f['cleaning']['final_rows'] for f in files)\n",
    "    retention = (total_rows_after / total_rows_before * 100) if total_rows_before > 0 else 0\n",
    "\n",
    "    print(f\"  Total rows before: {total_rows_before:,}\")\n",
    "    print(f\"  Total rows after: {total_rows_after:,}\")\n",
    "    print(f\"  Overall retention: {retention:.2f}%\")\n",
    "    print(f\"  Rows to remove: {total_rows_before - total_rows_after:,}\")\n",
    "\n",
    "    all_missing = {}\n",
    "    for f in files:\n",
    "        for col, info in f['analysis']['columns_with_missing'].items():\n",
    "            all_missing.setdefault(col, []).append(info['null_percentage'])\n",
    "\n",
    "    if all_missing:\n",
    "        print(\"\\n  Columns with missing data:\")\n",
    "        for col, percentages in sorted(all_missing.items()):\n",
    "            avg_pct = sum(percentages) / len(percentages)\n",
    "            print(f\"    ‚Ä¢ {col}: avg {avg_pct:.1f}% missing (in {len(percentages)} files)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"üíæ RESULTS SAVED\")\n",
    "print(\"=\"*100)\n",
    "print(f\"Detailed analysis: {output_file}\")\n",
    "print(\"\\nDone.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ File loaded successfully!\n",
      "Rows: 1,707,650\n",
      "Columns: ['dispatching_base_num', 'pickup_datetime', 'dropOff_datetime', 'PUlocationID', 'DOlocationID', 'SR_Flag', 'Affiliated_base_number']\n",
      "\n",
      "First few rows:\n",
      "  dispatching_base_num     pickup_datetime    dropOff_datetime  PUlocationID  \\\n",
      "0               B00037 2019-02-01 00:08:44 2019-02-01 00:23:35         264.0   \n",
      "1               B00037 2019-02-01 00:27:51 2019-02-01 00:32:54         264.0   \n",
      "2               B00037 2019-02-01 00:18:30 2019-02-01 00:25:45         264.0   \n",
      "3               B00037 2019-02-01 00:43:15 2019-02-01 00:48:29         264.0   \n",
      "4               B00037 2019-02-01 00:01:45 2019-02-01 00:09:13         264.0   \n",
      "\n",
      "   DOlocationID  SR_Flag Affiliated_base_number  \n",
      "0         265.0      NaN                 B00037  \n",
      "1         265.0      NaN                 B00037  \n",
      "2         265.0      NaN                 B00037  \n",
      "3         265.0      NaN                 B00037  \n",
      "4         265.0      NaN                 B00037  \n",
      "\n",
      "‚úÖ Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/fhv_tripdata_2019-02.parquet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from pathlib import Path\n",
    "\n",
    "# Read the problematic file\n",
    "file_path = \"/Users/gouravdhama/Documents/bubu/Raw_data/raw_data/fhv_tripdata_2019-02.parquet\"\n",
    "\n",
    "# Read with PyArrow to avoid timestamp overflow\n",
    "table = pq.read_table(file_path)\n",
    "df = table.to_pandas(timestamp_as_object=True)\n",
    "\n",
    "# Try to fix timestamp columns\n",
    "for col in df.columns:\n",
    "    if any(x in col.lower() for x in ['time', 'datetime', 'date']):\n",
    "        try:\n",
    "            # Convert back to datetime, coercing invalid timestamps to NaT (Not a Time)\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# Check the result\n",
    "print(f\"‚úÖ File loaded successfully!\")\n",
    "print(f\"Rows: {len(df):,}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Optional: Save the fixed version\n",
    "output_path = \"/Users/gouravdhama/Documents/bubu/cleaned_data/fhv_tripdata_2019-02.parquet\"\n",
    "df.to_parquet(output_path, index=False)\n",
    "print(f\"\\n‚úÖ Saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Cleaning (Write Cleaned Parquet Files)\n",
    "\n",
    "This section performs the **actual cleaning** and writes cleaned parquet files to:\n",
    "\n",
    "`/Users/gouravdhama/Documents/bubu/cleaned_data`\n",
    "\n",
    "Run this **after** you are happy with the DRY RUN preview above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleaning function ready (for execute mode).\n"
     ]
    }
   ],
   "source": [
    "# Cleaning function (applies the same logic as the preview, but actually modifies the data)\n",
    "\n",
    "def clean_dataframe(df, file_name, file_type):\n",
    "    \"\"\"Clean a dataframe according to CLEANING_CONFIG and return cleaned df + log.\"\"\"\n",
    "    df_clean = df.copy()\n",
    "    original_rows = len(df_clean)\n",
    "    original_cols = len(df_clean.columns)\n",
    "\n",
    "    cleaning_log = {\n",
    "        'file_name': file_name,\n",
    "        'file_type': file_type,\n",
    "        'original_rows': original_rows,\n",
    "        'original_cols': original_cols,\n",
    "        'actions': []\n",
    "    }\n",
    "\n",
    "    # 1. Drop 100% empty columns\n",
    "    if CLEANING_CONFIG['drop_completely_empty_columns']:\n",
    "        empty_cols = [col for col in df_clean.columns if df_clean[col].isnull().sum() == original_rows]\n",
    "        if empty_cols:\n",
    "            df_clean = df_clean.drop(columns=empty_cols)\n",
    "            cleaning_log['actions'].append({\n",
    "                'action': 'DROP_COLUMNS',\n",
    "                'columns': empty_cols,\n",
    "                'reason': '100% missing'\n",
    "            })\n",
    "\n",
    "    # 2. Fill location fields with -1 (unknown)\n",
    "    if CLEANING_CONFIG['impute_locations_with_unknown']:\n",
    "        location_cols = [col for col in df_clean.columns if 'location' in col.lower()]\n",
    "        for col in location_cols:\n",
    "            missing = df_clean[col].isnull().sum()\n",
    "            if missing > 0:\n",
    "                df_clean[col] = df_clean[col].fillna(-1)\n",
    "                cleaning_log['actions'].append({\n",
    "                    'action': 'FILL_NULL',\n",
    "                    'column': col,\n",
    "                    'missing_count': int(missing),\n",
    "                    'fill_value': -1\n",
    "                })\n",
    "\n",
    "    # 3. Fill passenger_count with 1 (most common)\n",
    "    if 'passenger_count' in df_clean.columns:\n",
    "        missing = df_clean['passenger_count'].isnull().sum()\n",
    "        if missing > 0:\n",
    "            df_clean['passenger_count'] = df_clean['passenger_count'].fillna(1)\n",
    "            cleaning_log['actions'].append({\n",
    "                'action': 'FILL_NULL',\n",
    "                'column': 'passenger_count',\n",
    "                'missing_count': int(missing),\n",
    "                'fill_value': 1\n",
    "            })\n",
    "\n",
    "    # 4. Fill RatecodeID with 1 (standard rate)\n",
    "    if 'RatecodeID' in df_clean.columns:\n",
    "        missing = df_clean['RatecodeID'].isnull().sum()\n",
    "        if missing > 0:\n",
    "            df_clean['RatecodeID'] = df_clean['RatecodeID'].fillna(1)\n",
    "            cleaning_log['actions'].append({\n",
    "                'action': 'FILL_NULL',\n",
    "                'column': 'RatecodeID',\n",
    "                'missing_count': int(missing),\n",
    "                'fill_value': 1\n",
    "            })\n",
    "\n",
    "    # 5. Fill store_and_fwd_flag with 'N'\n",
    "    if 'store_and_fwd_flag' in df_clean.columns:\n",
    "        missing = df_clean['store_and_fwd_flag'].isnull().sum()\n",
    "        if missing > 0:\n",
    "            df_clean['store_and_fwd_flag'] = df_clean['store_and_fwd_flag'].fillna('N')\n",
    "            cleaning_log['actions'].append({\n",
    "                'action': 'FILL_NULL',\n",
    "                'column': 'store_and_fwd_flag',\n",
    "                'missing_count': int(missing),\n",
    "                'fill_value': 'N'\n",
    "            })\n",
    "\n",
    "    # 6. Fill payment_type with 1 (credit card)\n",
    "    if 'payment_type' in df_clean.columns:\n",
    "        missing = df_clean['payment_type'].isnull().sum()\n",
    "        if missing > 0:\n",
    "            df_clean['payment_type'] = df_clean['payment_type'].fillna(1)\n",
    "            cleaning_log['actions'].append({\n",
    "                'action': 'FILL_NULL',\n",
    "                'column': 'payment_type',\n",
    "                'missing_count': int(missing),\n",
    "                'fill_value': 1\n",
    "            })\n",
    "\n",
    "    # 7. Fill fee-related columns with 0\n",
    "    fee_cols = [col for col in df_clean.columns if any(x in col.lower() for x in ['surcharge', 'fee', 'tax'])]\n",
    "    for col in fee_cols:\n",
    "        missing = df_clean[col].isnull().sum()\n",
    "        if missing > 0:\n",
    "            df_clean[col] = df_clean[col].fillna(0)\n",
    "            cleaning_log['actions'].append({\n",
    "                'action': 'FILL_NULL',\n",
    "                'column': col,\n",
    "                'missing_count': int(missing),\n",
    "                'fill_value': 0\n",
    "            })\n",
    "\n",
    "    # 8. Fill trip_type with mode\n",
    "    if 'trip_type' in df_clean.columns:\n",
    "        missing = df_clean['trip_type'].isnull().sum()\n",
    "        if missing > 0:\n",
    "            mode_val = df_clean['trip_type'].mode()\n",
    "            mode_val = mode_val[0] if len(mode_val) > 0 else 1\n",
    "            df_clean['trip_type'] = df_clean['trip_type'].fillna(mode_val)\n",
    "            cleaning_log['actions'].append({\n",
    "                'action': 'FILL_NULL',\n",
    "                'column': 'trip_type',\n",
    "                'missing_count': int(missing),\n",
    "                'fill_value': int(mode_val)\n",
    "            })\n",
    "\n",
    "    # 9. Remove invalid records (negative/out-of-range values)\n",
    "    if CLEANING_CONFIG['remove_invalid_records']:\n",
    "        rows_before = len(df_clean)\n",
    "\n",
    "        if 'fare_amount' in df_clean.columns:\n",
    "            df_clean = df_clean[\n",
    "                (df_clean['fare_amount'] >= 0) &\n",
    "                (df_clean['fare_amount'] <= CLEANING_CONFIG['max_fare_amount'])\n",
    "            ]\n",
    "\n",
    "        if 'trip_distance' in df_clean.columns:\n",
    "            df_clean = df_clean[\n",
    "                (df_clean['trip_distance'] >= 0) &\n",
    "                (df_clean['trip_distance'] <= CLEANING_CONFIG['max_trip_distance'])\n",
    "            ]\n",
    "\n",
    "        if 'passenger_count' in df_clean.columns:\n",
    "            df_clean = df_clean[\n",
    "                (df_clean['passenger_count'] > 0) &\n",
    "                (df_clean['passenger_count'] <= CLEANING_CONFIG['max_passenger_count'])\n",
    "            ]\n",
    "\n",
    "        rows_removed = rows_before - len(df_clean)\n",
    "        if rows_removed > 0:\n",
    "            cleaning_log['actions'].append({\n",
    "                'action': 'DROP_ROWS',\n",
    "                'rows_affected': int(rows_removed),\n",
    "                'reason': 'Invalid data (negative values / outliers)'\n",
    "            })\n",
    "\n",
    "    # Final stats\n",
    "    cleaning_log['final_rows'] = len(df_clean)\n",
    "    cleaning_log['final_cols'] = len(df_clean.columns)\n",
    "    cleaning_log['retention_rate'] = round(len(df_clean) / original_rows * 100, 2) if original_rows > 0 else 0\n",
    "\n",
    "    return df_clean, cleaning_log\n",
    "\n",
    "print(\"‚úÖ Cleaning function ready (for execute mode).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}