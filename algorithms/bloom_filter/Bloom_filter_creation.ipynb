{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a06117bf-f5d1-44cd-b18d-058629330c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully connected to S3!\n",
      "   Bucket: data228-bigdata-nyc\n",
      "   Profile: data228\n",
      "\n",
      "üìÅ Found 8 parquet files in staging/\n",
      "Sample files:\n",
      "   - staging/2025/fhv_tripdata_2025-03.parquet\n",
      "   - staging/2025/fhv_tripdata_2025-04.parquet\n",
      "   - staging/2025/fhvhv_tripdata_2025-01.parquet\n",
      "   - staging/2025/fhvhv_tripdata_2025-03.parquet\n",
      "   - staging/2025/green_tripdata_2025-05.parquet\n"
     ]
    }
   ],
   "source": [
    "# S3 Configuration Cell - Add this to your Jupyter notebook\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "# Install required packages if needed (uncomment if needed)\n",
    "# !pip install boto3 s3fs pyarrow\n",
    "\n",
    "# S3 Configuration\n",
    "AWS_PROFILE = \"data228\"\n",
    "BUCKET_NAME = \"data228-bigdata-nyc\"\n",
    "# STAGING_PREFIX = \"staging/\"\n",
    "STAGING_PREFIX = \"staging/2025/\"\n",
    "\n",
    "# Create boto3 session with profile\n",
    "session = boto3.Session(profile_name=AWS_PROFILE)\n",
    "s3_client = session.client('s3')\n",
    "s3_resource = session.resource('s3')\n",
    "\n",
    "# Test connection\n",
    "try:\n",
    "    response = s3_client.head_bucket(Bucket=BUCKET_NAME)\n",
    "    print(f\"‚úÖ Successfully connected to S3!\")\n",
    "    print(f\"   Bucket: {BUCKET_NAME}\")\n",
    "    print(f\"   Profile: {AWS_PROFILE}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "# List files in staging\n",
    "bucket = s3_resource.Bucket(BUCKET_NAME)\n",
    "parquet_files = [\n",
    "    obj.key \n",
    "    for obj in bucket.objects.filter(Prefix=STAGING_PREFIX)\n",
    "    if obj.key.endswith(\".parquet\")\n",
    "]\n",
    "\n",
    "print(f\"\\nüìÅ Found {len(parquet_files)} parquet files in staging/\")\n",
    "print(\"Sample files:\")\n",
    "for f in parquet_files[:5]:\n",
    "    print(f\"   - {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "593d5d75-a617-43e6-8902-d3bad35a933e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Reading file: s3://data228-bigdata-nyc/staging/2025/green_tripdata_2025-06.parquet\n",
      "\n",
      "üìã Columns and Data Types:\n",
      "VendorID                          int32\n",
      "lpep_pickup_datetime     datetime64[us]\n",
      "lpep_dropoff_datetime    datetime64[us]\n",
      "store_and_fwd_flag               object\n",
      "RatecodeID                      float64\n",
      "PULocationID                      int32\n",
      "DOLocationID                      int32\n",
      "passenger_count                 float64\n",
      "trip_distance                   float64\n",
      "fare_amount                     float64\n",
      "extra                           float64\n",
      "mta_tax                         float64\n",
      "tip_amount                      float64\n",
      "tolls_amount                    float64\n",
      "improvement_surcharge           float64\n",
      "total_amount                    float64\n",
      "payment_type                    float64\n",
      "trip_type                       float64\n",
      "congestion_surcharge            float64\n",
      "cbd_congestion_fee              float64\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>lpep_pickup_datetime</th>\n",
       "      <th>lpep_dropoff_datetime</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>trip_type</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>cbd_congestion_fee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2025-06-01 00:33:43</td>\n",
       "      <td>2025-06-01 01:04:33</td>\n",
       "      <td>N</td>\n",
       "      <td>2.0</td>\n",
       "      <td>74</td>\n",
       "      <td>132</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.60</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>19.61</td>\n",
       "      <td>6.94</td>\n",
       "      <td>1.0</td>\n",
       "      <td>98.05</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2025-06-01 00:07:45</td>\n",
       "      <td>2025-06-01 00:14:52</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>75</td>\n",
       "      <td>74</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.37</td>\n",
       "      <td>9.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.80</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2025-06-01 00:24:07</td>\n",
       "      <td>2025-06-01 00:48:24</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>83</td>\n",
       "      <td>83</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.11</td>\n",
       "      <td>25.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.90</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2025-06-01 00:00:14</td>\n",
       "      <td>2025-06-01 00:08:29</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>97</td>\n",
       "      <td>49</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.29</td>\n",
       "      <td>9.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.36</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2025-06-01 00:31:15</td>\n",
       "      <td>2025-06-01 00:43:35</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>66</td>\n",
       "      <td>25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.97</td>\n",
       "      <td>13.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VendorID lpep_pickup_datetime lpep_dropoff_datetime store_and_fwd_flag  \\\n",
       "0         2  2025-06-01 00:33:43   2025-06-01 01:04:33                  N   \n",
       "1         2  2025-06-01 00:07:45   2025-06-01 00:14:52                  N   \n",
       "2         2  2025-06-01 00:24:07   2025-06-01 00:48:24                  N   \n",
       "3         2  2025-06-01 00:00:14   2025-06-01 00:08:29                  N   \n",
       "4         2  2025-06-01 00:31:15   2025-06-01 00:43:35                  N   \n",
       "\n",
       "   RatecodeID  PULocationID  DOLocationID  passenger_count  trip_distance  \\\n",
       "0         2.0            74           132              1.0          19.60   \n",
       "1         1.0            75            74              2.0           1.37   \n",
       "2         1.0            83            83              1.0           4.11   \n",
       "3         1.0            97            49              1.0           1.29   \n",
       "4         1.0            66            25              1.0           1.97   \n",
       "\n",
       "   fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
       "0         70.0    0.0      0.5       19.61          6.94   \n",
       "1          9.3    1.0      0.5        0.00          0.00   \n",
       "2         25.4    1.0      0.5        0.00          0.00   \n",
       "3          9.3    1.0      0.5        2.36          0.00   \n",
       "4         13.5    1.0      0.5        0.00          0.00   \n",
       "\n",
       "   improvement_surcharge  total_amount  payment_type  trip_type  \\\n",
       "0                    1.0         98.05           1.0        1.0   \n",
       "1                    1.0         11.80           2.0        1.0   \n",
       "2                    1.0         27.90           2.0        1.0   \n",
       "3                    1.0         14.16           1.0        1.0   \n",
       "4                    1.0         16.00           1.0        1.0   \n",
       "\n",
       "   congestion_surcharge  cbd_congestion_fee  \n",
       "0                   0.0                 0.0  \n",
       "1                   0.0                 0.0  \n",
       "2                   0.0                 0.0  \n",
       "3                   0.0                 0.0  \n",
       "4                   0.0                 0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inspect schema for 2025_06 Green Taxi dataset\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "target_file = \"staging/2025/green_tripdata_2025-06.parquet\"\n",
    "s3_path = f\"s3://{BUCKET_NAME}/{target_file}\"\n",
    "print(f\"\\nüîç Reading file: {s3_path}\")\n",
    "\n",
    "try:\n",
    "    # Read Parquet file directly from S3\n",
    "    df = pd.read_parquet(s3_path, storage_options={\"profile\": AWS_PROFILE})\n",
    "\n",
    "    # Show schema (columns and types)\n",
    "    print(\"\\nüìã Columns and Data Types:\")\n",
    "    print(df.dtypes)\n",
    "\n",
    "    # Optional: preview first few rows\n",
    "    display(df.head())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error reading file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ef7010-86d8-497f-a558-c4f92ff134c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Creating Bloom Filter for all NYC Taxi data (excluding /2025/)\n",
      "Bucket: s3://data228-bigdata-nyc/staging/\n",
      "Exclude: s3://data228-bigdata-nyc/staging/2025/\n",
      "Output Bloom Filter: s3://data228-bigdata-nyc/bloom_filter/bloom_all_years.pkl\n",
      "\n",
      "‚úÖ Initialized Bloom Filter (capacity=250,000,000, FPR=1.00%)\n",
      "\n",
      "üìÇ Listing all Parquet files from S3...\n",
      "‚úÖ Found 91 parquet files (excluded 8 from /2025/)\n",
      "\n",
      "üì¶ [1/91] Processing fhv_tripdata_2015-02.parquet ...\n",
      "   ‚Üí Added 2,000,000 rows (Total: 2,000,000)\n",
      "   ‚Üí Added 1,053,183 rows (Total: 3,053,183)\n",
      "üì¶ [2/91] Processing fhv_tripdata_2015-12.parquet ...\n",
      "   ‚Üí Added 2,000,000 rows (Total: 5,053,183)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 7,053,183)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 9,053,183)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 11,053,183)\n",
      "   ‚Üí Added 888,809 rows (Total: 11,941,992)\n",
      "üì¶ [3/91] Processing fhv_tripdata_2019-02.parquet ...\n",
      "   ‚Üí Added 1,707,650 rows (Total: 13,649,642)\n",
      "üì¶ [4/91] Processing fhv_tripdata_2019-12.parquet ...\n",
      "   ‚Üí Added 2,000,000 rows (Total: 15,649,642)\n",
      "   ‚Üí Added 44,196 rows (Total: 15,693,838)\n",
      "üì¶ [5/91] Processing fhv_tripdata_2024-01.parquet ...\n",
      "   ‚Üí Added 1,290,116 rows (Total: 16,983,954)\n",
      "üì¶ [6/91] Processing fhv_tripdata_2024-02.parquet ...\n",
      "   ‚Üí Added 1,176,093 rows (Total: 18,160,047)\n",
      "üì¶ [7/91] Processing fhv_tripdata_2024-03.parquet ...\n",
      "   ‚Üí Added 1,469,352 rows (Total: 19,629,399)\n",
      "üì¶ [8/91] Processing fhv_tripdata_2024-04.parquet ...\n",
      "   ‚Üí Added 1,444,626 rows (Total: 21,074,025)\n",
      "üì¶ [9/91] Processing fhv_tripdata_2024-05.parquet ...\n",
      "   ‚Üí Added 1,352,502 rows (Total: 22,426,527)\n",
      "üì¶ [10/91] Processing fhv_tripdata_2024-06.parquet ...\n",
      "   ‚Üí Added 1,386,539 rows (Total: 23,813,066)\n",
      "üì¶ [11/91] Processing fhv_tripdata_2024-07.parquet ...\n",
      "   ‚Üí Added 1,382,739 rows (Total: 25,195,805)\n",
      "üì¶ [12/91] Processing fhv_tripdata_2024-08.parquet ...\n",
      "   ‚Üí Added 1,484,471 rows (Total: 26,680,276)\n",
      "üì¶ [13/91] Processing fhv_tripdata_2024-09.parquet ...\n",
      "   ‚Üí Added 1,718,375 rows (Total: 28,398,651)\n",
      "üì¶ [14/91] Processing fhv_tripdata_2024-10.parquet ...\n",
      "   ‚Üí Added 1,421,231 rows (Total: 29,819,882)\n",
      "üì¶ [15/91] Processing fhv_tripdata_2024-11.parquet ...\n",
      "   ‚Üí Added 1,591,082 rows (Total: 31,410,964)\n",
      "üì¶ [16/91] Processing fhv_tripdata_2024-12.parquet ...\n",
      "   ‚Üí Added 1,913,200 rows (Total: 33,324,164)\n",
      "üì¶ [17/91] Processing fhv_tripdata_2025-01.parquet ...\n",
      "   ‚Üí Added 1,898,108 rows (Total: 35,222,272)\n",
      "üì¶ [18/91] Processing fhv_tripdata_2025-02.parquet ...\n",
      "   ‚Üí Added 1,578,722 rows (Total: 36,800,994)\n",
      "üì¶ [19/91] Processing fhv_tripdata_2025-05.parquet ...\n",
      "   ‚Üí Added 2,000,000 rows (Total: 38,800,994)\n",
      "   ‚Üí Added 210,721 rows (Total: 39,011,715)\n",
      "üì¶ [20/91] Processing fhv_tripdata_2025-06.parquet ...\n",
      "   ‚Üí Added 2,000,000 rows (Total: 41,011,715)\n",
      "   ‚Üí Added 231,731 rows (Total: 41,243,446)\n",
      "üì¶ [21/91] Processing fhv_tripdata_2025-07.parquet ...\n",
      "   ‚Üí Added 2,000,000 rows (Total: 43,243,446)\n",
      "   ‚Üí Added 187,536 rows (Total: 43,430,982)\n",
      "üì¶ [22/91] Processing fhv_tripdata_2025-08.parquet ...\n",
      "   ‚Üí Added 2,000,000 rows (Total: 45,430,982)\n",
      "   ‚Üí Added 256,854 rows (Total: 45,687,836)\n",
      "üì¶ [23/91] Processing fhvhv_tripdata_2019-12.parquet ...\n",
      "   ‚Üí Added 2,000,000 rows (Total: 47,687,836)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 49,687,836)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 51,687,836)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 53,687,836)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 55,687,836)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 57,687,836)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 59,687,836)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 61,687,836)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 63,687,836)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 65,687,836)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 67,687,836)\n",
      "   ‚Üí Added 243,901 rows (Total: 67,931,737)\n",
      "üì¶ [24/91] Processing fhvhv_tripdata_2024-01.parquet ...\n",
      "   ‚Üí Added 2,000,000 rows (Total: 69,931,737)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 71,931,737)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 73,931,737)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 75,931,737)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 77,931,737)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 79,931,737)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 81,931,737)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 83,931,737)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 85,931,737)\n",
      "   ‚Üí Added 1,663,930 rows (Total: 87,595,667)\n",
      "üì¶ [25/91] Processing fhvhv_tripdata_2024-02.parquet ...\n",
      "   ‚Üí Added 2,000,000 rows (Total: 89,595,667)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 91,595,667)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 93,595,667)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 95,595,667)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 97,595,667)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 99,595,667)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 101,595,667)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 103,595,667)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 105,595,667)\n",
      "   ‚Üí Added 1,359,148 rows (Total: 106,954,815)\n",
      "üì¶ [26/91] Processing fhvhv_tripdata_2024-03.parquet ...\n",
      "   ‚Üí Added 2,000,000 rows (Total: 108,954,815)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 110,954,815)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 112,954,815)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 114,954,815)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 116,954,815)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 118,954,815)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 120,954,815)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 122,954,815)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 124,954,815)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 126,954,815)\n",
      "   ‚Üí Added 1,280,788 rows (Total: 128,235,603)\n",
      "üì¶ [27/91] Processing fhvhv_tripdata_2024-04.parquet ...\n",
      "   ‚Üí Added 2,000,000 rows (Total: 130,235,603)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 132,235,603)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 134,235,603)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 136,235,603)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 138,235,603)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 140,235,603)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 142,235,603)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 144,235,603)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 146,235,603)\n",
      "   ‚Üí Added 1,733,038 rows (Total: 147,968,641)\n",
      "üì¶ [28/91] Processing fhvhv_tripdata_2024-05.parquet ...\n",
      "   ‚Üí Added 2,000,000 rows (Total: 149,968,641)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 151,968,641)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 153,968,641)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 155,968,641)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 157,968,641)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 159,968,641)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 161,968,641)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 163,968,641)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 165,968,641)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 167,968,641)\n",
      "   ‚Üí Added 704,538 rows (Total: 168,673,179)\n",
      "üì¶ [29/91] Processing fhvhv_tripdata_2024-06.parquet ...\n",
      "   ‚Üí Added 2,000,000 rows (Total: 170,673,179)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 172,673,179)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 174,673,179)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 176,673,179)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 178,673,179)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 180,673,179)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 182,673,179)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 184,673,179)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 186,673,179)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 188,673,179)\n",
      "   ‚Üí Added 123,226 rows (Total: 188,796,405)\n",
      "üì¶ [30/91] Processing fhvhv_tripdata_2024-07.parquet ...\n",
      "   ‚Üí Added 2,000,000 rows (Total: 190,796,405)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 192,796,405)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 194,796,405)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 196,796,405)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 198,796,405)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 200,796,405)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 202,796,405)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 204,796,405)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 206,796,405)\n",
      "   ‚Üí Added 1,182,934 rows (Total: 207,979,339)\n",
      "üì¶ [31/91] Processing fhvhv_tripdata_2024-08.parquet ...\n",
      "   ‚Üí Added 2,000,000 rows (Total: 209,979,339)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 211,979,339)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 213,979,339)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 215,979,339)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 217,979,339)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 219,979,339)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 221,979,339)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 223,979,339)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 225,979,339)\n",
      "   ‚Üí Added 1,128,392 rows (Total: 227,107,731)\n",
      "üì¶ [32/91] Processing fhvhv_tripdata_2024-09.parquet ...\n",
      "   ‚Üí Added 2,000,000 rows (Total: 229,107,731)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 231,107,731)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 233,107,731)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 235,107,731)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 237,107,731)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 239,107,731)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 241,107,731)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 243,107,731)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 245,107,731)\n",
      "   ‚Üí Added 1,209,788 rows (Total: 246,317,519)\n",
      "üì¶ [33/91] Processing fhvhv_tripdata_2024-10.parquet ...\n",
      "   ‚Üí Added 2,000,000 rows (Total: 248,317,519)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 250,317,519)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 252,317,519)\n",
      "   ‚Üí Added 2,000,000 rows (Total: 254,317,519)\n",
      "‚ö†Ô∏è Skipped s3://data228-bigdata-nyc/staging/fhvhv_tripdata_2024-10.parquet: BloomFilter is at capacity\n",
      "üì¶ [34/91] Processing fhvhv_tripdata_2024-11.parquet ...\n",
      "‚ö†Ô∏è Skipped s3://data228-bigdata-nyc/staging/fhvhv_tripdata_2024-11.parquet: BloomFilter is at capacity\n",
      "üì¶ [35/91] Processing fhvhv_tripdata_2024-12.parquet ...\n",
      "‚ö†Ô∏è Skipped s3://data228-bigdata-nyc/staging/fhvhv_tripdata_2024-12.parquet: BloomFilter is at capacity\n",
      "üì¶ [36/91] Processing fhvhv_tripdata_2025-02.parquet ...\n",
      "‚ö†Ô∏è Skipped s3://data228-bigdata-nyc/staging/fhvhv_tripdata_2025-02.parquet: BloomFilter is at capacity\n",
      "üì¶ [37/91] Processing fhvhv_tripdata_2025-04.parquet ...\n",
      "‚ö†Ô∏è Skipped s3://data228-bigdata-nyc/staging/fhvhv_tripdata_2025-04.parquet: BloomFilter is at capacity\n",
      "üì¶ [38/91] Processing fhvhv_tripdata_2025-05.parquet ...\n",
      "‚ö†Ô∏è Skipped s3://data228-bigdata-nyc/staging/fhvhv_tripdata_2025-05.parquet: BloomFilter is at capacity\n",
      "üì¶ [39/91] Processing fhvhv_tripdata_2025-06.parquet ...\n",
      "‚ö†Ô∏è Skipped s3://data228-bigdata-nyc/staging/fhvhv_tripdata_2025-06.parquet: BloomFilter is at capacity\n",
      "üì¶ [40/91] Processing fhvhv_tripdata_2025-07.parquet ...\n",
      "‚ö†Ô∏è Skipped s3://data228-bigdata-nyc/staging/fhvhv_tripdata_2025-07.parquet: BloomFilter is at capacity\n",
      "üì¶ [41/91] Processing fhvhv_tripdata_2025-08.parquet ...\n",
      "‚ö†Ô∏è Skipped s3://data228-bigdata-nyc/staging/fhvhv_tripdata_2025-08.parquet: BloomFilter is at capacity\n",
      "üì¶ [42/91] Processing green_tripdata_2014-02.parquet ...\n",
      "‚ö†Ô∏è Skipped s3://data228-bigdata-nyc/staging/green_tripdata_2014-02.parquet: BloomFilter is at capacity\n",
      "üì¶ [43/91] Processing green_tripdata_2014-12.parquet ...\n",
      "‚ö†Ô∏è Skipped s3://data228-bigdata-nyc/staging/green_tripdata_2014-12.parquet: BloomFilter is at capacity\n",
      "üì¶ [44/91] Processing green_tripdata_2015-02.parquet ...\n",
      "‚ö†Ô∏è Skipped s3://data228-bigdata-nyc/staging/green_tripdata_2015-02.parquet: BloomFilter is at capacity\n",
      "üì¶ [45/91] Processing green_tripdata_2015-12.parquet ...\n",
      "‚ö†Ô∏è Skipped s3://data228-bigdata-nyc/staging/green_tripdata_2015-12.parquet: BloomFilter is at capacity\n",
      "üì¶ [46/91] Processing green_tripdata_2019-02.parquet ...\n",
      "‚ö†Ô∏è Skipped s3://data228-bigdata-nyc/staging/green_tripdata_2019-02.parquet: BloomFilter is at capacity\n",
      "üì¶ [47/91] Processing green_tripdata_2019-12.parquet ...\n",
      "‚ö†Ô∏è Skipped s3://data228-bigdata-nyc/staging/green_tripdata_2019-12.parquet: BloomFilter is at capacity\n",
      "üì¶ [48/91] Processing green_tripdata_2024-01.parquet ...\n",
      "‚ö†Ô∏è Skipped s3://data228-bigdata-nyc/staging/green_tripdata_2024-01.parquet: BloomFilter is at capacity\n",
      "üì¶ [49/91] Processing green_tripdata_2024-02.parquet ...\n",
      "‚ö†Ô∏è Skipped s3://data228-bigdata-nyc/staging/green_tripdata_2024-02.parquet: BloomFilter is at capacity\n",
      "üì¶ [50/91] Processing green_tripdata_2024-03.parquet ...\n",
      "‚ö†Ô∏è Skipped s3://data228-bigdata-nyc/staging/green_tripdata_2024-03.parquet: BloomFilter is at capacity\n",
      "üì¶ [51/91] Processing green_tripdata_2024-04.parquet ...\n",
      "‚ö†Ô∏è Skipped s3://data228-bigdata-nyc/staging/green_tripdata_2024-04.parquet: BloomFilter is at capacity\n",
      "üì¶ [52/91] Processing green_tripdata_2024-05.parquet ...\n",
      "‚ö†Ô∏è Skipped s3://data228-bigdata-nyc/staging/green_tripdata_2024-05.parquet: BloomFilter is at capacity\n",
      "üì¶ [53/91] Processing green_tripdata_2024-06.parquet ...\n",
      "‚ö†Ô∏è Skipped s3://data228-bigdata-nyc/staging/green_tripdata_2024-06.parquet: BloomFilter is at capacity\n",
      "üì¶ [54/91] Processing green_tripdata_2024-07.parquet ...\n",
      "‚ö†Ô∏è Skipped s3://data228-bigdata-nyc/staging/green_tripdata_2024-07.parquet: BloomFilter is at capacity\n",
      "üì¶ [55/91] Processing green_tripdata_2024-08.parquet ...\n",
      "‚ö†Ô∏è Skipped s3://data228-bigdata-nyc/staging/green_tripdata_2024-08.parquet: BloomFilter is at capacity\n",
      "üì¶ [56/91] Processing green_tripdata_2024-09.parquet ...\n",
      "‚ö†Ô∏è Skipped s3://data228-bigdata-nyc/staging/green_tripdata_2024-09.parquet: BloomFilter is at capacity\n",
      "üì¶ [57/91] Processing green_tripdata_2024-10.parquet ...\n",
      "‚ö†Ô∏è Skipped s3://data228-bigdata-nyc/staging/green_tripdata_2024-10.parquet: BloomFilter is at capacity\n",
      "üì¶ [58/91] Processing green_tripdata_2024-11.parquet ...\n",
      "‚ö†Ô∏è Skipped s3://data228-bigdata-nyc/staging/green_tripdata_2024-11.parquet: BloomFilter is at capacity\n",
      "üì¶ [59/91] Processing green_tripdata_2024-12.parquet ...\n",
      "‚ö†Ô∏è Skipped s3://data228-bigdata-nyc/staging/green_tripdata_2024-12.parquet: BloomFilter is at capacity\n",
      "üì¶ [60/91] Processing green_tripdata_2025-01.parquet ...\n",
      "‚ö†Ô∏è Skipped s3://data228-bigdata-nyc/staging/green_tripdata_2025-01.parquet: BloomFilter is at capacity\n",
      "üì¶ [61/91] Processing green_tripdata_2025-02.parquet ...\n",
      "‚ö†Ô∏è Skipped s3://data228-bigdata-nyc/staging/green_tripdata_2025-02.parquet: BloomFilter is at capacity\n",
      "üì¶ [62/91] Processing green_tripdata_2025-03.parquet ...\n",
      "‚ö†Ô∏è Skipped s3://data228-bigdata-nyc/staging/green_tripdata_2025-03.parquet: BloomFilter is at capacity\n",
      "üì¶ [63/91] Processing green_tripdata_2025-04.parquet ...\n",
      "‚ö†Ô∏è Skipped s3://data228-bigdata-nyc/staging/green_tripdata_2025-04.parquet: BloomFilter is at capacity\n",
      "üì¶ [64/91] Processing green_tripdata_2025-07.parquet ...\n",
      "‚ö†Ô∏è Skipped s3://data228-bigdata-nyc/staging/green_tripdata_2025-07.parquet: BloomFilter is at capacity\n",
      "üì¶ [65/91] Processing green_tripdata_2025-08.parquet ...\n",
      "‚ö†Ô∏è Skipped s3://data228-bigdata-nyc/staging/green_tripdata_2025-08.parquet: BloomFilter is at capacity\n",
      "üì¶ [66/91] Processing yellow_tripdata_2009-02.parquet ...\n",
      "‚ö†Ô∏è Skipped s3://data228-bigdata-nyc/staging/yellow_tripdata_2009-02.parquet: Length of values (0) does not match length of index (2000000)\n",
      "üì¶ [67/91] Processing yellow_tripdata_2009-12.parquet ...\n",
      "‚ö†Ô∏è Skipped s3://data228-bigdata-nyc/staging/yellow_tripdata_2009-12.parquet: Length of values (0) does not match length of index (2000000)\n",
      "üì¶ [68/91] Processing yellow_tripdata_2014-02.parquet ...\n",
      "‚ö†Ô∏è Skipped s3://data228-bigdata-nyc/staging/yellow_tripdata_2014-02.parquet: BloomFilter is at capacity\n",
      "üì¶ [69/91] Processing yellow_tripdata_2014-12.parquet ...\n",
      "‚ö†Ô∏è Skipped s3://data228-bigdata-nyc/staging/yellow_tripdata_2014-12.parquet: BloomFilter is at capacity\n",
      "üì¶ [70/91] Processing yellow_tripdata_2015-02.parquet ...\n",
      "‚ö†Ô∏è Skipped s3://data228-bigdata-nyc/staging/yellow_tripdata_2015-02.parquet: BloomFilter is at capacity\n",
      "üì¶ [71/91] Processing yellow_tripdata_2015-12.parquet ...\n",
      "‚ö†Ô∏è Skipped s3://data228-bigdata-nyc/staging/yellow_tripdata_2015-12.parquet: BloomFilter is at capacity\n",
      "üì¶ [72/91] Processing yellow_tripdata_2019-02.parquet ...\n",
      "‚ö†Ô∏è Skipped s3://data228-bigdata-nyc/staging/yellow_tripdata_2019-02.parquet: BloomFilter is at capacity\n",
      "üì¶ [73/91] Processing yellow_tripdata_2019-12.parquet ...\n",
      "‚ö†Ô∏è Skipped s3://data228-bigdata-nyc/staging/yellow_tripdata_2019-12.parquet: BloomFilter is at capacity\n",
      "üì¶ [74/91] Processing yellow_tripdata_2024-01.parquet ...\n",
      "‚ö†Ô∏è Skipped s3://data228-bigdata-nyc/staging/yellow_tripdata_2024-01.parquet: BloomFilter is at capacity\n",
      "üì¶ [75/91] Processing yellow_tripdata_2024-02.parquet ...\n",
      "‚ö†Ô∏è Skipped s3://data228-bigdata-nyc/staging/yellow_tripdata_2024-02.parquet: BloomFilter is at capacity\n",
      "üì¶ [76/91] Processing yellow_tripdata_2024-03.parquet ...\n",
      "‚ö†Ô∏è Skipped s3://data228-bigdata-nyc/staging/yellow_tripdata_2024-03.parquet: BloomFilter is at capacity\n",
      "üì¶ [77/91] Processing yellow_tripdata_2024-04.parquet ...\n",
      "‚ö†Ô∏è Skipped s3://data228-bigdata-nyc/staging/yellow_tripdata_2024-04.parquet: BloomFilter is at capacity\n",
      "üì¶ [78/91] Processing yellow_tripdata_2024-05.parquet ...\n",
      "‚ö†Ô∏è Skipped s3://data228-bigdata-nyc/staging/yellow_tripdata_2024-05.parquet: BloomFilter is at capacity\n",
      "üì¶ [79/91] Processing yellow_tripdata_2024-06.parquet ...\n",
      "‚ö†Ô∏è Skipped s3://data228-bigdata-nyc/staging/yellow_tripdata_2024-06.parquet: BloomFilter is at capacity\n",
      "üì¶ [80/91] Processing yellow_tripdata_2024-07.parquet ...\n",
      "‚ö†Ô∏è Skipped s3://data228-bigdata-nyc/staging/yellow_tripdata_2024-07.parquet: BloomFilter is at capacity\n",
      "üì¶ [81/91] Processing yellow_tripdata_2024-08.parquet ...\n",
      "‚ö†Ô∏è Skipped s3://data228-bigdata-nyc/staging/yellow_tripdata_2024-08.parquet: BloomFilter is at capacity\n",
      "üì¶ [82/91] Processing yellow_tripdata_2024-09.parquet ...\n",
      "‚ö†Ô∏è Skipped s3://data228-bigdata-nyc/staging/yellow_tripdata_2024-09.parquet: BloomFilter is at capacity\n",
      "üì¶ [83/91] Processing yellow_tripdata_2024-10.parquet ...\n",
      "‚ö†Ô∏è Skipped s3://data228-bigdata-nyc/staging/yellow_tripdata_2024-10.parquet: BloomFilter is at capacity\n",
      "üì¶ [84/91] Processing yellow_tripdata_2024-11.parquet ...\n",
      "‚ö†Ô∏è Skipped s3://data228-bigdata-nyc/staging/yellow_tripdata_2024-11.parquet: BloomFilter is at capacity\n",
      "üì¶ [85/91] Processing yellow_tripdata_2024-12.parquet ...\n",
      "‚ö†Ô∏è Skipped s3://data228-bigdata-nyc/staging/yellow_tripdata_2024-12.parquet: BloomFilter is at capacity\n",
      "üì¶ [86/91] Processing yellow_tripdata_2025-01.parquet ...\n",
      "‚ö†Ô∏è Skipped s3://data228-bigdata-nyc/staging/yellow_tripdata_2025-01.parquet: BloomFilter is at capacity\n",
      "üì¶ [87/91] Processing yellow_tripdata_2025-02.parquet ...\n",
      "‚ö†Ô∏è Skipped s3://data228-bigdata-nyc/staging/yellow_tripdata_2025-02.parquet: BloomFilter is at capacity\n",
      "üì¶ [88/91] Processing yellow_tripdata_2025-03.parquet ...\n",
      "‚ö†Ô∏è Skipped s3://data228-bigdata-nyc/staging/yellow_tripdata_2025-03.parquet: BloomFilter is at capacity\n",
      "üì¶ [89/91] Processing yellow_tripdata_2025-04.parquet ...\n",
      "‚ö†Ô∏è Skipped s3://data228-bigdata-nyc/staging/yellow_tripdata_2025-04.parquet: BloomFilter is at capacity\n",
      "üì¶ [90/91] Processing yellow_tripdata_2025-07.parquet ...\n",
      "‚ö†Ô∏è Skipped s3://data228-bigdata-nyc/staging/yellow_tripdata_2025-07.parquet: BloomFilter is at capacity\n",
      "üì¶ [91/91] Processing yellow_tripdata_2025-08.parquet ...\n",
      "‚ö†Ô∏è Skipped s3://data228-bigdata-nyc/staging/yellow_tripdata_2025-08.parquet: BloomFilter is at capacity\n",
      "\n",
      "‚úÖ Finished populating Bloom filter.\n",
      "   Total processed rows: 254,317,519\n",
      "   Current fill ratio: 100.00%\n",
      "\n",
      "üíæ Saved locally: bloom_all_years_20251117_234312.pkl\n",
      "‚òÅÔ∏è Uploaded Bloom Filter to s3://data228-bigdata-nyc/bloom_filter/bloom_all_years.pkl\n",
      "\n",
      "üèÅ Bloom Filter creation complete!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Create Bloom Filter from S3 Parquet Files (All Years Except Subfolder 2025/)\n",
    "------------------------------------------------------------------------\n",
    "This script builds a Bloom filter for all NYC taxi trip data in:\n",
    "    s3://data228-bigdata-nyc/staging/\n",
    "excluding any files under:\n",
    "    s3://data228-bigdata-nyc/staging/2025/\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import s3fs\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "from pybloom_live import BloomFilter\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# -----------------------------------------------------------------------\n",
    "AWS_PROFILE = \"data228\"\n",
    "BUCKET_NAME = \"data228-bigdata-nyc\"\n",
    "INPUT_PREFIX = \"staging/\"     # base folder\n",
    "EXCLUDE_PREFIX = \"staging/2025/\"  # subfolder to skip\n",
    "OUTPUT_BLOOM_PATH = \"bloom_filter/bloom_all_years.pkl\"\n",
    "\n",
    "FALSE_POSITIVE_RATE = 0.01\n",
    "ESTIMATED_TRIPS = 400_000_000  # increased to safely hold historical + 2025 trips\n",
    "BATCH_SIZE = 2_000_000         # process rows in chunks\n",
    "\n",
    "os.environ[\"AWS_PROFILE\"] = AWS_PROFILE\n",
    "\n",
    "print(\"üöÄ Creating Bloom Filter for all NYC Taxi data (excluding /2025/)\")\n",
    "print(f\"Bucket: s3://{BUCKET_NAME}/{INPUT_PREFIX}\")\n",
    "print(f\"Exclude: s3://{BUCKET_NAME}/{EXCLUDE_PREFIX}\")\n",
    "print(f\"Output Bloom Filter: s3://{BUCKET_NAME}/{OUTPUT_BLOOM_PATH}\\n\")\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# INITIALIZE FILESYSTEM AND BLOOM FILTER\n",
    "# -----------------------------------------------------------------------\n",
    "fs = s3fs.S3FileSystem(profile=AWS_PROFILE)\n",
    "bloom = BloomFilter(capacity=ESTIMATED_TRIPS, error_rate=FALSE_POSITIVE_RATE)\n",
    "print(f\"‚úÖ Initialized Bloom Filter (capacity={ESTIMATED_TRIPS:,}, FPR={FALSE_POSITIVE_RATE:.2%})\")\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# HELPER: BUILD trip_id STRING\n",
    "# -----------------------------------------------------------------------\n",
    "def build_trip_id(df: pd.DataFrame):\n",
    "    pickup = next((c for c in [\"tpep_pickup_datetime\", \"lpep_pickup_datetime\", \"pickup_datetime\"] if c in df.columns), None)\n",
    "    dropoff = next((c for c in [\"tpep_dropoff_datetime\", \"lpep_dropoff_datetime\", \"dropoff_datetime\", \"dropOff_datetime\"] if c in df.columns), None)\n",
    "    puloc = next((c for c in [\"PULocationID\", \"PUlocationID\"] if c in df.columns), None)\n",
    "    doloc = next((c for c in [\"DOLocationID\", \"DOlocationID\"] if c in df.columns), None)\n",
    "    vendor = \"VendorID\" if \"VendorID\" in df.columns else None\n",
    "\n",
    "    parts = []\n",
    "    if vendor: parts.append(df[vendor].astype(str))\n",
    "    if pickup: parts.append(df[pickup].astype(str))\n",
    "    if dropoff: parts.append(df[dropoff].astype(str))\n",
    "    if puloc: parts.append(df[puloc].astype(str))\n",
    "    if doloc: parts.append(df[doloc].astype(str))\n",
    "\n",
    "    return pd.Series([\"_\".join(x) for x in zip(*parts)], index=df.index)\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# GET ALL PARQUET FILES (EXCLUDING SUBFOLDER)\n",
    "# -----------------------------------------------------------------------\n",
    "print(\"\\nüìÇ Listing all Parquet files from S3...\")\n",
    "\n",
    "all_files = fs.glob(f\"{BUCKET_NAME}/{INPUT_PREFIX}*.parquet\")\n",
    "excluded_files = fs.glob(f\"{BUCKET_NAME}/{EXCLUDE_PREFIX}*.parquet\")\n",
    "\n",
    "# Filter out excluded files\n",
    "files = [f\"s3://{f}\" for f in all_files if f not in excluded_files]\n",
    "print(f\"‚úÖ Found {len(files)} parquet files (excluded {len(excluded_files)} from /2025/)\\n\")\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# PROCESS FILES AND POPULATE BLOOM FILTER\n",
    "# -----------------------------------------------------------------------\n",
    "total_rows = 0\n",
    "added_rows = 0\n",
    "\n",
    "for i, path in enumerate(files, 1):\n",
    "    file_name = os.path.basename(path)\n",
    "    print(f\"üì¶ [{i}/{len(files)}] Processing {file_name} ...\")\n",
    "\n",
    "    try:\n",
    "        pq_file = pq.ParquetFile(path, filesystem=fs)\n",
    "        for batch in pq_file.iter_batches(batch_size=BATCH_SIZE):\n",
    "            df = batch.to_pandas()\n",
    "            trip_ids = build_trip_id(df)\n",
    "\n",
    "            for tid in trip_ids:\n",
    "                bloom.add(tid)\n",
    "\n",
    "            total_rows += len(df)\n",
    "            added_rows += len(df)\n",
    "            print(f\"   ‚Üí Added {len(df):,} rows (Total: {total_rows:,})\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Skipped {path}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(\"\\n‚úÖ Finished populating Bloom filter.\")\n",
    "print(f\"   Total processed rows: {total_rows:,}\")\n",
    "print(f\"   Current fill ratio: {bloom.count / bloom.capacity:.2%}\\n\")\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# SAVE BLOOM FILTER (LOCAL + S3)\n",
    "# -----------------------------------------------------------------------\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "local_file = f\"bloom_all_years_{timestamp}.pkl\"\n",
    "\n",
    "with open(local_file, \"wb\") as f:\n",
    "    pickle.dump(bloom, f)\n",
    "print(f\"üíæ Saved locally: {local_file}\")\n",
    "\n",
    "# Upload to S3\n",
    "fs.put(local_file, f\"{BUCKET_NAME}/{OUTPUT_BLOOM_PATH}\")\n",
    "print(f\"‚òÅÔ∏è Uploaded Bloom Filter to s3://{BUCKET_NAME}/{OUTPUT_BLOOM_PATH}\")\n",
    "\n",
    "print(\"\\nüèÅ Bloom Filter creation complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c80fbb-dc04-4f53-9992-7a7178e3bc2d",
   "metadata": {},
   "source": [
    "## bloom Filter check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edf95c61-c08b-4055-a98a-ef4d861fd9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading Bloom filter from s3://data228-bigdata-nyc/bloom_filter/bloom_all_years.pkl ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müì• Loading Bloom filter from s3://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBUCKET_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBLOOM_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m fs.open(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBUCKET_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBLOOM_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     bloom = \u001b[43mpickle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Bloom filter loaded successfully!\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# --- Inspect key properties ---\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/venv/lib/python3.13/site-packages/fsspec/spec.py:2140\u001b[39m, in \u001b[36mAbstractBufferedFile.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m   2135\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"mirrors builtin file's readinto method\u001b[39;00m\n\u001b[32m   2136\u001b[39m \n\u001b[32m   2137\u001b[39m \u001b[33;03mhttps://docs.python.org/3/library/io.html#io.RawIOBase.readinto\u001b[39;00m\n\u001b[32m   2138\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2139\u001b[39m out = \u001b[38;5;28mmemoryview\u001b[39m(b).cast(\u001b[33m\"\u001b[39m\u001b[33mB\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2140\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2141\u001b[39m out[: \u001b[38;5;28mlen\u001b[39m(data)] = data\n\u001b[32m   2142\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/venv/lib/python3.13/site-packages/fsspec/spec.py:2122\u001b[39m, in \u001b[36mAbstractBufferedFile.read\u001b[39m\u001b[34m(self, length)\u001b[39m\n\u001b[32m   2119\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m length == \u001b[32m0\u001b[39m:\n\u001b[32m   2120\u001b[39m     \u001b[38;5;66;03m# don't even bother calling fetch\u001b[39;00m\n\u001b[32m   2121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m2122\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcache\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_fetch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2124\u001b[39m logger.debug(\n\u001b[32m   2125\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m read: \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   2126\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2129\u001b[39m     \u001b[38;5;28mself\u001b[39m.cache._log_stats(),\n\u001b[32m   2130\u001b[39m )\n\u001b[32m   2131\u001b[39m \u001b[38;5;28mself\u001b[39m.loc += \u001b[38;5;28mlen\u001b[39m(out)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/venv/lib/python3.13/site-packages/fsspec/caching.py:556\u001b[39m, in \u001b[36mBytesCache._fetch\u001b[39m\u001b[34m(self, start, end)\u001b[39m\n\u001b[32m    554\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m end - \u001b[38;5;28mself\u001b[39m.end > \u001b[38;5;28mself\u001b[39m.blocksize:\n\u001b[32m    555\u001b[39m     \u001b[38;5;28mself\u001b[39m.total_requested_bytes += bend - start\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m     \u001b[38;5;28mself\u001b[39m.cache = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfetcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    557\u001b[39m     \u001b[38;5;28mself\u001b[39m.start = start\n\u001b[32m    558\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/venv/lib/python3.13/site-packages/s3fs/core.py:1200\u001b[39m, in \u001b[36mS3File._fetch_range\u001b[39m\u001b[34m(self, start, end)\u001b[39m\n\u001b[32m   1199\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_fetch_range\u001b[39m(\u001b[38;5;28mself\u001b[39m, start, end):\n\u001b[32m-> \u001b[39m\u001b[32m1200\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_fetch_range\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m.\u001b[49m\u001b[43ms3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbucket\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mversion_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq_kw\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreq_kw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/venv/lib/python3.13/site-packages/s3fs/core.py:1337\u001b[39m, in \u001b[36m_fetch_range\u001b[39m\u001b[34m(client, bucket, key, version_id, start, end, max_attempts, req_kw)\u001b[39m\n\u001b[32m   1332\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1333\u001b[39m     resp = client.get_object(Bucket=bucket, Key=key,\n\u001b[32m   1334\u001b[39m                              Range=\u001b[33m'\u001b[39m\u001b[33mbytes=\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[33m-\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[33m'\u001b[39m % (start, end - \u001b[32m1\u001b[39m),\n\u001b[32m   1335\u001b[39m                              **version_id_kw(version_id),\n\u001b[32m   1336\u001b[39m                              **req_kw)\n\u001b[32m-> \u001b[39m\u001b[32m1337\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresp\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mBody\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1338\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m S3_RETRYABLE_ERRORS \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1339\u001b[39m     logger.debug(\u001b[33m'\u001b[39m\u001b[33mException \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m on S3 download, retrying\u001b[39m\u001b[33m'\u001b[39m, e,\n\u001b[32m   1340\u001b[39m                  exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/venv/lib/python3.13/site-packages/botocore/response.py:98\u001b[39m, in \u001b[36mStreamingBody.read\u001b[39m\u001b[34m(self, amt)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Read at most amt bytes from the stream.\u001b[39;00m\n\u001b[32m     94\u001b[39m \n\u001b[32m     95\u001b[39m \u001b[33;03mIf the amt argument is omitted, read all data.\u001b[39;00m\n\u001b[32m     96\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m     chunk = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raw_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m URLLib3ReadTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    100\u001b[39m     \u001b[38;5;66;03m# TODO: the url will be None as urllib3 isn't setting it yet\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeoutError(endpoint_url=e.url, error=e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/venv/lib/python3.13/site-packages/urllib3/response.py:980\u001b[39m, in \u001b[36mHTTPResponse.read\u001b[39m\u001b[34m(self, amt, decode_content, cache_content)\u001b[39m\n\u001b[32m    977\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) >= amt:\n\u001b[32m    978\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._decoded_buffer.get(amt)\n\u001b[32m--> \u001b[39m\u001b[32m980\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    982\u001b[39m flush_decoder = amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt != \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[32m    984\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/venv/lib/python3.13/site-packages/urllib3/response.py:904\u001b[39m, in \u001b[36mHTTPResponse._raw_read\u001b[39m\u001b[34m(self, amt, read1)\u001b[39m\n\u001b[32m    901\u001b[39m fp_closed = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m._fp, \u001b[33m\"\u001b[39m\u001b[33mclosed\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    903\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._error_catcher():\n\u001b[32m--> \u001b[39m\u001b[32m904\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    905\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt != \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[32m    906\u001b[39m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    912\u001b[39m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;28mself\u001b[39m._fp.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/venv/lib/python3.13/site-packages/urllib3/response.py:887\u001b[39m, in \u001b[36mHTTPResponse._fp_read\u001b[39m\u001b[34m(self, amt, read1)\u001b[39m\n\u001b[32m    884\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fp.read1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fp.read1()\n\u001b[32m    885\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    886\u001b[39m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m887\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fp.read(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.3/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py:495\u001b[39m, in \u001b[36mHTTPResponse.read\u001b[39m\u001b[34m(self, amt)\u001b[39m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    494\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m495\u001b[39m         s = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_safe_read\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    496\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m IncompleteRead:\n\u001b[32m    497\u001b[39m         \u001b[38;5;28mself\u001b[39m._close_conn()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.3/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py:642\u001b[39m, in \u001b[36mHTTPResponse._safe_read\u001b[39m\u001b[34m(self, amt)\u001b[39m\n\u001b[32m    635\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_safe_read\u001b[39m(\u001b[38;5;28mself\u001b[39m, amt):\n\u001b[32m    636\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Read the number of bytes requested.\u001b[39;00m\n\u001b[32m    637\u001b[39m \n\u001b[32m    638\u001b[39m \u001b[33;03m    This function should be used when <amt> bytes \"should\" be present for\u001b[39;00m\n\u001b[32m    639\u001b[39m \u001b[33;03m    reading. If the bytes are truly not available (due to EOF), then the\u001b[39;00m\n\u001b[32m    640\u001b[39m \u001b[33;03m    IncompleteRead exception can be used to detect the problem.\u001b[39;00m\n\u001b[32m    641\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m642\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    643\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) < amt:\n\u001b[32m    644\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m IncompleteRead(data, amt-\u001b[38;5;28mlen\u001b[39m(data))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.3/Frameworks/Python.framework/Versions/3.13/lib/python3.13/socket.py:719\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot read from timed out object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m719\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    721\u001b[39m     \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.3/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py:1304\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1300\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1301\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1302\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1303\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1304\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1305\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1306\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.3/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py:1138\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1140\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import s3fs\n",
    "\n",
    "# Configuration\n",
    "AWS_PROFILE = \"data228\"\n",
    "BUCKET_NAME = \"data228-bigdata-nyc\"\n",
    "BLOOM_PATH  = \"bloom_filter/bloom_all_years.pkl\"\n",
    "\n",
    "# Load from S3 using s3fs\n",
    "fs = s3fs.S3FileSystem(profile=AWS_PROFILE)\n",
    "\n",
    "print(f\"üì• Loading Bloom filter from s3://{BUCKET_NAME}/{BLOOM_PATH} ...\")\n",
    "\n",
    "with fs.open(f\"{BUCKET_NAME}/{BLOOM_PATH}\", \"rb\") as f:\n",
    "    bloom = pickle.load(f)\n",
    "\n",
    "print(\"‚úÖ Bloom filter loaded successfully!\\n\")\n",
    "\n",
    "# --- Inspect key properties ---\n",
    "print(\"üìä Bloom Filter Summary:\")\n",
    "print(f\"   ‚Üí Capacity:         {bloom.capacity:,}\")\n",
    "print(f\"   ‚Üí Elements stored:  {bloom.count:,}\")\n",
    "print(f\"   ‚Üí Fill ratio:       {bloom.count / bloom.capacity:.4%}\")\n",
    "print(f\"   ‚Üí Error rate:       {bloom.error_rate * 100:.2f}%\")\n",
    "\n",
    "# --- Optional: check membership manually ---\n",
    "print(\"\\nüîç Membership test (example):\")\n",
    "test_ids = [\n",
    "    \"2024-06-01 10:00:00_2024-06-01 10:45:00_132_256\",\n",
    "    \"random_trip_not_existing\"\n",
    "]\n",
    "for tid in test_ids:\n",
    "    print(f\"   {tid} ‚Üí {'Possibly in filter' if tid in bloom else 'Definitely not in filter'}\")\n",
    "\n",
    "# --- Optional: show internal details (bit array stats) ---\n",
    "print(\"\\nüß† Internal representation:\")\n",
    "print(f\"   Bit array length:   {len(bloom.bitarray):,}\")\n",
    "print(f\"   Bits set to 1:      {bloom.bitarray.count(1):,}\")\n",
    "print(f\"   Bits still 0:       {len(bloom.bitarray) - bloom.bitarray.count(1):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e4ce00-5c1a-4fe3-9a85-8f98ab68df7d",
   "metadata": {},
   "source": [
    "## Creating streaming and apllyong bloom filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb12d1fb-b818-4e5b-b92c-f8e150235c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc, time, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.dataset as ds\n",
    "import s3fs\n",
    "from pyspark.sql import SparkSession\n",
    "from pybloom_live import BloomFilter\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# -----------------------------------------------------------------------\n",
    "AWS_PROFILE   = \"data228\"\n",
    "BUCKET_NAME   = \"data228-bigdata-nyc\"\n",
    "STAGING_PREFIX = \"staging/2025/\"     # STREAMING DATA LOCATION\n",
    "OUTPUT_PREFIX = \"dedup/2025_stream_dedup.parquet\"\n",
    "BLOOM_PATH = \"bloom_filter/bloom_all_years.pkl\"  # Historical Bloom filter\n",
    "\n",
    "os.environ[\"AWS_PROFILE\"] = AWS_PROFILE\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# LOAD BLOOM FILTER FROM S3\n",
    "# -----------------------------------------------------------------------\n",
    "fs = s3fs.S3FileSystem(profile=AWS_PROFILE)\n",
    "print(f\"üì• Loading Bloom filter from s3://{BUCKET_NAME}/{BLOOM_PATH} ...\")\n",
    "\n",
    "with fs.open(f\"{BUCKET_NAME}/{BLOOM_PATH}\", \"rb\") as f:\n",
    "    bloom = pickle.load(f)\n",
    "\n",
    "print(\"‚úÖ Bloom filter loaded!\")\n",
    "print(f\"   Capacity: {bloom.capacity:,}\")\n",
    "print(f\"   Existing elements: {bloom.count:,}\")\n",
    "print(f\"   Fill ratio: {bloom.count / bloom.capacity:.2%}\\n\")\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# INITIALIZE SPARK (for writing deduplicated data)\n",
    "# -----------------------------------------------------------------------\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"StreamDedup-Bloom-2025\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.sql.shuffle.partitions\",\"16\")\n",
    "    .config(\"spark.driver.memory\",\"8g\")\n",
    "    .config(\"spark.executor.memory\",\"8g\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# DISCOVER 2025 FILES FROM S3\n",
    "# -----------------------------------------------------------------------\n",
    "print(f\"üìÇ Scanning s3://{BUCKET_NAME}/{STAGING_PREFIX} ...\")\n",
    "files = fs.glob(f\"{BUCKET_NAME}/{STAGING_PREFIX}*.parquet\")\n",
    "print(f\"‚úÖ Found {len(files)} 2025 files.\\n\")\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# HELPER: BUILD TRIP_ID (vectorized)\n",
    "# -----------------------------------------------------------------------\n",
    "def build_trip_id_vectorized(df: pd.DataFrame):\n",
    "    cols = df.columns\n",
    "    pickup = next((c for c in [\"tpep_pickup_datetime\",\"lpep_pickup_datetime\",\"pickup_datetime\"] if c in cols), None)\n",
    "    dropoff = next((c for c in [\"tpep_dropoff_datetime\",\"lpep_dropoff_datetime\",\"dropoff_datetime\",\"dropOff_datetime\"] if c in cols), None)\n",
    "    puloc = next((c for c in [\"PULocationID\",\"PUlocationID\"] if c in cols), None)\n",
    "    doloc = next((c for c in [\"DOLocationID\",\"DOlocationID\"] if c in cols), None)\n",
    "    \n",
    "    parts = []\n",
    "    if pickup:  parts.append(df[pickup].astype(str))\n",
    "    if dropoff: parts.append(df[dropoff].astype(str))\n",
    "    if puloc:   parts.append(df[puloc].astype(str))\n",
    "    if doloc:   parts.append(df[doloc].astype(str))\n",
    "    return pd.Series([\"_\".join(x) for x in zip(*parts)], index=df.index)\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# STREAM THROUGH 2025 FILES\n",
    "# -----------------------------------------------------------------------\n",
    "local_tmp = \"/tmp/stream_2025_dedup\"\n",
    "os.system(f\"rm -rf {local_tmp}\")\n",
    "total_rows, unique_rows, dup_rows = 0, 0, 0\n",
    "batch_no = 0\n",
    "\n",
    "start = time.time()\n",
    "for fpath in files:\n",
    "    batch_no += 1\n",
    "    fname = fpath.split(\"/\")[-1]\n",
    "    print(f\"\\nüì¶ [{batch_no}/{len(files)}] Processing {fname} ...\")\n",
    "\n",
    "    df = pd.read_parquet(f\"s3://{fpath}\", storage_options={\"profile\": AWS_PROFILE})\n",
    "    trip_ids = build_trip_id_vectorized(df)\n",
    "\n",
    "    # Membership test (True if already in Bloom ‚Üí duplicate)\n",
    "    seen_mask = np.fromiter((tid in bloom for tid in trip_ids), bool, len(trip_ids))\n",
    "    keep_mask = ~seen_mask\n",
    "    unique_df = df.loc[keep_mask].copy()\n",
    "\n",
    "    # Add new unique trip IDs to Bloom filter (protect against capacity overflow)\n",
    "    if bloom.count < bloom.capacity:\n",
    "        for tid in trip_ids[keep_mask]:\n",
    "            try:\n",
    "                bloom.add(tid)\n",
    "            except IndexError:\n",
    "                print(\"   ‚ö†Ô∏è Bloom filter reached capacity; stopping further additions.\")\n",
    "                break\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è Bloom filter already at capacity; skipping additions for this batch.\")\n",
    "\n",
    "    total_rows += len(df)\n",
    "    unique_rows += len(unique_df)\n",
    "    dup_rows += seen_mask.sum()\n",
    "\n",
    "    print(f\"   Rows: {len(df):,} | Unique: {len(unique_df):,} | Dups: {seen_mask.sum():,}\")\n",
    "    \n",
    "    # Write unique batch to local parquet (repartition to avoid huge tasks / OOM)\n",
    "    if not unique_df.empty:\n",
    "        sdf = spark.createDataFrame(unique_df).repartition(200)\n",
    "        sdf.write.mode(\"append\").parquet(local_tmp)\n",
    "        del sdf\n",
    "\n",
    "    del df, trip_ids, unique_df, seen_mask, keep_mask\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\n‚úÖ STREAMING DEDUP COMPLETE\")\n",
    "print(f\"   Total processed: {total_rows:,}\")\n",
    "print(f\"   Unique kept:     {unique_rows:,}\")\n",
    "print(f\"   Duplicates:      {dup_rows:,} ({dup_rows/total_rows*100:.2f}%)\")\n",
    "print(f\"   Time elapsed:    {time.time()-start:.2f}s\")\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# WRITE MERGED RESULT BACK TO S3\n",
    "# -----------------------------------------------------------------------\n",
    "output_s3 = f\"s3://{BUCKET_NAME}/{OUTPUT_PREFIX}\"\n",
    "print(f\"\\nüíæ Writing deduplicated data to {output_s3} ...\")\n",
    "\n",
    "df_final = spark.read.parquet(local_tmp)\n",
    "df_final.write.mode(\"overwrite\").option(\"compression\",\"snappy\").parquet(output_s3)\n",
    "\n",
    "print(f\"‚úÖ Wrote deduplicated stream data to: {output_s3}\")\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# SAVE UPDATED BLOOM FILTER (now includes 2025 trips)\n",
    "# -----------------------------------------------------------------------\n",
    "updated_path = f\"bloom_filter/bloom_all_years_plus2025.pkl\"\n",
    "with fs.open(f\"{BUCKET_NAME}/{updated_path}\", \"wb\") as f:\n",
    "    pickle.dump(bloom, f)\n",
    "print(f\"‚úÖ Updated Bloom filter uploaded to s3://{BUCKET_NAME}/{updated_path}\")\n",
    "\n",
    "spark.stop()\n",
    "print(\"üèÅ Spark session stopped\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922c1113-77d4-4f4f-99fa-b75399caba59",
   "metadata": {},
   "source": [
    "#check stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fc3ad00-01a9-4c45-a1a1-a2c59d36cefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/18 02:06:33 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: s3a://data228-bigdata-nyc/dedup/2025_stream_dedup.parquet.\n",
      "java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n",
      "\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2737)\n",
      "\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)\n",
      "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)\n",
      "\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)\n",
      "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)\n",
      "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:55)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:381)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)\n",
      "\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:457)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.parquet(DataFrameReader.scala:306)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n",
      "\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2641)\n",
      "\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2735)\n",
      "\t... 83 more\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o35.parquet.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2737)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:777)\n\tat scala.collection.immutable.List.map(List.scala:247)\n\tat scala.collection.immutable.List.map(List.scala:79)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:775)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:575)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:419)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:457)\n\tat org.apache.spark.sql.classic.DataFrameReader.parquet(DataFrameReader.scala:306)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n\t\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2737)\n\t\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)\n\t\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)\n\t\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)\n\t\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)\n\t\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)\n\t\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)\n\t\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)\n\t\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:777)\n\t\tat scala.collection.immutable.List.map(List.scala:247)\n\t\tat scala.collection.immutable.List.map(List.scala:79)\n\t\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:775)\n\t\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:575)\n\t\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:419)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n\t\tat scala.Option.getOrElse(Option.scala:201)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n\t\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\t\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\t\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n\t\tat scala.collection.immutable.List.foreach(List.scala:334)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)\n\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n\t\tat scala.util.Try$.apply(Try.scala:217)\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t\t... 22 more\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2641)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2735)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:777)\n\tat scala.collection.immutable.List.map(List.scala:247)\n\tat scala.collection.immutable.List.map(List.scala:79)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:775)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:575)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:419)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t... 22 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[32m      3\u001b[39m spark = SparkSession.builder.appName(\u001b[33m\"\u001b[39m\u001b[33mcheck-dedup-2025\u001b[39m\u001b[33m\"\u001b[39m).getOrCreate()\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m df = \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43ms3a://data228-bigdata-nyc/dedup/2025_stream_dedup.parquet\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(df.count())\n\u001b[32m      7\u001b[39m df.show(\u001b[32m5\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/venv/lib/python3.13/site-packages/pyspark/sql/readwriter.py:642\u001b[39m, in \u001b[36mDataFrameReader.parquet\u001b[39m\u001b[34m(self, *paths, **options)\u001b[39m\n\u001b[32m    631\u001b[39m int96RebaseMode = options.get(\u001b[33m\"\u001b[39m\u001b[33mint96RebaseMode\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    632\u001b[39m \u001b[38;5;28mself\u001b[39m._set_opts(\n\u001b[32m    633\u001b[39m     mergeSchema=mergeSchema,\n\u001b[32m    634\u001b[39m     pathGlobFilter=pathGlobFilter,\n\u001b[32m   (...)\u001b[39m\u001b[32m    639\u001b[39m     int96RebaseMode=int96RebaseMode,\n\u001b[32m    640\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m642\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_spark\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/venv/lib/python3.13/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/venv/lib/python3.13/site-packages/pyspark/errors/exceptions/captured.py:282\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpy4j\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    284\u001b[39m     converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/venv/lib/python3.13/site-packages/py4j/protocol.py:327\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    325\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    332\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    333\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o35.parquet.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2737)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:777)\n\tat scala.collection.immutable.List.map(List.scala:247)\n\tat scala.collection.immutable.List.map(List.scala:79)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:775)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:575)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:419)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:457)\n\tat org.apache.spark.sql.classic.DataFrameReader.parquet(DataFrameReader.scala:306)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n\t\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2737)\n\t\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)\n\t\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)\n\t\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)\n\t\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)\n\t\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)\n\t\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)\n\t\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)\n\t\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:777)\n\t\tat scala.collection.immutable.List.map(List.scala:247)\n\t\tat scala.collection.immutable.List.map(List.scala:79)\n\t\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:775)\n\t\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:575)\n\t\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:419)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n\t\tat scala.Option.getOrElse(Option.scala:201)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n\t\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\t\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\t\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n\t\tat scala.collection.immutable.List.foreach(List.scala:334)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)\n\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n\t\tat scala.util.Try$.apply(Try.scala:217)\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t\t... 22 more\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2641)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2735)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:777)\n\tat scala.collection.immutable.List.map(List.scala:247)\n\tat scala.collection.immutable.List.map(List.scala:79)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:775)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:575)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:419)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t... 22 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"check-dedup-2025\").getOrCreate()\n",
    "\n",
    "df = spark.read.parquet(\"s3a://data228-bigdata-nyc/dedup/2025_stream_dedup.parquet\")\n",
    "print(df.count())\n",
    "df.show(5)\n",
    "\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf1f6dd5-00cd-400f-ae7e-55d9ff2d2316",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vidushi/Desktop/venv/lib/python3.13/site-packages/fsspec/registry.py:298: UserWarning: Your installed version of s3fs is very old and known to cause\n",
      "severe performance issues, see also https://github.com/dask/dask/issues/10276\n",
      "\n",
      "To fix, you should specify a lower version bound on s3fs, or\n",
      "update the current installation.\n",
      "\n",
      "  warnings.warn(s3_msg)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "data228-bigdata-nyc/dedup/2025_stream_dedup.parquet",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m AWS_PROFILE = \u001b[33m\"\u001b[39m\u001b[33mdata228\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m path = \u001b[33m\"\u001b[39m\u001b[33ms3://data228-bigdata-nyc/dedup/2025_stream_dedup.parquet\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprofile\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mAWS_PROFILE\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRows in deduped 2025 dataset:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(df))\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(df.head())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/venv/lib/python3.13/site-packages/pandas/io/parquet.py:669\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[39m\n\u001b[32m    666\u001b[39m     use_nullable_dtypes = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    667\u001b[39m check_dtype_backend(dtype_backend)\n\u001b[32m--> \u001b[39m\u001b[32m669\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/venv/lib/python3.13/site-packages/pandas/io/parquet.py:265\u001b[39m, in \u001b[36mPyArrowImpl.read\u001b[39m\u001b[34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[39m\n\u001b[32m    258\u001b[39m path_or_handle, handles, filesystem = _get_path_or_handle(\n\u001b[32m    259\u001b[39m     path,\n\u001b[32m    260\u001b[39m     filesystem,\n\u001b[32m    261\u001b[39m     storage_options=storage_options,\n\u001b[32m    262\u001b[39m     mode=\u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    263\u001b[39m )\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m     pa_table = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    273\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m catch_warnings():\n\u001b[32m    274\u001b[39m         filterwarnings(\n\u001b[32m    275\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    276\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmake_block is deprecated\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    277\u001b[39m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[32m    278\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/venv/lib/python3.13/site-packages/pyarrow/parquet/core.py:1843\u001b[39m, in \u001b[36mread_table\u001b[39m\u001b[34m(source, columns, use_threads, schema, use_pandas_metadata, read_dictionary, binary_type, list_type, memory_map, buffer_size, partitioning, filesystem, filters, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification, arrow_extensions_enabled)\u001b[39m\n\u001b[32m   1831\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread_table\u001b[39m(source, *, columns=\u001b[38;5;28;01mNone\u001b[39;00m, use_threads=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   1832\u001b[39m                schema=\u001b[38;5;28;01mNone\u001b[39;00m, use_pandas_metadata=\u001b[38;5;28;01mFalse\u001b[39;00m, read_dictionary=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1833\u001b[39m                binary_type=\u001b[38;5;28;01mNone\u001b[39;00m, list_type=\u001b[38;5;28;01mNone\u001b[39;00m, memory_map=\u001b[38;5;28;01mFalse\u001b[39;00m, buffer_size=\u001b[32m0\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1839\u001b[39m                page_checksum_verification=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1840\u001b[39m                arrow_extensions_enabled=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m   1842\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1843\u001b[39m         dataset = \u001b[43mParquetDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1844\u001b[39m \u001b[43m            \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1845\u001b[39m \u001b[43m            \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1846\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1847\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1848\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1849\u001b[39m \u001b[43m            \u001b[49m\u001b[43mread_dictionary\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread_dictionary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1850\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbinary_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbinary_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1851\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlist_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlist_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1852\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1853\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1854\u001b[39m \u001b[43m            \u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1855\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpre_buffer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpre_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1856\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcoerce_int96_timestamp_unit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcoerce_int96_timestamp_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1857\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdecryption_properties\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecryption_properties\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1858\u001b[39m \u001b[43m            \u001b[49m\u001b[43mthrift_string_size_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthrift_string_size_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1859\u001b[39m \u001b[43m            \u001b[49m\u001b[43mthrift_container_size_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthrift_container_size_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1860\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpage_checksum_verification\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpage_checksum_verification\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1861\u001b[39m \u001b[43m            \u001b[49m\u001b[43marrow_extensions_enabled\u001b[49m\u001b[43m=\u001b[49m\u001b[43marrow_extensions_enabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1862\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1863\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m   1864\u001b[39m         \u001b[38;5;66;03m# fall back on ParquetFile for simple cases when pyarrow.dataset\u001b[39;00m\n\u001b[32m   1865\u001b[39m         \u001b[38;5;66;03m# module is not available\u001b[39;00m\n\u001b[32m   1866\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m filters \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/venv/lib/python3.13/site-packages/pyarrow/parquet/core.py:1424\u001b[39m, in \u001b[36mParquetDataset.__init__\u001b[39m\u001b[34m(self, path_or_paths, filesystem, schema, filters, read_dictionary, binary_type, list_type, memory_map, buffer_size, partitioning, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification, arrow_extensions_enabled)\u001b[39m\n\u001b[32m   1420\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m partitioning == \u001b[33m\"\u001b[39m\u001b[33mhive\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1421\u001b[39m     partitioning = ds.HivePartitioning.discover(\n\u001b[32m   1422\u001b[39m         infer_dictionary=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1424\u001b[39m \u001b[38;5;28mself\u001b[39m._dataset = \u001b[43mds\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1425\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mparquet_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1426\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1427\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/venv/lib/python3.13/site-packages/pyarrow/dataset.py:790\u001b[39m, in \u001b[36mdataset\u001b[39m\u001b[34m(source, schema, format, filesystem, partitioning, partition_base_dir, exclude_invalid_files, ignore_prefixes)\u001b[39m\n\u001b[32m    779\u001b[39m kwargs = \u001b[38;5;28mdict\u001b[39m(\n\u001b[32m    780\u001b[39m     schema=schema,\n\u001b[32m    781\u001b[39m     filesystem=filesystem,\n\u001b[32m   (...)\u001b[39m\u001b[32m    786\u001b[39m     selector_ignore_prefixes=ignore_prefixes\n\u001b[32m    787\u001b[39m )\n\u001b[32m    789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_path_like(source):\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_filesystem_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(source, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[32m    792\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(_is_path_like(elem) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, FileInfo) \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m source):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/venv/lib/python3.13/site-packages/pyarrow/dataset.py:472\u001b[39m, in \u001b[36m_filesystem_dataset\u001b[39m\u001b[34m(source, schema, filesystem, partitioning, format, partition_base_dir, exclude_invalid_files, selector_ignore_prefixes)\u001b[39m\n\u001b[32m    470\u001b[39m         fs, paths_or_selector = _ensure_multiple_sources(source, filesystem)\n\u001b[32m    471\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m     fs, paths_or_selector = \u001b[43m_ensure_single_source\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    474\u001b[39m options = FileSystemFactoryOptions(\n\u001b[32m    475\u001b[39m     partitioning=partitioning,\n\u001b[32m    476\u001b[39m     partition_base_dir=partition_base_dir,\n\u001b[32m    477\u001b[39m     exclude_invalid_files=exclude_invalid_files,\n\u001b[32m    478\u001b[39m     selector_ignore_prefixes=selector_ignore_prefixes\n\u001b[32m    479\u001b[39m )\n\u001b[32m    480\u001b[39m factory = FileSystemDatasetFactory(fs, paths_or_selector, \u001b[38;5;28mformat\u001b[39m, options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/venv/lib/python3.13/site-packages/pyarrow/dataset.py:437\u001b[39m, in \u001b[36m_ensure_single_source\u001b[39m\u001b[34m(path, filesystem)\u001b[39m\n\u001b[32m    435\u001b[39m     paths_or_selector = [path]\n\u001b[32m    436\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m437\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(path)\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m filesystem, paths_or_selector\n",
      "\u001b[31mFileNotFoundError\u001b[39m: data228-bigdata-nyc/dedup/2025_stream_dedup.parquet"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "AWS_PROFILE = \"data228\"\n",
    "path = \"s3://data228-bigdata-nyc/dedup/2025_stream_dedup.parquet\"\n",
    "\n",
    "df = pd.read_parquet(path, storage_options={\"profile\": AWS_PROFILE})\n",
    "print(\"Rows in deduped 2025 dataset:\", len(df))\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24098dbe-17b1-4d69-9f97-3c3f2542bacc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
