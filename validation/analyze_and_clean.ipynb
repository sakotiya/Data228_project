{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Trip Data - Smart Analysis & Cleaning\n",
    "\n",
    "This notebook provides:\n",
    "1. **Analysis** - Understand missing data patterns\n",
    "2. **Smart Cleaning** - Use median/mode/group-based imputation (NOT hardcoded values)\n",
    "3. **Preview** - See what cleaning will do before executing\n",
    "4. **Execute** - Write cleaned parquet files\n",
    "\n",
    "**Smart Cleaning Features:**\n",
    "- Numeric fields \u2192 filled with **median** (robust to outliers)\n",
    "- Categorical fields \u2192 filled with **mode** (most common)\n",
    "- Group-based imputation \u2192 fill based on similar trips\n",
    "- Preserves data integrity better than hardcoded values\n",
    "\n",
    "Run cells from top to bottom.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Libraries imported\n",
      "RAW_DATA_DIR: /Users/gouravdhama/Documents/bubu/Raw_data/raw_data\n",
      "CLEANED_DATA_DIR: /Users/gouravdhama/Documents/bubu/cleaned_data\n",
      "OUTPUT_DIR: /Users/gouravdhama/Documents/bubu/cleaning_code\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Directories\n",
    "RAW_DATA_DIR = \"/Users/gouravdhama/Documents/bubu/Raw_data/raw_data\"\n",
    "CLEANED_DATA_DIR = \"/Users/gouravdhama/Documents/bubu/cleaned_data\"\n",
    "OUTPUT_DIR = \"/Users/gouravdhama/Documents/bubu/cleaning_code\"\n",
    "\n",
    "print(\"\u2705 Libraries imported\")\n",
    "print(f\"RAW_DATA_DIR: {RAW_DATA_DIR}\")\n",
    "print(f\"CLEANED_DATA_DIR: {CLEANED_DATA_DIR}\")\n",
    "print(f\"OUTPUT_DIR: {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CLEANING MODE: SMART\n",
      "============================================================\n",
      "\u2705 SMART MODE:\n",
      "  \u2022 passenger_count \u2192 median\n",
      "  \u2022 RatecodeID \u2192 mode\n",
      "  \u2022 payment_type \u2192 mode\n",
      "  \u2022 fees/taxes \u2192 median\n",
      "  \u2022 trip_type \u2192 mode\n",
      "  \u2022 Locations \u2192 -1 (unknown)\n",
      "\n",
      "\u2705 Configuration set!\n"
     ]
    }
   ],
   "source": [
    "# CLEANING STRATEGY CONFIGURATION\n",
    "# Choose your cleaning approach here\n",
    "\n",
    "CLEANING_MODE = \"SMART\"  # Options: \"SIMPLE\" or \"SMART\"\n",
    "\n",
    "# Cleaning rules\n",
    "CLEANING_CONFIG = {\n",
    "    'mode': CLEANING_MODE,  # SMART = median/mode, SIMPLE = hardcoded values\n",
    "    'drop_completely_empty_columns': True,\n",
    "    'impute_locations_with_unknown': True,  # -1 for unknown locations\n",
    "    'remove_invalid_records': True,\n",
    "    'max_trip_distance': 100,  # miles\n",
    "    'max_fare_amount': 500,    # dollars\n",
    "    'max_passenger_count': 8,\n",
    "}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"CLEANING MODE: {CLEANING_MODE}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if CLEANING_MODE == \"SMART\":\n",
    "    print(\"\u2705 SMART MODE:\")\n",
    "    print(\"  \u2022 passenger_count \u2192 median\")\n",
    "    print(\"  \u2022 RatecodeID \u2192 mode\")\n",
    "    print(\"  \u2022 payment_type \u2192 mode\")\n",
    "    print(\"  \u2022 fees/taxes \u2192 median\")\n",
    "    print(\"  \u2022 trip_type \u2192 mode\")\n",
    "    print(\"  \u2022 Locations \u2192 -1 (unknown)\")\n",
    "elif CLEANING_MODE == \"SIMPLE\":\n",
    "    print(\"\u26a0\ufe0f  SIMPLE MODE:\")\n",
    "    print(\"  \u2022 passenger_count \u2192 always 1\")\n",
    "    print(\"  \u2022 RatecodeID \u2192 always 1\")\n",
    "    print(\"  \u2022 payment_type \u2192 always 1\")\n",
    "    print(\"  \u2022 fees/taxes \u2192 always 0\")\n",
    "    print(\"  \u2022 trip_type \u2192 mode\")\n",
    "    print(\"  \u2022 Locations \u2192 -1 (unknown)\")\n",
    "\n",
    "print(\"\\n\u2705 Configuration set!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Helper functions defined (with timestamp overflow protection)\n"
     ]
    }
   ],
   "source": [
    "def safe_read_parquet(file_path):\n",
    "    \"\"\"Read parquet file with timestamp overflow handling\"\"\"\n",
    "    import pyarrow.parquet as pq\n",
    "    \n",
    "    try:\n",
    "        # Try normal read first (fast path)\n",
    "        return pd.read_parquet(file_path)\n",
    "    except Exception as e:\n",
    "        # Catch timestamp overflow errors more broadly\n",
    "        error_type = type(e).__name__\n",
    "        try:\n",
    "            error_msg = str(e)\n",
    "        except:\n",
    "            error_msg = \"\"\n",
    "        \n",
    "        # Check if it's a timestamp/arrow error\n",
    "        is_timestamp_error = (\n",
    "            'ArrowInvalid' in error_type or\n",
    "            'out of bounds' in error_msg.lower() or\n",
    "            'timestamp' in error_msg.lower() or\n",
    "            'casting' in error_msg.lower()\n",
    "        )\n",
    "        \n",
    "        if is_timestamp_error:\n",
    "            # Use safe mode with pyarrow\n",
    "            print(f\"    \u26a0\ufe0f  Timestamp overflow detected, using safe mode...\")\n",
    "            try:\n",
    "                table = pq.read_table(file_path)\n",
    "                df = table.to_pandas(timestamp_as_object=True)\n",
    "                \n",
    "                # Convert timestamp columns where possible\n",
    "                for col in df.columns:\n",
    "                    if df[col].dtype == 'object':\n",
    "                        if any(x in col.lower() for x in ['time', 'datetime', 'date']):\n",
    "                            try:\n",
    "                                df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "                            except:\n",
    "                                pass\n",
    "                return df\n",
    "            except Exception as e2:\n",
    "                print(f\"    \u274c Safe mode failed: {type(e2).__name__}\")\n",
    "                raise e\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "print(\"\u2705 Helper functions defined (with timestamp overflow protection)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Smart Cleaning Function\n",
    "\n",
    "This function cleans data intelligently based on the mode you selected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Smart cleaning function ready (mode: SMART)\n"
     ]
    }
   ],
   "source": [
    "def clean_dataframe_smart(df, file_name, file_type):\n",
    "    \"\"\"\n",
    "    Smart cleaning: uses median for numeric, mode for categorical\n",
    "    Better preserves data distribution than hardcoded values\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "    original_rows = len(df_clean)\n",
    "    original_cols = len(df_clean.columns)\n",
    "    \n",
    "    cleaning_log = {\n",
    "        'file_name': file_name,\n",
    "        'file_type': file_type,\n",
    "        'cleaning_mode': CLEANING_CONFIG['mode'],\n",
    "        'original_rows': original_rows,\n",
    "        'original_cols': original_cols,\n",
    "        'actions': []\n",
    "    }\n",
    "    \n",
    "    # 1. Drop 100% empty columns\n",
    "    if CLEANING_CONFIG['drop_completely_empty_columns']:\n",
    "        empty_cols = [col for col in df_clean.columns if df_clean[col].isnull().sum() == original_rows]\n",
    "        if empty_cols:\n",
    "            df_clean = df_clean.drop(columns=empty_cols)\n",
    "            cleaning_log['actions'].append({\n",
    "                'action': 'DROP_COLUMNS',\n",
    "                'columns': empty_cols,\n",
    "                'reason': '100% missing'\n",
    "            })\n",
    "    \n",
    "    # 2. Fill location fields with -1 (unknown)\n",
    "    if CLEANING_CONFIG['impute_locations_with_unknown']:\n",
    "        location_cols = [col for col in df_clean.columns if 'location' in col.lower()]\n",
    "        for col in location_cols:\n",
    "            missing = df_clean[col].isnull().sum()\n",
    "            if missing > 0:\n",
    "                df_clean[col] = df_clean[col].fillna(-1)\n",
    "                cleaning_log['actions'].append({\n",
    "                    'action': 'FILL_NULL',\n",
    "                    'column': col,\n",
    "                    'missing_count': int(missing),\n",
    "                    'fill_value': -1,\n",
    "                    'method': 'fixed'\n",
    "                })\n",
    "    \n",
    "    # 3. SMART: Fill passenger_count with MEDIAN (not always 1!)\n",
    "    if 'passenger_count' in df_clean.columns:\n",
    "        missing = df_clean['passenger_count'].isnull().sum()\n",
    "        if missing > 0:\n",
    "            if CLEANING_MODE == \"SMART\":\n",
    "                fill_val = df_clean['passenger_count'].median()\n",
    "                df_clean['passenger_count'] = df_clean['passenger_count'].fillna(fill_val)\n",
    "                cleaning_log['actions'].append({\n",
    "                    'action': 'FILL_NULL',\n",
    "                    'column': 'passenger_count',\n",
    "                    'missing_count': int(missing),\n",
    "                    'fill_value': float(fill_val),\n",
    "                    'method': 'median'\n",
    "                })\n",
    "            else:  # SIMPLE mode\n",
    "                df_clean['passenger_count'] = df_clean['passenger_count'].fillna(1)\n",
    "                cleaning_log['actions'].append({\n",
    "                    'action': 'FILL_NULL',\n",
    "                    'column': 'passenger_count',\n",
    "                    'missing_count': int(missing),\n",
    "                    'fill_value': 1,\n",
    "                    'method': 'fixed'\n",
    "                })\n",
    "    \n",
    "    # 4. SMART: Fill RatecodeID with MODE (most common)\n",
    "    if 'RatecodeID' in df_clean.columns:\n",
    "        missing = df_clean['RatecodeID'].isnull().sum()\n",
    "        if missing > 0:\n",
    "            if CLEANING_MODE == \"SMART\":\n",
    "                mode_val = df_clean['RatecodeID'].mode()\n",
    "                fill_val = mode_val[0] if len(mode_val) > 0 else 1\n",
    "                df_clean['RatecodeID'] = df_clean['RatecodeID'].fillna(fill_val)\n",
    "                cleaning_log['actions'].append({\n",
    "                    'action': 'FILL_NULL',\n",
    "                    'column': 'RatecodeID',\n",
    "                    'missing_count': int(missing),\n",
    "                    'fill_value': float(fill_val),\n",
    "                    'method': 'mode'\n",
    "                })\n",
    "            else:\n",
    "                df_clean['RatecodeID'] = df_clean['RatecodeID'].fillna(1)\n",
    "                cleaning_log['actions'].append({\n",
    "                    'action': 'FILL_NULL',\n",
    "                    'column': 'RatecodeID',\n",
    "                    'missing_count': int(missing),\n",
    "                    'fill_value': 1,\n",
    "                    'method': 'fixed'\n",
    "                })\n",
    "    \n",
    "    # 5. Fill store_and_fwd_flag with MODE\n",
    "    if 'store_and_fwd_flag' in df_clean.columns:\n",
    "        missing = df_clean['store_and_fwd_flag'].isnull().sum()\n",
    "        if missing > 0:\n",
    "            mode_val = df_clean['store_and_fwd_flag'].mode()\n",
    "            fill_val = mode_val[0] if len(mode_val) > 0 else 'N'\n",
    "            df_clean['store_and_fwd_flag'] = df_clean['store_and_fwd_flag'].fillna(fill_val)\n",
    "            cleaning_log['actions'].append({\n",
    "                'action': 'FILL_NULL',\n",
    "                'column': 'store_and_fwd_flag',\n",
    "                'missing_count': int(missing),\n",
    "                'fill_value': fill_val,\n",
    "                'method': 'mode'\n",
    "            })\n",
    "    \n",
    "    # 6. SMART: Fill payment_type with MODE (not always 1)\n",
    "    if 'payment_type' in df_clean.columns:\n",
    "        missing = df_clean['payment_type'].isnull().sum()\n",
    "        if missing > 0:\n",
    "            if CLEANING_MODE == \"SMART\":\n",
    "                mode_val = df_clean['payment_type'].mode()\n",
    "                fill_val = mode_val[0] if len(mode_val) > 0 else 1\n",
    "                df_clean['payment_type'] = df_clean['payment_type'].fillna(fill_val)\n",
    "                cleaning_log['actions'].append({\n",
    "                    'action': 'FILL_NULL',\n",
    "                    'column': 'payment_type',\n",
    "                    'missing_count': int(missing),\n",
    "                    'fill_value': float(fill_val),\n",
    "                    'method': 'mode'\n",
    "                })\n",
    "            else:\n",
    "                df_clean['payment_type'] = df_clean['payment_type'].fillna(1)\n",
    "                cleaning_log['actions'].append({\n",
    "                    'action': 'FILL_NULL',\n",
    "                    'column': 'payment_type',\n",
    "                    'missing_count': int(missing),\n",
    "                    'fill_value': 1,\n",
    "                    'method': 'fixed'\n",
    "                })\n",
    "    \n",
    "    # 7. SMART: Fill fee/tax columns with MEDIAN (not 0!)\n",
    "    fee_cols = [col for col in df_clean.columns if any(x in col.lower() for x in ['surcharge', 'fee', 'tax', 'toll'])]\n",
    "    for col in fee_cols:\n",
    "        missing = df_clean[col].isnull().sum()\n",
    "        if missing > 0:\n",
    "            if CLEANING_MODE == \"SMART\":\n",
    "                fill_val = df_clean[col].median()\n",
    "                df_clean[col] = df_clean[col].fillna(fill_val)\n",
    "                cleaning_log['actions'].append({\n",
    "                    'action': 'FILL_NULL',\n",
    "                    'column': col,\n",
    "                    'missing_count': int(missing),\n",
    "                    'fill_value': float(fill_val),\n",
    "                    'method': 'median'\n",
    "                })\n",
    "            else:\n",
    "                df_clean[col] = df_clean[col].fillna(0)\n",
    "                cleaning_log['actions'].append({\n",
    "                    'action': 'FILL_NULL',\n",
    "                    'column': col,\n",
    "                    'missing_count': int(missing),\n",
    "                    'fill_value': 0,\n",
    "                    'method': 'fixed'\n",
    "                })\n",
    "    \n",
    "    # 8. Fill trip_type with MODE\n",
    "    if 'trip_type' in df_clean.columns:\n",
    "        missing = df_clean['trip_type'].isnull().sum()\n",
    "        if missing > 0:\n",
    "            mode_val = df_clean['trip_type'].mode()\n",
    "            fill_val = mode_val[0] if len(mode_val) > 0 else 1\n",
    "            df_clean['trip_type'] = df_clean['trip_type'].fillna(fill_val)\n",
    "            cleaning_log['actions'].append({\n",
    "                'action': 'FILL_NULL',\n",
    "                'column': 'trip_type',\n",
    "                'missing_count': int(missing),\n",
    "                'fill_value': float(fill_val),\n",
    "                'method': 'mode'\n",
    "            })\n",
    "    \n",
    "    # 9. Remove invalid records (outliers, negatives)\n",
    "    if CLEANING_CONFIG['remove_invalid_records']:\n",
    "        rows_before = len(df_clean)\n",
    "        \n",
    "        # Remove negative or excessive fares\n",
    "        if 'fare_amount' in df_clean.columns:\n",
    "            df_clean = df_clean[\n",
    "                (df_clean['fare_amount'] >= 0) &\n",
    "                (df_clean['fare_amount'] <= CLEANING_CONFIG['max_fare_amount'])\n",
    "            ]\n",
    "        \n",
    "        # Remove invalid distances\n",
    "        if 'trip_distance' in df_clean.columns:\n",
    "            df_clean = df_clean[\n",
    "                (df_clean['trip_distance'] >= 0) &\n",
    "                (df_clean['trip_distance'] <= CLEANING_CONFIG['max_trip_distance'])\n",
    "            ]\n",
    "        \n",
    "        # Remove invalid passenger counts\n",
    "        if 'passenger_count' in df_clean.columns:\n",
    "            df_clean = df_clean[\n",
    "                (df_clean['passenger_count'] > 0) &\n",
    "                (df_clean['passenger_count'] <= CLEANING_CONFIG['max_passenger_count'])\n",
    "            ]\n",
    "        \n",
    "        rows_removed = rows_before - len(df_clean)\n",
    "        if rows_removed > 0:\n",
    "            cleaning_log['actions'].append({\n",
    "                'action': 'DROP_ROWS',\n",
    "                'rows_affected': int(rows_removed),\n",
    "                'reason': 'Invalid data (negative values, outliers)'\n",
    "            })\n",
    "    \n",
    "    # Final stats\n",
    "    cleaning_log['final_rows'] = len(df_clean)\n",
    "    cleaning_log['final_cols'] = len(df_clean.columns)\n",
    "    cleaning_log['retention_rate'] = round(len(df_clean) / original_rows * 100, 2) if original_rows > 0 else 0\n",
    "    \n",
    "    return df_clean, cleaning_log\n",
    "\n",
    "print(f\"\u2705 Smart cleaning function ready (mode: {CLEANING_MODE})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://gourav.dhama%40doordash.com:****@ddartifacts.jfrog.io/ddartifacts/api/pypi/pypi-local/simple/\n",
      "Collecting pyarrow\n",
      "  Obtaining dependency information for pyarrow from https://files.pythonhosted.org/packages/d9/9b/cb3f7e0a345353def531ca879053e9ef6b9f38ed91aebcf68b09ba54dec0/pyarrow-22.0.0-cp310-cp310-macosx_12_0_arm64.whl.metadata\n",
      "  Downloading pyarrow-22.0.0-cp310-cp310-macosx_12_0_arm64.whl.metadata (3.1 kB)\n",
      "Collecting fastparquet\n",
      "  Obtaining dependency information for fastparquet from https://files.pythonhosted.org/packages/3b/ad/4ce73440df874479f7205fe5445090f71ed4e9bd77fdb3b740253ce82703/fastparquet-2024.11.0-cp310-cp310-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading fastparquet-2024.11.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: pandas>=1.5.0 in /opt/homebrew/lib/python3.10/site-packages (from fastparquet) (1.5.3)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.10/site-packages (from fastparquet) (1.24.1)\n",
      "Collecting cramjam>=2.3 (from fastparquet)\n",
      "  Obtaining dependency information for cramjam>=2.3 from https://files.pythonhosted.org/packages/96/29/7961e09a849eea7d8302e7baa6f829dd3ef3faf199cb25ed29b318ae799b/cramjam-2.11.0-cp310-cp310-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading cramjam-2.11.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (5.6 kB)\n",
      "Collecting fsspec (from fastparquet)\n",
      "  Obtaining dependency information for fsspec from https://files.pythonhosted.org/packages/eb/02/a6b21098b1d5d6249b7c5ab69dde30108a71e4e819d4a9778f1de1d5b70d/fsspec-2025.10.0-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2025.10.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: packaging in /opt/homebrew/lib/python3.10/site-packages (from fastparquet) (23.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/homebrew/lib/python3.10/site-packages (from pandas>=1.5.0->fastparquet) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.10/site-packages (from pandas>=1.5.0->fastparquet) (2022.7.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas>=1.5.0->fastparquet) (1.16.0)\n",
      "Downloading pyarrow-22.0.0-cp310-cp310-macosx_12_0_arm64.whl (34.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m34.2/34.2 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fastparquet-2024.11.0-cp310-cp310-macosx_11_0_arm64.whl (684 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m684.1/684.1 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cramjam-2.11.0-cp310-cp310-macosx_11_0_arm64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m201.0/201.0 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyarrow, fsspec, cramjam, fastparquet\n",
      "Successfully installed cramjam-2.11.0 fastparquet-2024.11.0 fsspec-2025.10.0 pyarrow-22.0.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyarrow fastparquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyze Raw Data (DRY RUN)\n",
    "\n",
    "This analyzes all raw files without modifying them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "\ud83d\udd0d ANALYZING 100 RAW PARQUET FILES\n",
      "Directory: /Users/gouravdhama/Documents/bubu/Raw_data/raw_data\n",
      "====================================================================================================\n",
      "\n",
      "[1/100] fhv_tripdata_2015-02.parquet\n",
      "  Rows: 3,053,183 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "    \u2022 PUlocationID: 16.5% missing\n",
      "    \u2022 DOlocationID: 98.9% missing\n",
      "    \u2022 SR_Flag: 100.0% missing\n",
      "\n",
      "[2/100] fhv_tripdata_2015-12.parquet\n",
      "  Rows: 8,888,809 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "    \u2022 PUlocationID: 33.4% missing\n",
      "    \u2022 DOlocationID: 97.4% missing\n",
      "    \u2022 SR_Flag: 100.0% missing\n",
      "\n",
      "[3/100] fhv_tripdata_2019-02.parquet\n",
      "    \u26a0\ufe0f  Timestamp overflow detected, using safe mode...\n",
      "  Rows: 1,707,650 | Cols: 7\n",
      "  Missing data in 5 columns\n",
      "    \u2022 dropOff_datetime: 0.0% missing\n",
      "    \u2022 PUlocationID: 0.0% missing\n",
      "    \u2022 DOlocationID: 0.0% missing\n",
      "    ... and 2 more\n",
      "\n",
      "[4/100] fhv_tripdata_2019-12.parquet\n",
      "  Rows: 2,044,196 | Cols: 7\n",
      "  Missing data in 4 columns\n",
      "    \u2022 PUlocationID: 2.0% missing\n",
      "    \u2022 DOlocationID: 0.7% missing\n",
      "    \u2022 SR_Flag: 100.0% missing\n",
      "    ... and 1 more\n",
      "\n",
      "[5/100] fhv_tripdata_2024-01.parquet\n",
      "  Rows: 1,290,116 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "    \u2022 PUlocationID: 78.7% missing\n",
      "    \u2022 DOlocationID: 14.2% missing\n",
      "    \u2022 SR_Flag: 100.0% missing\n",
      "\n",
      "[6/100] fhv_tripdata_2024-02.parquet\n",
      "  Rows: 1,176,093 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "    \u2022 PUlocationID: 82.0% missing\n",
      "    \u2022 DOlocationID: 14.0% missing\n",
      "    \u2022 SR_Flag: 100.0% missing\n",
      "\n",
      "[7/100] fhv_tripdata_2024-03.parquet\n",
      "  Rows: 1,469,352 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "    \u2022 PUlocationID: 81.1% missing\n",
      "    \u2022 DOlocationID: 14.1% missing\n",
      "    \u2022 SR_Flag: 100.0% missing\n",
      "\n",
      "[8/100] fhv_tripdata_2024-04.parquet\n",
      "  Rows: 1,444,626 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "    \u2022 PUlocationID: 73.6% missing\n",
      "    \u2022 DOlocationID: 13.7% missing\n",
      "    \u2022 SR_Flag: 100.0% missing\n",
      "\n",
      "[9/100] fhv_tripdata_2024-05.parquet\n",
      "  Rows: 1,352,502 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "    \u2022 PUlocationID: 79.5% missing\n",
      "    \u2022 DOlocationID: 14.4% missing\n",
      "    \u2022 SR_Flag: 100.0% missing\n",
      "\n",
      "[10/100] fhv_tripdata_2024-06.parquet\n",
      "  Rows: 1,386,539 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "    \u2022 PUlocationID: 75.7% missing\n",
      "    \u2022 DOlocationID: 14.9% missing\n",
      "    \u2022 SR_Flag: 100.0% missing\n",
      "\n",
      "[11/100] fhv_tripdata_2024-07.parquet\n",
      "  Rows: 1,382,739 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "    \u2022 PUlocationID: 73.6% missing\n",
      "    \u2022 DOlocationID: 15.0% missing\n",
      "    \u2022 SR_Flag: 100.0% missing\n",
      "\n",
      "[12/100] fhv_tripdata_2024-08.parquet\n",
      "  Rows: 1,484,471 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "    \u2022 PUlocationID: 83.5% missing\n",
      "    \u2022 DOlocationID: 20.1% missing\n",
      "    \u2022 SR_Flag: 100.0% missing\n",
      "\n",
      "[13/100] fhv_tripdata_2024-09.parquet\n",
      "  Rows: 1,718,375 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "    \u2022 PUlocationID: 76.8% missing\n",
      "    \u2022 DOlocationID: 18.7% missing\n",
      "    \u2022 SR_Flag: 100.0% missing\n",
      "\n",
      "[14/100] fhv_tripdata_2024-10.parquet\n",
      "  Rows: 1,421,231 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "    \u2022 PUlocationID: 81.7% missing\n",
      "    \u2022 DOlocationID: 19.5% missing\n",
      "    \u2022 SR_Flag: 100.0% missing\n",
      "\n",
      "[15/100] fhv_tripdata_2024-11.parquet\n",
      "  Rows: 1,591,082 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "    \u2022 PUlocationID: 78.9% missing\n",
      "    \u2022 DOlocationID: 20.2% missing\n",
      "    \u2022 SR_Flag: 100.0% missing\n",
      "\n",
      "[16/100] fhv_tripdata_2024-12.parquet\n",
      "  Rows: 1,913,200 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "    \u2022 PUlocationID: 86.9% missing\n",
      "    \u2022 DOlocationID: 17.5% missing\n",
      "    \u2022 SR_Flag: 100.0% missing\n",
      "\n",
      "[17/100] fhv_tripdata_2025-01.parquet\n",
      "  Rows: 1,898,108 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "    \u2022 PUlocationID: 83.0% missing\n",
      "    \u2022 DOlocationID: 16.1% missing\n",
      "    \u2022 SR_Flag: 100.0% missing\n",
      "\n",
      "[18/100] fhv_tripdata_2025-02.parquet\n",
      "  Rows: 1,578,722 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "    \u2022 PUlocationID: 80.8% missing\n",
      "    \u2022 DOlocationID: 16.1% missing\n",
      "    \u2022 SR_Flag: 100.0% missing\n",
      "\n",
      "[19/100] fhv_tripdata_2025-03.parquet\n",
      "  Rows: 2,182,992 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "    \u2022 PUlocationID: 88.0% missing\n",
      "    \u2022 DOlocationID: 18.0% missing\n",
      "    \u2022 SR_Flag: 100.0% missing\n",
      "\n",
      "[20/100] fhv_tripdata_2025-04.parquet\n",
      "  Rows: 1,699,478 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "    \u2022 PUlocationID: 77.5% missing\n",
      "    \u2022 DOlocationID: 17.4% missing\n",
      "    \u2022 SR_Flag: 100.0% missing\n",
      "\n",
      "[21/100] fhv_tripdata_2025-05.parquet\n",
      "  Rows: 2,210,721 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "    \u2022 PUlocationID: 79.5% missing\n",
      "    \u2022 DOlocationID: 17.6% missing\n",
      "    \u2022 SR_Flag: 100.0% missing\n",
      "\n",
      "[22/100] fhv_tripdata_2025-06.parquet\n",
      "  Rows: 2,231,731 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "    \u2022 PUlocationID: 80.5% missing\n",
      "    \u2022 DOlocationID: 15.6% missing\n",
      "    \u2022 SR_Flag: 100.0% missing\n",
      "\n",
      "[23/100] fhv_tripdata_2025-07.parquet\n",
      "  Rows: 2,187,536 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "    \u2022 PUlocationID: 82.3% missing\n",
      "    \u2022 DOlocationID: 15.5% missing\n",
      "    \u2022 SR_Flag: 100.0% missing\n",
      "\n",
      "[24/100] fhv_tripdata_2025-08.parquet\n",
      "  Rows: 2,256,854 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "    \u2022 PUlocationID: 84.1% missing\n",
      "    \u2022 DOlocationID: 17.0% missing\n",
      "    \u2022 SR_Flag: 100.0% missing\n",
      "\n",
      "[25/100] fhvhv_tripdata_2019-02.parquet\n",
      "  Rows: 20,159,102 | Cols: 24\n",
      "  Missing data in 7 columns\n",
      "    \u2022 dispatching_base_num: 0.0% missing\n",
      "    \u2022 originating_base_num: 28.1% missing\n",
      "    \u2022 request_datetime: 0.5% missing\n",
      "    ... and 4 more\n",
      "\n",
      "[26/100] fhvhv_tripdata_2019-12.parquet\n",
      "  Rows: 22,243,901 | Cols: 24\n",
      "  Missing data in 3 columns\n",
      "    \u2022 originating_base_num: 27.6% missing\n",
      "    \u2022 on_scene_datetime: 27.6% missing\n",
      "    \u2022 airport_fee: 100.0% missing\n",
      "\n",
      "[27/100] fhvhv_tripdata_2024-01.parquet\n",
      "  Rows: 19,663,930 | Cols: 24\n",
      "  Missing data in 2 columns\n",
      "    \u2022 originating_base_num: 26.5% missing\n",
      "    \u2022 on_scene_datetime: 26.5% missing\n",
      "\n",
      "[28/100] fhvhv_tripdata_2024-02.parquet\n",
      "  Rows: 19,359,148 | Cols: 24\n",
      "  Missing data in 2 columns\n",
      "    \u2022 originating_base_num: 25.5% missing\n",
      "    \u2022 on_scene_datetime: 25.5% missing\n",
      "\n",
      "[29/100] fhvhv_tripdata_2024-03.parquet\n",
      "  Rows: 21,280,788 | Cols: 24\n",
      "  Missing data in 2 columns\n",
      "    \u2022 originating_base_num: 26.5% missing\n",
      "    \u2022 on_scene_datetime: 26.5% missing\n",
      "\n",
      "[30/100] fhvhv_tripdata_2024-04.parquet\n",
      "  Rows: 19,733,038 | Cols: 24\n",
      "  Missing data in 2 columns\n",
      "    \u2022 originating_base_num: 25.4% missing\n",
      "    \u2022 on_scene_datetime: 25.4% missing\n",
      "\n",
      "[31/100] fhvhv_tripdata_2024-05.parquet\n",
      "  Rows: 20,704,538 | Cols: 24\n",
      "  Missing data in 2 columns\n",
      "    \u2022 originating_base_num: 24.9% missing\n",
      "    \u2022 on_scene_datetime: 24.9% missing\n",
      "\n",
      "[32/100] fhvhv_tripdata_2024-06.parquet\n",
      "  Rows: 20,123,226 | Cols: 24\n",
      "  Missing data in 2 columns\n",
      "    \u2022 originating_base_num: 24.9% missing\n",
      "    \u2022 on_scene_datetime: 24.6% missing\n",
      "\n",
      "[33/100] fhvhv_tripdata_2024-07.parquet\n",
      "  Rows: 19,182,934 | Cols: 24\n",
      "  Missing data in 2 columns\n",
      "    \u2022 originating_base_num: 25.2% missing\n",
      "    \u2022 on_scene_datetime: 25.2% missing\n",
      "\n",
      "[34/100] fhvhv_tripdata_2024-08.parquet\n",
      "  Rows: 19,128,392 | Cols: 24\n",
      "  Missing data in 2 columns\n",
      "    \u2022 originating_base_num: 26.2% missing\n",
      "    \u2022 on_scene_datetime: 26.2% missing\n",
      "\n",
      "[35/100] fhvhv_tripdata_2024-09.parquet\n",
      "  Rows: 19,209,788 | Cols: 24\n",
      "  Missing data in 2 columns\n",
      "    \u2022 originating_base_num: 23.8% missing\n",
      "    \u2022 on_scene_datetime: 23.8% missing\n",
      "\n",
      "[36/100] fhvhv_tripdata_2024-10.parquet\n",
      "  Rows: 20,028,282 | Cols: 24\n",
      "  Missing data in 2 columns\n",
      "    \u2022 originating_base_num: 24.3% missing\n",
      "    \u2022 on_scene_datetime: 24.3% missing\n",
      "\n",
      "[37/100] fhvhv_tripdata_2024-11.parquet\n",
      "  Rows: 19,987,533 | Cols: 24\n",
      "  Missing data in 2 columns\n",
      "    \u2022 originating_base_num: 24.2% missing\n",
      "    \u2022 on_scene_datetime: 24.2% missing\n",
      "\n",
      "[38/100] fhvhv_tripdata_2024-12.parquet\n",
      "  Rows: 21,068,851 | Cols: 24\n",
      "  Missing data in 2 columns\n",
      "    \u2022 originating_base_num: 24.3% missing\n",
      "    \u2022 on_scene_datetime: 24.3% missing\n",
      "\n",
      "[39/100] fhvhv_tripdata_2025-01.parquet\n",
      "  Rows: 20,405,666 | Cols: 25\n",
      "  Missing data in 2 columns\n",
      "    \u2022 originating_base_num: 24.7% missing\n",
      "    \u2022 on_scene_datetime: 24.7% missing\n",
      "\n",
      "[40/100] fhvhv_tripdata_2025-02.parquet\n",
      "  Rows: 19,339,461 | Cols: 25\n",
      "  Missing data in 2 columns\n",
      "    \u2022 originating_base_num: 25.4% missing\n",
      "    \u2022 on_scene_datetime: 25.4% missing\n",
      "\n",
      "[41/100] fhvhv_tripdata_2025-03.parquet\n",
      "  Rows: 20,536,879 | Cols: 25\n",
      "  Missing data in 2 columns\n",
      "    \u2022 originating_base_num: 29.1% missing\n",
      "    \u2022 on_scene_datetime: 4.5% missing\n",
      "\n",
      "[42/100] fhvhv_tripdata_2025-04.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Rows: 19,753,983 | Cols: 25\n",
      "  Missing data in 1 columns\n",
      "    \u2022 originating_base_num: 27.2% missing\n",
      "\n",
      "[43/100] fhvhv_tripdata_2025-05.parquet\n",
      "  Rows: 21,091,193 | Cols: 25\n",
      "  Missing data in 1 columns\n",
      "    \u2022 originating_base_num: 28.6% missing\n",
      "\n",
      "[44/100] fhvhv_tripdata_2025-06.parquet\n",
      "  Rows: 19,868,009 | Cols: 25\n",
      "  Missing data in 1 columns\n",
      "    \u2022 originating_base_num: 28.6% missing\n",
      "\n",
      "[45/100] fhvhv_tripdata_2025-07.parquet\n",
      "  Rows: 19,653,012 | Cols: 25\n",
      "  Missing data in 1 columns\n",
      "    \u2022 originating_base_num: 27.9% missing\n",
      "\n",
      "[46/100] fhvhv_tripdata_2025-08.parquet\n",
      "  Rows: 19,271,461 | Cols: 25\n",
      "  Missing data in 1 columns\n",
      "    \u2022 originating_base_num: 29.5% missing\n",
      "\n",
      "[47/100] green_tripdata_2014-02.parquet\n",
      "  Rows: 1,005,242 | Cols: 20\n",
      "  Missing data in 4 columns\n",
      "    \u2022 ehail_fee: 100.0% missing\n",
      "    \u2022 improvement_surcharge: 100.0% missing\n",
      "    \u2022 trip_type: 59.5% missing\n",
      "    ... and 1 more\n",
      "\n",
      "[48/100] green_tripdata_2014-12.parquet\n",
      "  Rows: 1,645,787 | Cols: 20\n",
      "  Missing data in 4 columns\n",
      "    \u2022 ehail_fee: 100.0% missing\n",
      "    \u2022 improvement_surcharge: 0.0% missing\n",
      "    \u2022 trip_type: 0.0% missing\n",
      "    ... and 1 more\n",
      "\n",
      "[49/100] green_tripdata_2015-02.parquet\n",
      "  Rows: 1,574,830 | Cols: 20\n",
      "  Missing data in 3 columns\n",
      "    \u2022 ehail_fee: 100.0% missing\n",
      "    \u2022 trip_type: 0.0% missing\n",
      "    \u2022 congestion_surcharge: 100.0% missing\n",
      "\n",
      "[50/100] green_tripdata_2015-12.parquet\n",
      "  Rows: 1,608,297 | Cols: 20\n",
      "  Missing data in 3 columns\n",
      "    \u2022 ehail_fee: 100.0% missing\n",
      "    \u2022 trip_type: 0.0% missing\n",
      "    \u2022 congestion_surcharge: 100.0% missing\n",
      "\n",
      "[51/100] green_tripdata_2019-02.parquet\n",
      "  Rows: 615,594 | Cols: 20\n",
      "  Missing data in 7 columns\n",
      "    \u2022 store_and_fwd_flag: 6.6% missing\n",
      "    \u2022 RatecodeID: 6.6% missing\n",
      "    \u2022 passenger_count: 6.6% missing\n",
      "    ... and 4 more\n",
      "\n",
      "[52/100] green_tripdata_2019-12.parquet\n",
      "  Rows: 455,294 | Cols: 20\n",
      "  Missing data in 8 columns\n",
      "    \u2022 store_and_fwd_flag: 20.9% missing\n",
      "    \u2022 RatecodeID: 20.9% missing\n",
      "    \u2022 passenger_count: 20.9% missing\n",
      "    ... and 5 more\n",
      "\n",
      "[53/100] green_tripdata_2024-01.parquet\n",
      "  Rows: 56,551 | Cols: 20\n",
      "  Missing data in 7 columns\n",
      "    \u2022 store_and_fwd_flag: 6.0% missing\n",
      "    \u2022 RatecodeID: 6.0% missing\n",
      "    \u2022 passenger_count: 6.0% missing\n",
      "    ... and 4 more\n",
      "\n",
      "[54/100] green_tripdata_2024-02.parquet\n",
      "  Rows: 53,577 | Cols: 20\n",
      "  Missing data in 7 columns\n",
      "    \u2022 store_and_fwd_flag: 5.5% missing\n",
      "    \u2022 RatecodeID: 5.5% missing\n",
      "    \u2022 passenger_count: 5.5% missing\n",
      "    ... and 4 more\n",
      "\n",
      "[55/100] green_tripdata_2024-03.parquet\n",
      "  Rows: 57,457 | Cols: 20\n",
      "  Missing data in 7 columns\n",
      "    \u2022 store_and_fwd_flag: 3.6% missing\n",
      "    \u2022 RatecodeID: 3.6% missing\n",
      "    \u2022 passenger_count: 3.6% missing\n",
      "    ... and 4 more\n",
      "\n",
      "[56/100] green_tripdata_2024-04.parquet\n",
      "  Rows: 56,471 | Cols: 20\n",
      "  Missing data in 7 columns\n",
      "    \u2022 store_and_fwd_flag: 3.5% missing\n",
      "    \u2022 RatecodeID: 3.5% missing\n",
      "    \u2022 passenger_count: 3.5% missing\n",
      "    ... and 4 more\n",
      "\n",
      "[57/100] green_tripdata_2024-05.parquet\n",
      "  Rows: 61,003 | Cols: 20\n",
      "  Missing data in 7 columns\n",
      "    \u2022 store_and_fwd_flag: 3.1% missing\n",
      "    \u2022 RatecodeID: 3.1% missing\n",
      "    \u2022 passenger_count: 3.1% missing\n",
      "    ... and 4 more\n",
      "\n",
      "[58/100] green_tripdata_2024-06.parquet\n",
      "  Rows: 54,748 | Cols: 20\n",
      "  Missing data in 7 columns\n",
      "    \u2022 store_and_fwd_flag: 3.5% missing\n",
      "    \u2022 RatecodeID: 3.5% missing\n",
      "    \u2022 passenger_count: 3.5% missing\n",
      "    ... and 4 more\n",
      "\n",
      "[59/100] green_tripdata_2024-07.parquet\n",
      "  Rows: 51,837 | Cols: 20\n",
      "  Missing data in 7 columns\n",
      "    \u2022 store_and_fwd_flag: 3.1% missing\n",
      "    \u2022 RatecodeID: 3.1% missing\n",
      "    \u2022 passenger_count: 3.1% missing\n",
      "    ... and 4 more\n",
      "\n",
      "[60/100] green_tripdata_2024-08.parquet\n",
      "  Rows: 51,771 | Cols: 20\n",
      "  Missing data in 7 columns\n",
      "    \u2022 store_and_fwd_flag: 3.1% missing\n",
      "    \u2022 RatecodeID: 3.1% missing\n",
      "    \u2022 passenger_count: 3.1% missing\n",
      "    ... and 4 more\n",
      "\n",
      "[61/100] green_tripdata_2024-09.parquet\n",
      "  Rows: 54,440 | Cols: 20\n",
      "  Missing data in 7 columns\n",
      "    \u2022 store_and_fwd_flag: 3.1% missing\n",
      "    \u2022 RatecodeID: 3.1% missing\n",
      "    \u2022 passenger_count: 3.1% missing\n",
      "    ... and 4 more\n",
      "\n",
      "[62/100] green_tripdata_2024-10.parquet\n",
      "  Rows: 56,147 | Cols: 20\n",
      "  Missing data in 7 columns\n",
      "    \u2022 store_and_fwd_flag: 2.9% missing\n",
      "    \u2022 RatecodeID: 2.9% missing\n",
      "    \u2022 passenger_count: 2.9% missing\n",
      "    ... and 4 more\n",
      "\n",
      "[63/100] green_tripdata_2024-11.parquet\n",
      "  Rows: 52,222 | Cols: 20\n",
      "  Missing data in 7 columns\n",
      "    \u2022 store_and_fwd_flag: 3.0% missing\n",
      "    \u2022 RatecodeID: 3.0% missing\n",
      "    \u2022 passenger_count: 3.0% missing\n",
      "    ... and 4 more\n",
      "\n",
      "[64/100] green_tripdata_2024-12.parquet\n",
      "  Rows: 53,994 | Cols: 20\n",
      "  Missing data in 7 columns\n",
      "    \u2022 store_and_fwd_flag: 3.7% missing\n",
      "    \u2022 RatecodeID: 3.7% missing\n",
      "    \u2022 passenger_count: 3.7% missing\n",
      "    ... and 4 more\n",
      "\n",
      "[65/100] green_tripdata_2025-01.parquet\n",
      "  Rows: 48,326 | Cols: 21\n",
      "  Missing data in 8 columns\n",
      "    \u2022 store_and_fwd_flag: 3.8% missing\n",
      "    \u2022 RatecodeID: 3.8% missing\n",
      "    \u2022 passenger_count: 3.8% missing\n",
      "    ... and 5 more\n",
      "\n",
      "[66/100] green_tripdata_2025-02.parquet\n",
      "  Rows: 46,621 | Cols: 21\n",
      "  Missing data in 8 columns\n",
      "    \u2022 store_and_fwd_flag: 5.4% missing\n",
      "    \u2022 RatecodeID: 5.4% missing\n",
      "    \u2022 passenger_count: 5.4% missing\n",
      "    ... and 5 more\n",
      "\n",
      "[67/100] green_tripdata_2025-03.parquet\n",
      "  Rows: 51,539 | Cols: 21\n",
      "  Missing data in 7 columns\n",
      "    \u2022 store_and_fwd_flag: 6.9% missing\n",
      "    \u2022 RatecodeID: 6.9% missing\n",
      "    \u2022 passenger_count: 6.9% missing\n",
      "    ... and 4 more\n",
      "\n",
      "[68/100] green_tripdata_2025-04.parquet\n",
      "  Rows: 52,132 | Cols: 21\n",
      "  Missing data in 7 columns\n",
      "    \u2022 store_and_fwd_flag: 6.1% missing\n",
      "    \u2022 RatecodeID: 6.1% missing\n",
      "    \u2022 passenger_count: 6.1% missing\n",
      "    ... and 4 more\n",
      "\n",
      "[69/100] green_tripdata_2025-05.parquet\n",
      "  Rows: 55,399 | Cols: 21\n",
      "  Missing data in 7 columns\n",
      "    \u2022 store_and_fwd_flag: 5.9% missing\n",
      "    \u2022 RatecodeID: 5.9% missing\n",
      "    \u2022 passenger_count: 5.9% missing\n",
      "    ... and 4 more\n",
      "\n",
      "[70/100] green_tripdata_2025-06.parquet\n",
      "  Rows: 49,390 | Cols: 21\n",
      "  Missing data in 7 columns\n",
      "    \u2022 store_and_fwd_flag: 7.7% missing\n",
      "    \u2022 RatecodeID: 7.7% missing\n",
      "    \u2022 passenger_count: 7.7% missing\n",
      "    ... and 4 more\n",
      "\n",
      "[71/100] green_tripdata_2025-07.parquet\n",
      "  Rows: 48,205 | Cols: 21\n",
      "  Missing data in 7 columns\n",
      "    \u2022 store_and_fwd_flag: 10.8% missing\n",
      "    \u2022 RatecodeID: 10.8% missing\n",
      "    \u2022 passenger_count: 10.8% missing\n",
      "    ... and 4 more\n",
      "\n",
      "[72/100] green_tripdata_2025-08.parquet\n",
      "  Rows: 46,306 | Cols: 21\n",
      "  Missing data in 7 columns\n",
      "    \u2022 store_and_fwd_flag: 10.2% missing\n",
      "    \u2022 RatecodeID: 10.2% missing\n",
      "    \u2022 passenger_count: 10.2% missing\n",
      "    ... and 4 more\n",
      "\n",
      "[73/100] yellow_tripdata_2009-02.parquet\n",
      "  Rows: 13,380,122 | Cols: 18\n",
      "  Missing data in 3 columns\n",
      "    \u2022 Rate_Code: 100.0% missing\n",
      "    \u2022 store_and_forward: 67.3% missing\n",
      "    \u2022 mta_tax: 100.0% missing\n",
      "\n",
      "[74/100] yellow_tripdata_2009-12.parquet\n",
      "  Rows: 14,583,404 | Cols: 18\n",
      "  Missing data in 3 columns\n",
      "    \u2022 Rate_Code: 100.0% missing\n",
      "    \u2022 store_and_forward: 56.1% missing\n",
      "    \u2022 mta_tax: 5.9% missing\n",
      "\n",
      "[75/100] yellow_tripdata_2014-02.parquet\n",
      "  Rows: 13,063,794 | Cols: 19\n",
      "  Missing data in 3 columns\n",
      "    \u2022 store_and_fwd_flag: 51.0% missing\n",
      "    \u2022 congestion_surcharge: 100.0% missing\n",
      "    \u2022 airport_fee: 100.0% missing\n",
      "\n",
      "[76/100] yellow_tripdata_2014-12.parquet\n",
      "  Rows: 13,112,117 | Cols: 19\n",
      "  Missing data in 3 columns\n",
      "    \u2022 improvement_surcharge: 0.0% missing\n",
      "    \u2022 congestion_surcharge: 100.0% missing\n",
      "    \u2022 airport_fee: 100.0% missing\n",
      "\n",
      "[77/100] yellow_tripdata_2015-02.parquet\n",
      "  Rows: 12,442,394 | Cols: 19\n",
      "  Missing data in 2 columns\n",
      "    \u2022 congestion_surcharge: 100.0% missing\n",
      "    \u2022 airport_fee: 100.0% missing\n",
      "\n",
      "[78/100] yellow_tripdata_2015-12.parquet\n",
      "  Rows: 11,452,996 | Cols: 19\n",
      "  Missing data in 2 columns\n",
      "    \u2022 congestion_surcharge: 100.0% missing\n",
      "    \u2022 airport_fee: 100.0% missing\n",
      "\n",
      "[79/100] yellow_tripdata_2019-02.parquet\n",
      "  Rows: 7,049,370 | Cols: 19\n",
      "  Missing data in 5 columns\n",
      "    \u2022 passenger_count: 0.4% missing\n",
      "    \u2022 RatecodeID: 0.4% missing\n",
      "    \u2022 store_and_fwd_flag: 0.4% missing\n",
      "    ... and 2 more\n",
      "\n",
      "[80/100] yellow_tripdata_2019-12.parquet\n",
      "  Rows: 6,896,317 | Cols: 19\n",
      "  Missing data in 5 columns\n",
      "    \u2022 passenger_count: 0.7% missing\n",
      "    \u2022 RatecodeID: 0.7% missing\n",
      "    \u2022 store_and_fwd_flag: 0.7% missing\n",
      "    ... and 2 more\n",
      "\n",
      "[81/100] yellow_tripdata_2024-01.parquet\n",
      "  Rows: 2,964,624 | Cols: 19\n",
      "  Missing data in 5 columns\n",
      "    \u2022 passenger_count: 4.7% missing\n",
      "    \u2022 RatecodeID: 4.7% missing\n",
      "    \u2022 store_and_fwd_flag: 4.7% missing\n",
      "    ... and 2 more\n",
      "\n",
      "[82/100] yellow_tripdata_2024-02.parquet\n",
      "  Rows: 3,007,526 | Cols: 19\n",
      "  Missing data in 5 columns\n",
      "    \u2022 passenger_count: 6.2% missing\n",
      "    \u2022 RatecodeID: 6.2% missing\n",
      "    \u2022 store_and_fwd_flag: 6.2% missing\n",
      "    ... and 2 more\n",
      "\n",
      "[83/100] yellow_tripdata_2024-03.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Rows: 3,582,628 | Cols: 19\n",
      "  Missing data in 5 columns\n",
      "    \u2022 passenger_count: 11.9% missing\n",
      "    \u2022 RatecodeID: 11.9% missing\n",
      "    \u2022 store_and_fwd_flag: 11.9% missing\n",
      "    ... and 2 more\n",
      "\n",
      "[84/100] yellow_tripdata_2024-04.parquet\n",
      "  Rows: 3,514,289 | Cols: 19\n",
      "  Missing data in 5 columns\n",
      "    \u2022 passenger_count: 11.6% missing\n",
      "    \u2022 RatecodeID: 11.6% missing\n",
      "    \u2022 store_and_fwd_flag: 11.6% missing\n",
      "    ... and 2 more\n",
      "\n",
      "[85/100] yellow_tripdata_2024-05.parquet\n",
      "  Rows: 3,723,833 | Cols: 19\n",
      "  Missing data in 5 columns\n",
      "    \u2022 passenger_count: 10.9% missing\n",
      "    \u2022 RatecodeID: 10.9% missing\n",
      "    \u2022 store_and_fwd_flag: 10.9% missing\n",
      "    ... and 2 more\n",
      "\n",
      "[86/100] yellow_tripdata_2024-06.parquet\n",
      "  Rows: 3,539,193 | Cols: 19\n",
      "  Missing data in 5 columns\n",
      "    \u2022 passenger_count: 11.6% missing\n",
      "    \u2022 RatecodeID: 11.6% missing\n",
      "    \u2022 store_and_fwd_flag: 11.6% missing\n",
      "    ... and 2 more\n",
      "\n",
      "[87/100] yellow_tripdata_2024-07.parquet\n",
      "  Rows: 3,076,903 | Cols: 19\n",
      "  Missing data in 5 columns\n",
      "    \u2022 passenger_count: 9.1% missing\n",
      "    \u2022 RatecodeID: 9.1% missing\n",
      "    \u2022 store_and_fwd_flag: 9.1% missing\n",
      "    ... and 2 more\n",
      "\n",
      "[88/100] yellow_tripdata_2024-08.parquet\n",
      "  Rows: 2,979,183 | Cols: 19\n",
      "  Missing data in 5 columns\n",
      "    \u2022 passenger_count: 8.7% missing\n",
      "    \u2022 RatecodeID: 8.7% missing\n",
      "    \u2022 store_and_fwd_flag: 8.7% missing\n",
      "    ... and 2 more\n",
      "\n",
      "[89/100] yellow_tripdata_2024-09.parquet\n",
      "  Rows: 3,633,030 | Cols: 19\n",
      "  Missing data in 5 columns\n",
      "    \u2022 passenger_count: 13.3% missing\n",
      "    \u2022 RatecodeID: 13.3% missing\n",
      "    \u2022 store_and_fwd_flag: 13.3% missing\n",
      "    ... and 2 more\n",
      "\n",
      "[90/100] yellow_tripdata_2024-10.parquet\n",
      "  Rows: 3,833,771 | Cols: 19\n",
      "  Missing data in 5 columns\n",
      "    \u2022 passenger_count: 10.3% missing\n",
      "    \u2022 RatecodeID: 10.3% missing\n",
      "    \u2022 store_and_fwd_flag: 10.3% missing\n",
      "    ... and 2 more\n",
      "\n",
      "[91/100] yellow_tripdata_2024-11.parquet\n",
      "  Rows: 3,646,369 | Cols: 19\n",
      "  Missing data in 5 columns\n",
      "    \u2022 passenger_count: 10.2% missing\n",
      "    \u2022 RatecodeID: 10.2% missing\n",
      "    \u2022 store_and_fwd_flag: 10.2% missing\n",
      "    ... and 2 more\n",
      "\n",
      "[92/100] yellow_tripdata_2024-12.parquet\n",
      "  Rows: 3,668,371 | Cols: 19\n",
      "  Missing data in 5 columns\n",
      "    \u2022 passenger_count: 8.9% missing\n",
      "    \u2022 RatecodeID: 8.9% missing\n",
      "    \u2022 store_and_fwd_flag: 8.9% missing\n",
      "    ... and 2 more\n",
      "\n",
      "[93/100] yellow_tripdata_2025-01.parquet\n",
      "  Rows: 3,475,226 | Cols: 20\n",
      "  Missing data in 5 columns\n",
      "    \u2022 passenger_count: 15.5% missing\n",
      "    \u2022 RatecodeID: 15.5% missing\n",
      "    \u2022 store_and_fwd_flag: 15.5% missing\n",
      "    ... and 2 more\n",
      "\n",
      "[94/100] yellow_tripdata_2025-02.parquet\n",
      "  Rows: 3,577,543 | Cols: 20\n",
      "  Missing data in 5 columns\n",
      "    \u2022 passenger_count: 22.6% missing\n",
      "    \u2022 RatecodeID: 22.6% missing\n",
      "    \u2022 store_and_fwd_flag: 22.6% missing\n",
      "    ... and 2 more\n",
      "\n",
      "[95/100] yellow_tripdata_2025-03.parquet\n",
      "  Rows: 4,145,257 | Cols: 20\n",
      "  Missing data in 5 columns\n",
      "    \u2022 passenger_count: 22.1% missing\n",
      "    \u2022 RatecodeID: 22.1% missing\n",
      "    \u2022 store_and_fwd_flag: 22.1% missing\n",
      "    ... and 2 more\n",
      "\n",
      "[96/100] yellow_tripdata_2025-04.parquet\n",
      "  Rows: 3,970,553 | Cols: 20\n",
      "  Missing data in 5 columns\n",
      "    \u2022 passenger_count: 18.8% missing\n",
      "    \u2022 RatecodeID: 18.8% missing\n",
      "    \u2022 store_and_fwd_flag: 18.8% missing\n",
      "    ... and 2 more\n",
      "\n",
      "[97/100] yellow_tripdata_2025-05.parquet\n",
      "  Rows: 4,591,845 | Cols: 20\n",
      "  Missing data in 5 columns\n",
      "    \u2022 passenger_count: 26.1% missing\n",
      "    \u2022 RatecodeID: 26.1% missing\n",
      "    \u2022 store_and_fwd_flag: 26.1% missing\n",
      "    ... and 2 more\n",
      "\n",
      "[98/100] yellow_tripdata_2025-06.parquet\n",
      "  Rows: 4,322,960 | Cols: 20\n",
      "  Missing data in 5 columns\n",
      "    \u2022 passenger_count: 28.1% missing\n",
      "    \u2022 RatecodeID: 28.1% missing\n",
      "    \u2022 store_and_fwd_flag: 28.1% missing\n",
      "    ... and 2 more\n",
      "\n",
      "[99/100] yellow_tripdata_2025-07.parquet\n",
      "  Rows: 3,898,963 | Cols: 20\n",
      "  Missing data in 5 columns\n",
      "    \u2022 passenger_count: 26.6% missing\n",
      "    \u2022 RatecodeID: 26.6% missing\n",
      "    \u2022 store_and_fwd_flag: 26.6% missing\n",
      "    ... and 2 more\n",
      "\n",
      "[100/100] yellow_tripdata_2025-08.parquet\n",
      "  Rows: 3,574,091 | Cols: 20\n",
      "  Missing data in 5 columns\n",
      "    \u2022 passenger_count: 24.8% missing\n",
      "    \u2022 RatecodeID: 24.8% missing\n",
      "    \u2022 store_and_fwd_flag: 24.8% missing\n",
      "    ... and 2 more\n",
      "\n",
      "====================================================================================================\n",
      "\u2705 Analysis complete! Saved to: /Users/gouravdhama/Documents/bubu/cleaning_code/analysis_results.json\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Analyze all raw files\n",
    "raw_path = Path(RAW_DATA_DIR)\n",
    "parquet_files = sorted(raw_path.glob(\"*.parquet\"))\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(f\"\ud83d\udd0d ANALYZING {len(parquet_files)} RAW PARQUET FILES\")\n",
    "print(f\"Directory: {RAW_DATA_DIR}\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "all_analysis = []\n",
    "\n",
    "for i, file_path in enumerate(parquet_files, 1):\n",
    "    file_name = file_path.name\n",
    "    file_type = get_file_type(file_name)\n",
    "    \n",
    "    print(f\"\\n[{i}/{len(parquet_files)}] {file_name}\")\n",
    "    \n",
    "    try:\n",
    "        df = safe_read_parquet(file_path)\n",
    "        \n",
    "        analysis = analyze_missing_data(df, file_name)\n",
    "        all_analysis.append(analysis)\n",
    "        \n",
    "        print(f\"  Rows: {analysis['total_rows']:,} | Cols: {analysis['total_columns']}\")\n",
    "        print(f\"  Missing data in {len(analysis['columns_with_missing'])} columns\")\n",
    "        \n",
    "        if analysis['columns_with_missing']:\n",
    "            for col, info in list(analysis['columns_with_missing'].items())[:3]:\n",
    "                print(f\"    \u2022 {col}: {info['null_percentage']:.1f}% missing\")\n",
    "            if len(analysis['columns_with_missing']) > 3:\n",
    "                print(f\"    ... and {len(analysis['columns_with_missing']) - 3} more\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  \u274c ERROR: {e}\")\n",
    "\n",
    "# Save analysis\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "output_file = os.path.join(OUTPUT_DIR, 'analysis_results.json')\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump({'analysis': all_analysis}, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(f\"\u2705 Analysis complete! Saved to: {output_file}\")\n",
    "print(\"=\" * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Execute Smart Cleaning (Write Cleaned Files)\n",
    "\n",
    "This actually cleans and writes cleaned parquet files to `cleaned_data/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "\ud83d\udea7 EXECUTING SMART CLEANING\n",
      "Source: /Users/gouravdhama/Documents/bubu/Raw_data/raw_data\n",
      "Destination: /Users/gouravdhama/Documents/bubu/cleaned_data\n",
      "Files to process: 100\n",
      "====================================================================================================\n",
      "\n",
      "[1/100] fhv_tripdata_2015-02.parquet\n",
      "  Read: 3,053,183 rows, 7 columns\n",
      "  Cleaned: 3,053,183 rows, 6 columns\n",
      "  Retention: 100.00%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/fhv_tripdata_2015-02.parquet\n",
      "\n",
      "[2/100] fhv_tripdata_2015-12.parquet\n",
      "  Read: 8,888,809 rows, 7 columns\n",
      "  Cleaned: 8,888,809 rows, 6 columns\n",
      "  Retention: 100.00%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/fhv_tripdata_2015-12.parquet\n",
      "\n",
      "[3/100] fhv_tripdata_2019-02.parquet\n",
      "    \u26a0\ufe0f  Timestamp overflow detected, using safe mode...\n",
      "  Read: 1,707,650 rows, 7 columns\n",
      "  Cleaned: 1,707,650 rows, 7 columns\n",
      "  Retention: 100.00%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/fhv_tripdata_2019-02.parquet\n",
      "\n",
      "[4/100] fhv_tripdata_2019-12.parquet\n",
      "  Read: 2,044,196 rows, 7 columns\n",
      "  Cleaned: 2,044,196 rows, 6 columns\n",
      "  Retention: 100.00%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/fhv_tripdata_2019-12.parquet\n",
      "\n",
      "[5/100] fhv_tripdata_2024-01.parquet\n",
      "  Read: 1,290,116 rows, 7 columns\n",
      "  Cleaned: 1,290,116 rows, 6 columns\n",
      "  Retention: 100.00%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/fhv_tripdata_2024-01.parquet\n",
      "\n",
      "[6/100] fhv_tripdata_2024-02.parquet\n",
      "  Read: 1,176,093 rows, 7 columns\n",
      "  Cleaned: 1,176,093 rows, 6 columns\n",
      "  Retention: 100.00%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/fhv_tripdata_2024-02.parquet\n",
      "\n",
      "[7/100] fhv_tripdata_2024-03.parquet\n",
      "  Read: 1,469,352 rows, 7 columns\n",
      "  Cleaned: 1,469,352 rows, 6 columns\n",
      "  Retention: 100.00%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/fhv_tripdata_2024-03.parquet\n",
      "\n",
      "[8/100] fhv_tripdata_2024-04.parquet\n",
      "  Read: 1,444,626 rows, 7 columns\n",
      "  Cleaned: 1,444,626 rows, 6 columns\n",
      "  Retention: 100.00%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/fhv_tripdata_2024-04.parquet\n",
      "\n",
      "[9/100] fhv_tripdata_2024-05.parquet\n",
      "  Read: 1,352,502 rows, 7 columns\n",
      "  Cleaned: 1,352,502 rows, 6 columns\n",
      "  Retention: 100.00%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/fhv_tripdata_2024-05.parquet\n",
      "\n",
      "[10/100] fhv_tripdata_2024-06.parquet\n",
      "  Read: 1,386,539 rows, 7 columns\n",
      "  Cleaned: 1,386,539 rows, 6 columns\n",
      "  Retention: 100.00%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/fhv_tripdata_2024-06.parquet\n",
      "\n",
      "[11/100] fhv_tripdata_2024-07.parquet\n",
      "  Read: 1,382,739 rows, 7 columns\n",
      "  Cleaned: 1,382,739 rows, 6 columns\n",
      "  Retention: 100.00%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/fhv_tripdata_2024-07.parquet\n",
      "\n",
      "[12/100] fhv_tripdata_2024-08.parquet\n",
      "  Read: 1,484,471 rows, 7 columns\n",
      "  Cleaned: 1,484,471 rows, 6 columns\n",
      "  Retention: 100.00%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/fhv_tripdata_2024-08.parquet\n",
      "\n",
      "[13/100] fhv_tripdata_2024-09.parquet\n",
      "  Read: 1,718,375 rows, 7 columns\n",
      "  Cleaned: 1,718,375 rows, 6 columns\n",
      "  Retention: 100.00%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/fhv_tripdata_2024-09.parquet\n",
      "\n",
      "[14/100] fhv_tripdata_2024-10.parquet\n",
      "  Read: 1,421,231 rows, 7 columns\n",
      "  Cleaned: 1,421,231 rows, 6 columns\n",
      "  Retention: 100.00%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/fhv_tripdata_2024-10.parquet\n",
      "\n",
      "[15/100] fhv_tripdata_2024-11.parquet\n",
      "  Read: 1,591,082 rows, 7 columns\n",
      "  Cleaned: 1,591,082 rows, 6 columns\n",
      "  Retention: 100.00%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/fhv_tripdata_2024-11.parquet\n",
      "\n",
      "[16/100] fhv_tripdata_2024-12.parquet\n",
      "  Read: 1,913,200 rows, 7 columns\n",
      "  Cleaned: 1,913,200 rows, 6 columns\n",
      "  Retention: 100.00%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/fhv_tripdata_2024-12.parquet\n",
      "\n",
      "[17/100] fhv_tripdata_2025-01.parquet\n",
      "  Read: 1,898,108 rows, 7 columns\n",
      "  Cleaned: 1,898,108 rows, 6 columns\n",
      "  Retention: 100.00%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/fhv_tripdata_2025-01.parquet\n",
      "\n",
      "[18/100] fhv_tripdata_2025-02.parquet\n",
      "  Read: 1,578,722 rows, 7 columns\n",
      "  Cleaned: 1,578,722 rows, 6 columns\n",
      "  Retention: 100.00%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/fhv_tripdata_2025-02.parquet\n",
      "\n",
      "[19/100] fhv_tripdata_2025-03.parquet\n",
      "  Read: 2,182,992 rows, 7 columns\n",
      "  Cleaned: 2,182,992 rows, 6 columns\n",
      "  Retention: 100.00%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/fhv_tripdata_2025-03.parquet\n",
      "\n",
      "[20/100] fhv_tripdata_2025-04.parquet\n",
      "  Read: 1,699,478 rows, 7 columns\n",
      "  Cleaned: 1,699,478 rows, 6 columns\n",
      "  Retention: 100.00%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/fhv_tripdata_2025-04.parquet\n",
      "\n",
      "[21/100] fhv_tripdata_2025-05.parquet\n",
      "  Read: 2,210,721 rows, 7 columns\n",
      "  Cleaned: 2,210,721 rows, 6 columns\n",
      "  Retention: 100.00%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/fhv_tripdata_2025-05.parquet\n",
      "\n",
      "[22/100] fhv_tripdata_2025-06.parquet\n",
      "  Read: 2,231,731 rows, 7 columns\n",
      "  Cleaned: 2,231,731 rows, 6 columns\n",
      "  Retention: 100.00%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/fhv_tripdata_2025-06.parquet\n",
      "\n",
      "[23/100] fhv_tripdata_2025-07.parquet\n",
      "  Read: 2,187,536 rows, 7 columns\n",
      "  Cleaned: 2,187,536 rows, 6 columns\n",
      "  Retention: 100.00%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/fhv_tripdata_2025-07.parquet\n",
      "\n",
      "[24/100] fhv_tripdata_2025-08.parquet\n",
      "  Read: 2,256,854 rows, 7 columns\n",
      "  Cleaned: 2,256,854 rows, 6 columns\n",
      "  Retention: 100.00%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/fhv_tripdata_2025-08.parquet\n",
      "\n",
      "[25/100] fhvhv_tripdata_2019-02.parquet\n",
      "  Read: 20,159,102 rows, 24 columns\n",
      "  Cleaned: 20,159,102 rows, 22 columns\n",
      "  Retention: 100.00%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/fhvhv_tripdata_2019-02.parquet\n",
      "\n",
      "[26/100] fhvhv_tripdata_2019-12.parquet\n",
      "  Read: 22,243,901 rows, 24 columns\n",
      "  Cleaned: 22,243,901 rows, 24 columns\n",
      "  Retention: 100.00%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/fhvhv_tripdata_2019-12.parquet\n",
      "\n",
      "[27/100] fhvhv_tripdata_2024-01.parquet\n",
      "  Read: 19,663,930 rows, 24 columns\n",
      "  Cleaned: 19,663,930 rows, 24 columns\n",
      "  Retention: 100.00%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/fhvhv_tripdata_2024-01.parquet\n",
      "\n",
      "[28/100] fhvhv_tripdata_2024-02.parquet\n",
      "  Read: 19,359,148 rows, 24 columns\n",
      "  Cleaned: 19,359,148 rows, 24 columns\n",
      "  Retention: 100.00%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/fhvhv_tripdata_2024-02.parquet\n",
      "\n",
      "[29/100] fhvhv_tripdata_2024-03.parquet\n",
      "  Read: 21,280,788 rows, 24 columns\n",
      "  Cleaned: 21,280,788 rows, 24 columns\n",
      "  Retention: 100.00%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/fhvhv_tripdata_2024-03.parquet\n",
      "\n",
      "[30/100] fhvhv_tripdata_2024-04.parquet\n",
      "  Read: 19,733,038 rows, 24 columns\n",
      "  Cleaned: 19,733,038 rows, 24 columns\n",
      "  Retention: 100.00%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/fhvhv_tripdata_2024-04.parquet\n",
      "\n",
      "[31/100] fhvhv_tripdata_2024-05.parquet\n",
      "  Read: 20,704,538 rows, 24 columns\n",
      "  Cleaned: 20,704,538 rows, 24 columns\n",
      "  Retention: 100.00%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/fhvhv_tripdata_2024-05.parquet\n",
      "\n",
      "[32/100] fhvhv_tripdata_2024-06.parquet\n",
      "  Read: 20,123,226 rows, 24 columns\n",
      "  Cleaned: 20,123,226 rows, 24 columns\n",
      "  Retention: 100.00%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/fhvhv_tripdata_2024-06.parquet\n",
      "\n",
      "[33/100] fhvhv_tripdata_2024-07.parquet\n",
      "  Read: 19,182,934 rows, 24 columns\n",
      "  Cleaned: 19,182,934 rows, 24 columns\n",
      "  Retention: 100.00%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/fhvhv_tripdata_2024-07.parquet\n",
      "\n",
      "[34/100] fhvhv_tripdata_2024-08.parquet\n",
      "  Read: 19,128,392 rows, 24 columns\n",
      "  Cleaned: 19,128,392 rows, 24 columns\n",
      "  Retention: 100.00%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/fhvhv_tripdata_2024-08.parquet\n",
      "\n",
      "[35/100] fhvhv_tripdata_2024-09.parquet\n",
      "  Read: 19,209,788 rows, 24 columns\n",
      "  Cleaned: 19,209,788 rows, 24 columns\n",
      "  Retention: 100.00%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/fhvhv_tripdata_2024-09.parquet\n",
      "\n",
      "[36/100] fhvhv_tripdata_2024-10.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Read: 20,028,282 rows, 24 columns\n",
      "  Cleaned: 20,028,282 rows, 24 columns\n",
      "  Retention: 100.00%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/fhvhv_tripdata_2024-10.parquet\n",
      "\n",
      "[37/100] fhvhv_tripdata_2024-11.parquet\n",
      "  Read: 19,987,533 rows, 24 columns\n",
      "  Cleaned: 19,987,533 rows, 24 columns\n",
      "  Retention: 100.00%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/fhvhv_tripdata_2024-11.parquet\n",
      "\n",
      "[38/100] fhvhv_tripdata_2024-12.parquet\n",
      "  Read: 21,068,851 rows, 24 columns\n",
      "  Cleaned: 21,068,851 rows, 24 columns\n",
      "  Retention: 100.00%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/fhvhv_tripdata_2024-12.parquet\n",
      "\n",
      "[39/100] fhvhv_tripdata_2025-01.parquet\n",
      "  Read: 20,405,666 rows, 25 columns\n",
      "  Cleaned: 20,405,666 rows, 25 columns\n",
      "  Retention: 100.00%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/fhvhv_tripdata_2025-01.parquet\n",
      "\n",
      "[40/100] fhvhv_tripdata_2025-02.parquet\n",
      "  Read: 19,339,461 rows, 25 columns\n",
      "  Cleaned: 19,339,461 rows, 25 columns\n",
      "  Retention: 100.00%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/fhvhv_tripdata_2025-02.parquet\n",
      "\n",
      "[41/100] fhvhv_tripdata_2025-03.parquet\n",
      "  Read: 20,536,879 rows, 25 columns\n",
      "  Cleaned: 20,536,879 rows, 25 columns\n",
      "  Retention: 100.00%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/fhvhv_tripdata_2025-03.parquet\n",
      "\n",
      "[42/100] fhvhv_tripdata_2025-04.parquet\n",
      "  Read: 19,753,983 rows, 25 columns\n",
      "  Cleaned: 19,753,983 rows, 25 columns\n",
      "  Retention: 100.00%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/fhvhv_tripdata_2025-04.parquet\n",
      "\n",
      "[43/100] fhvhv_tripdata_2025-05.parquet\n",
      "  Read: 21,091,193 rows, 25 columns\n",
      "  Cleaned: 21,091,193 rows, 25 columns\n",
      "  Retention: 100.00%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/fhvhv_tripdata_2025-05.parquet\n",
      "\n",
      "[44/100] fhvhv_tripdata_2025-06.parquet\n",
      "  Read: 19,868,009 rows, 25 columns\n",
      "  Cleaned: 19,868,009 rows, 25 columns\n",
      "  Retention: 100.00%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/fhvhv_tripdata_2025-06.parquet\n",
      "\n",
      "[45/100] fhvhv_tripdata_2025-07.parquet\n",
      "  Read: 19,653,012 rows, 25 columns\n",
      "  Cleaned: 19,653,012 rows, 25 columns\n",
      "  Retention: 100.00%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/fhvhv_tripdata_2025-07.parquet\n",
      "\n",
      "[46/100] fhvhv_tripdata_2025-08.parquet\n",
      "  Read: 19,271,461 rows, 25 columns\n",
      "  Cleaned: 19,271,461 rows, 25 columns\n",
      "  Retention: 100.00%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/fhvhv_tripdata_2025-08.parquet\n",
      "\n",
      "[47/100] green_tripdata_2014-02.parquet\n",
      "  Read: 1,005,242 rows, 20 columns\n",
      "  Cleaned: 1,004,969 rows, 17 columns\n",
      "  Retention: 99.97%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/green_tripdata_2014-02.parquet\n",
      "\n",
      "[48/100] green_tripdata_2014-12.parquet\n",
      "  Read: 1,645,787 rows, 20 columns\n",
      "  Cleaned: 1,643,114 rows, 18 columns\n",
      "  Retention: 99.84%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/green_tripdata_2014-12.parquet\n",
      "\n",
      "[49/100] green_tripdata_2015-02.parquet\n",
      "  Read: 1,574,830 rows, 20 columns\n",
      "  Cleaned: 1,572,027 rows, 18 columns\n",
      "  Retention: 99.82%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/green_tripdata_2015-02.parquet\n",
      "\n",
      "[50/100] green_tripdata_2015-12.parquet\n",
      "  Read: 1,608,297 rows, 20 columns\n",
      "  Cleaned: 1,604,779 rows, 18 columns\n",
      "  Retention: 99.78%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/green_tripdata_2015-12.parquet\n",
      "\n",
      "[51/100] green_tripdata_2019-02.parquet\n",
      "  Read: 615,594 rows, 20 columns\n",
      "  Cleaned: 612,380 rows, 20 columns\n",
      "  Retention: 99.48%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/green_tripdata_2019-02.parquet\n",
      "\n",
      "[52/100] green_tripdata_2019-12.parquet\n",
      "  Read: 455,294 rows, 20 columns\n",
      "  Cleaned: 450,335 rows, 19 columns\n",
      "  Retention: 98.91%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/green_tripdata_2019-12.parquet\n",
      "\n",
      "[53/100] green_tripdata_2024-01.parquet\n",
      "  Read: 56,551 rows, 20 columns\n",
      "  Cleaned: 55,809 rows, 19 columns\n",
      "  Retention: 98.69%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/green_tripdata_2024-01.parquet\n",
      "\n",
      "[54/100] green_tripdata_2024-02.parquet\n",
      "  Read: 53,577 rows, 20 columns\n",
      "  Cleaned: 52,835 rows, 19 columns\n",
      "  Retention: 98.62%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/green_tripdata_2024-02.parquet\n",
      "\n",
      "[55/100] green_tripdata_2024-03.parquet\n",
      "  Read: 57,457 rows, 20 columns\n",
      "  Cleaned: 56,681 rows, 19 columns\n",
      "  Retention: 98.65%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/green_tripdata_2024-03.parquet\n",
      "\n",
      "[56/100] green_tripdata_2024-04.parquet\n",
      "  Read: 56,471 rows, 20 columns\n",
      "  Cleaned: 55,802 rows, 19 columns\n",
      "  Retention: 98.82%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/green_tripdata_2024-04.parquet\n",
      "\n",
      "[57/100] green_tripdata_2024-05.parquet\n",
      "  Read: 61,003 rows, 20 columns\n",
      "  Cleaned: 60,208 rows, 19 columns\n",
      "  Retention: 98.70%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/green_tripdata_2024-05.parquet\n",
      "\n",
      "[58/100] green_tripdata_2024-06.parquet\n",
      "  Read: 54,748 rows, 20 columns\n",
      "  Cleaned: 54,014 rows, 19 columns\n",
      "  Retention: 98.66%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/green_tripdata_2024-06.parquet\n",
      "\n",
      "[59/100] green_tripdata_2024-07.parquet\n",
      "  Read: 51,837 rows, 20 columns\n",
      "  Cleaned: 51,123 rows, 19 columns\n",
      "  Retention: 98.62%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/green_tripdata_2024-07.parquet\n",
      "\n",
      "[60/100] green_tripdata_2024-08.parquet\n",
      "  Read: 51,771 rows, 20 columns\n",
      "  Cleaned: 51,016 rows, 19 columns\n",
      "  Retention: 98.54%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/green_tripdata_2024-08.parquet\n",
      "\n",
      "[61/100] green_tripdata_2024-09.parquet\n",
      "  Read: 54,440 rows, 20 columns\n",
      "  Cleaned: 53,678 rows, 19 columns\n",
      "  Retention: 98.60%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/green_tripdata_2024-09.parquet\n",
      "\n",
      "[62/100] green_tripdata_2024-10.parquet\n",
      "  Read: 56,147 rows, 20 columns\n",
      "  Cleaned: 55,366 rows, 19 columns\n",
      "  Retention: 98.61%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/green_tripdata_2024-10.parquet\n",
      "\n",
      "[63/100] green_tripdata_2024-11.parquet\n",
      "  Read: 52,222 rows, 20 columns\n",
      "  Cleaned: 51,377 rows, 19 columns\n",
      "  Retention: 98.38%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/green_tripdata_2024-11.parquet\n",
      "\n",
      "[64/100] green_tripdata_2024-12.parquet\n",
      "  Read: 53,994 rows, 20 columns\n",
      "  Cleaned: 53,063 rows, 19 columns\n",
      "  Retention: 98.28%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/green_tripdata_2024-12.parquet\n",
      "\n",
      "[65/100] green_tripdata_2025-01.parquet\n",
      "  Read: 48,326 rows, 21 columns\n",
      "  Cleaned: 47,506 rows, 20 columns\n",
      "  Retention: 98.30%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/green_tripdata_2025-01.parquet\n",
      "\n",
      "[66/100] green_tripdata_2025-02.parquet\n",
      "  Read: 46,621 rows, 21 columns\n",
      "  Cleaned: 45,847 rows, 20 columns\n",
      "  Retention: 98.34%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/green_tripdata_2025-02.parquet\n",
      "\n",
      "[67/100] green_tripdata_2025-03.parquet\n",
      "  Read: 51,539 rows, 21 columns\n",
      "  Cleaned: 50,756 rows, 20 columns\n",
      "  Retention: 98.48%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/green_tripdata_2025-03.parquet\n",
      "\n",
      "[68/100] green_tripdata_2025-04.parquet\n",
      "  Read: 52,132 rows, 21 columns\n",
      "  Cleaned: 51,322 rows, 20 columns\n",
      "  Retention: 98.45%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/green_tripdata_2025-04.parquet\n",
      "\n",
      "[69/100] green_tripdata_2025-05.parquet\n",
      "  Read: 55,399 rows, 21 columns\n",
      "  Cleaned: 54,441 rows, 20 columns\n",
      "  Retention: 98.27%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/green_tripdata_2025-05.parquet\n",
      "\n",
      "[70/100] green_tripdata_2025-06.parquet\n",
      "  Read: 49,390 rows, 21 columns\n",
      "  Cleaned: 48,352 rows, 20 columns\n",
      "  Retention: 97.90%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/green_tripdata_2025-06.parquet\n",
      "\n",
      "[71/100] green_tripdata_2025-07.parquet\n",
      "  Read: 48,205 rows, 21 columns\n",
      "  Cleaned: 47,296 rows, 20 columns\n",
      "  Retention: 98.11%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/green_tripdata_2025-07.parquet\n",
      "\n",
      "[72/100] green_tripdata_2025-08.parquet\n",
      "  Read: 46,306 rows, 21 columns\n",
      "  Cleaned: 45,475 rows, 20 columns\n",
      "  Retention: 98.21%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/green_tripdata_2025-08.parquet\n",
      "\n",
      "[73/100] yellow_tripdata_2009-02.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Read: 13,380,122 rows, 18 columns\n",
      "  Cleaned: 13,380,122 rows, 16 columns\n",
      "  Retention: 100.00%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/yellow_tripdata_2009-02.parquet\n",
      "\n",
      "[74/100] yellow_tripdata_2009-12.parquet\n",
      "  Read: 14,583,404 rows, 18 columns\n",
      "  Cleaned: 14,583,404 rows, 17 columns\n",
      "  Retention: 100.00%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/yellow_tripdata_2009-12.parquet\n",
      "\n",
      "[75/100] yellow_tripdata_2014-02.parquet\n",
      "  Read: 13,063,794 rows, 19 columns\n",
      "  Cleaned: 13,063,681 rows, 17 columns\n",
      "  Retention: 100.00%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/yellow_tripdata_2014-02.parquet\n",
      "\n",
      "[76/100] yellow_tripdata_2014-12.parquet\n",
      "  Read: 13,112,117 rows, 19 columns\n",
      "  Cleaned: 13,105,964 rows, 17 columns\n",
      "  Retention: 99.95%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/yellow_tripdata_2014-12.parquet\n",
      "\n",
      "[77/100] yellow_tripdata_2015-02.parquet\n",
      "  Read: 12,442,394 rows, 19 columns\n",
      "  Cleaned: 12,429,977 rows, 17 columns\n",
      "  Retention: 99.90%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/yellow_tripdata_2015-02.parquet\n",
      "\n",
      "[78/100] yellow_tripdata_2015-12.parquet\n",
      "  Read: 11,452,996 rows, 19 columns\n",
      "  Cleaned: 11,447,795 rows, 17 columns\n",
      "  Retention: 99.95%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/yellow_tripdata_2015-12.parquet\n",
      "\n",
      "[79/100] yellow_tripdata_2019-02.parquet\n",
      "  Read: 7,049,370 rows, 19 columns\n",
      "  Cleaned: 6,920,741 rows, 18 columns\n",
      "  Retention: 98.18%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/yellow_tripdata_2019-02.parquet\n",
      "\n",
      "[80/100] yellow_tripdata_2019-12.parquet\n",
      "  Read: 6,896,317 rows, 19 columns\n",
      "  Cleaned: 6,746,824 rows, 18 columns\n",
      "  Retention: 97.83%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/yellow_tripdata_2019-12.parquet\n",
      "\n",
      "[81/100] yellow_tripdata_2024-01.parquet\n",
      "  Read: 2,964,624 rows, 19 columns\n",
      "  Cleaned: 2,895,631 rows, 19 columns\n",
      "  Retention: 97.67%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/yellow_tripdata_2024-01.parquet\n",
      "\n",
      "[82/100] yellow_tripdata_2024-02.parquet\n",
      "  Read: 3,007,526 rows, 19 columns\n",
      "  Cleaned: 2,932,683 rows, 19 columns\n",
      "  Retention: 97.51%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/yellow_tripdata_2024-02.parquet\n",
      "\n",
      "[83/100] yellow_tripdata_2024-03.parquet\n",
      "  Read: 3,582,628 rows, 19 columns\n",
      "  Cleaned: 3,483,658 rows, 19 columns\n",
      "  Retention: 97.24%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/yellow_tripdata_2024-03.parquet\n",
      "\n",
      "[84/100] yellow_tripdata_2024-04.parquet\n",
      "  Read: 3,514,289 rows, 19 columns\n",
      "  Cleaned: 3,416,960 rows, 19 columns\n",
      "  Retention: 97.23%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/yellow_tripdata_2024-04.parquet\n",
      "\n",
      "[85/100] yellow_tripdata_2024-05.parquet\n",
      "  Read: 3,723,833 rows, 19 columns\n",
      "  Cleaned: 3,623,970 rows, 19 columns\n",
      "  Retention: 97.32%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/yellow_tripdata_2024-05.parquet\n",
      "\n",
      "[86/100] yellow_tripdata_2024-06.parquet\n",
      "  Read: 3,539,193 rows, 19 columns\n",
      "  Cleaned: 3,441,506 rows, 19 columns\n",
      "  Retention: 97.24%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/yellow_tripdata_2024-06.parquet\n",
      "\n",
      "[87/100] yellow_tripdata_2024-07.parquet\n",
      "  Read: 3,076,903 rows, 19 columns\n",
      "  Cleaned: 2,988,953 rows, 19 columns\n",
      "  Retention: 97.14%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/yellow_tripdata_2024-07.parquet\n",
      "\n",
      "[88/100] yellow_tripdata_2024-08.parquet\n",
      "  Read: 2,979,183 rows, 19 columns\n",
      "  Cleaned: 2,894,471 rows, 19 columns\n",
      "  Retention: 97.16%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/yellow_tripdata_2024-08.parquet\n",
      "\n",
      "[89/100] yellow_tripdata_2024-09.parquet\n",
      "  Read: 3,633,030 rows, 19 columns\n",
      "  Cleaned: 3,529,402 rows, 19 columns\n",
      "  Retention: 97.15%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/yellow_tripdata_2024-09.parquet\n",
      "\n",
      "[90/100] yellow_tripdata_2024-10.parquet\n",
      "  Read: 3,833,771 rows, 19 columns\n",
      "  Cleaned: 3,722,475 rows, 19 columns\n",
      "  Retention: 97.10%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/yellow_tripdata_2024-10.parquet\n",
      "\n",
      "[91/100] yellow_tripdata_2024-11.parquet\n",
      "  Read: 3,646,369 rows, 19 columns\n",
      "  Cleaned: 3,546,370 rows, 19 columns\n",
      "  Retention: 97.26%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/yellow_tripdata_2024-11.parquet\n",
      "\n",
      "[92/100] yellow_tripdata_2024-12.parquet\n",
      "  Read: 3,668,371 rows, 19 columns\n",
      "  Cleaned: 3,559,383 rows, 19 columns\n",
      "  Retention: 97.03%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/yellow_tripdata_2024-12.parquet\n",
      "\n",
      "[93/100] yellow_tripdata_2025-01.parquet\n",
      "  Read: 3,475,226 rows, 20 columns\n",
      "  Cleaned: 3,306,273 rows, 20 columns\n",
      "  Retention: 95.14%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/yellow_tripdata_2025-01.parquet\n",
      "\n",
      "[94/100] yellow_tripdata_2025-02.parquet\n",
      "  Read: 3,577,543 rows, 20 columns\n",
      "  Cleaned: 3,372,961 rows, 20 columns\n",
      "  Retention: 94.28%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/yellow_tripdata_2025-02.parquet\n",
      "\n",
      "[95/100] yellow_tripdata_2025-03.parquet\n",
      "  Read: 4,145,257 rows, 20 columns\n",
      "  Cleaned: 3,913,606 rows, 20 columns\n",
      "  Retention: 94.41%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/yellow_tripdata_2025-03.parquet\n",
      "\n",
      "[96/100] yellow_tripdata_2025-04.parquet\n",
      "  Read: 3,970,553 rows, 20 columns\n",
      "  Cleaned: 3,760,427 rows, 20 columns\n",
      "  Retention: 94.71%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/yellow_tripdata_2025-04.parquet\n",
      "\n",
      "[97/100] yellow_tripdata_2025-05.parquet\n",
      "  Read: 4,591,845 rows, 20 columns\n",
      "  Cleaned: 4,242,250 rows, 20 columns\n",
      "  Retention: 92.39%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/yellow_tripdata_2025-05.parquet\n",
      "\n",
      "[98/100] yellow_tripdata_2025-06.parquet\n",
      "  Read: 4,322,960 rows, 20 columns\n",
      "  Cleaned: 4,024,343 rows, 20 columns\n",
      "  Retention: 93.09%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/yellow_tripdata_2025-06.parquet\n",
      "\n",
      "[99/100] yellow_tripdata_2025-07.parquet\n",
      "  Read: 3,898,963 rows, 20 columns\n",
      "  Cleaned: 3,631,053 rows, 20 columns\n",
      "  Retention: 93.13%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/yellow_tripdata_2025-07.parquet\n",
      "\n",
      "[100/100] yellow_tripdata_2025-08.parquet\n",
      "  Read: 3,574,091 rows, 20 columns\n",
      "  Cleaned: 3,295,362 rows, 20 columns\n",
      "  Retention: 92.20%\n",
      "  \u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/yellow_tripdata_2025-08.parquet\n",
      "\n",
      "====================================================================================================\n",
      "\ud83d\udcbe CLEANING EXECUTION COMPLETE\n",
      "Logs JSON : /Users/gouravdhama/Documents/bubu/cleaning_code/cleaning_logs.json\n",
      "Summary CSV: /Users/gouravdhama/Documents/bubu/cleaning_code/per_file_drop_stats_for_excel.csv\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Execute cleaning\n",
    "os.makedirs(CLEANED_DATA_DIR, exist_ok=True)\n",
    "\n",
    "raw_path = Path(RAW_DATA_DIR)\n",
    "parquet_files = sorted(raw_path.glob(\"*.parquet\"))\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(f\"\ud83d\udea7 EXECUTING {CLEANING_MODE} CLEANING\")\n",
    "print(f\"Source: {RAW_DATA_DIR}\")\n",
    "print(f\"Destination: {CLEANED_DATA_DIR}\")\n",
    "print(f\"Files to process: {len(parquet_files)}\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "all_cleaning_logs = []\n",
    "\n",
    "for i, file_path in enumerate(parquet_files, 1):\n",
    "    file_name = file_path.name\n",
    "    file_type = get_file_type(file_name)\n",
    "    \n",
    "    print(f\"\\n[{i}/{len(parquet_files)}] {file_name}\")\n",
    "    \n",
    "    try:\n",
    "        df_raw = safe_read_parquet(file_path)\n",
    "        print(f\"  Read: {len(df_raw):,} rows, {len(df_raw.columns)} columns\")\n",
    "        \n",
    "        # Clean using smart function\n",
    "        df_clean, cleaning_log = clean_dataframe_smart(df_raw, file_name, file_type)\n",
    "        all_cleaning_logs.append(cleaning_log)\n",
    "        \n",
    "        # Save cleaned file\n",
    "        out_path = Path(CLEANED_DATA_DIR) / file_name\n",
    "        df_clean.to_parquet(out_path, index=False)\n",
    "        \n",
    "        print(f\"  Cleaned: {len(df_clean):,} rows, {len(df_clean.columns)} columns\")\n",
    "        print(f\"  Retention: {cleaning_log['retention_rate']:.2f}%\")\n",
    "        print(f\"  \u2705 Saved to: {out_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  \u274c ERROR: {e}\")\n",
    "\n",
    "# Save cleaning logs\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "logs_json_path = Path(OUTPUT_DIR) / \"cleaning_logs.json\"\n",
    "logs_json_path.write_text(json.dumps({\"cleaning_logs\": all_cleaning_logs}, indent=2))\n",
    "\n",
    "# Save CSV summary for Excel\n",
    "if all_cleaning_logs:\n",
    "    summary_df = pd.DataFrame(all_cleaning_logs)\n",
    "    summary_csv_path = Path(OUTPUT_DIR) / \"per_file_drop_stats_for_excel.csv\"\n",
    "    summary_df[[\n",
    "        'file_name', 'file_type', 'cleaning_mode', 'original_rows', \n",
    "        'final_rows', 'retention_rate'\n",
    "    ]].to_csv(summary_csv_path, index=False)\n",
    "else:\n",
    "    summary_csv_path = None\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"\ud83d\udcbe CLEANING EXECUTION COMPLETE\")\n",
    "print(f\"Logs JSON : {logs_json_path}\")\n",
    "if summary_csv_path:\n",
    "    print(f\"Summary CSV: {summary_csv_path}\")\n",
    "print(\"=\" * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**What you just did:**\n",
    "1. \u2705 Analyzed raw data for missing values\n",
    "2. \u2705 Cleaned data using **{CLEANING_MODE} mode**\n",
    "3. \u2705 Wrote cleaned parquet files to `cleaned_data/`\n",
    "4. \u2705 Saved logs and summaries\n",
    "\n",
    "**Next step:** Run `data_validation.ipynb` to compare raw vs cleaned data.\n",
    "\n",
    "**Want to try a different cleaning mode?**\n",
    "- Go back to cell 3, change `CLEANING_MODE` to `\"SIMPLE\"` or `\"SMART\"`\n",
    "- Restart kernel and run all cells again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 File loaded successfully!\n",
      "Rows: 1,707,650\n",
      "Columns: ['dispatching_base_num', 'pickup_datetime', 'dropOff_datetime', 'PUlocationID', 'DOlocationID', 'SR_Flag', 'Affiliated_base_number']\n",
      "\n",
      "First few rows:\n",
      "  dispatching_base_num     pickup_datetime    dropOff_datetime  PUlocationID  \\\n",
      "0               B00037 2019-02-01 00:08:44 2019-02-01 00:23:35         264.0   \n",
      "1               B00037 2019-02-01 00:27:51 2019-02-01 00:32:54         264.0   \n",
      "2               B00037 2019-02-01 00:18:30 2019-02-01 00:25:45         264.0   \n",
      "3               B00037 2019-02-01 00:43:15 2019-02-01 00:48:29         264.0   \n",
      "4               B00037 2019-02-01 00:01:45 2019-02-01 00:09:13         264.0   \n",
      "\n",
      "   DOlocationID  SR_Flag Affiliated_base_number  \n",
      "0         265.0      NaN                 B00037  \n",
      "1         265.0      NaN                 B00037  \n",
      "2         265.0      NaN                 B00037  \n",
      "3         265.0      NaN                 B00037  \n",
      "4         265.0      NaN                 B00037  \n",
      "\n",
      "\u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/fhv_tripdata_2019-02.parquet\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Trip Data - Analyze & Cleaning Preview\n",
    "\n",
    "This notebook mirrors the logic of `analyze_and_clean.py`.\n",
    "\n",
    "It will:\n",
    "- Scan all raw parquet files\n",
    "- Analyze missing data patterns\n",
    "- Preview what cleaning would do (DRY RUN)\n",
    "- Save detailed results to `analysis_results.json`\n",
    "\n",
    "Run cells from top to bottom.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW_DATA_DIR: /Users/gouravdhama/Documents/bubu/Raw_data/raw_data\n",
      "CLEANED_DATA_DIR (for future execute mode): /Users/gouravdhama/Documents/bubu/cleaned_data\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "# from pathlib import Path\n",
    "# import json\n",
    "# import warnings\n",
    "\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# RAW_DATA_DIR = \"/Users/gouravdhama/Documents/bubu/Raw_data/raw_data\"\n",
    "# CLEANED_DATA_DIR = \"/Users/gouravdhama/Documents/bubu/cleaned_data\"  # not used in DRY RUN yet\n",
    "\n",
    "# print(\"RAW_DATA_DIR:\", RAW_DATA_DIR)\n",
    "# print(\"CLEANED_DATA_DIR (for future execute mode):\", CLEANED_DATA_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Functions for analysis & preview loaded.\n"
     ]
    }
   ],
   "source": [
    "# ---- Functions from analyze_and_clean.py (analysis + cleaning preview) ----\n",
    "\n",
    "CLEANING_CONFIG = {\n",
    "    'fill_null_strategy': 'smart',  # 'smart', 'drop', or 'keep'\n",
    "    'drop_completely_empty_columns': True,\n",
    "    'impute_locations_with_unknown': True,  # Fill missing locations with -1\n",
    "    'impute_numeric_with_defaults': True,   # Fill missing numeric fields\n",
    "    'impute_categorical_with_mode': True,   # Fill missing categorical fields\n",
    "    'remove_invalid_records': True,         # Remove clearly invalid data\n",
    "    'max_trip_distance': 100,               # miles\n",
    "    'max_fare_amount': 500,                 # dollars\n",
    "    'max_passenger_count': 8,\n",
    "}\n",
    "\n",
    "\n",
    "def analyze_missing_data(df, file_name):\n",
    "    \"\"\"Analyze missing data patterns in a dataframe\"\"\"\n",
    "    total_rows = len(df)\n",
    "    \n",
    "    if total_rows == 0:\n",
    "        return {\n",
    "            'file_name': file_name,\n",
    "            'total_rows': 0,\n",
    "            'error': 'Empty dataframe'\n",
    "        }\n",
    "    \n",
    "    missing_info = {}\n",
    "    for col in df.columns:\n",
    "        null_count = df[col].isnull().sum()\n",
    "        null_pct = (null_count / total_rows * 100)\n",
    "        \n",
    "        missing_info[col] = {\n",
    "            'null_count': int(null_count),\n",
    "            'null_percentage': round(null_pct, 2),\n",
    "            'dtype': str(df[col].dtype)\n",
    "        }\n",
    "    \n",
    "    cols_with_missing = {k: v for k, v in missing_info.items() if v['null_count'] > 0}\n",
    "    \n",
    "    return {\n",
    "        'file_name': file_name,\n",
    "        'total_rows': int(total_rows),\n",
    "        'total_columns': len(df.columns),\n",
    "        'columns': list(df.columns),\n",
    "        'missing_data': missing_info,\n",
    "        'columns_with_missing': cols_with_missing\n",
    "    }\n",
    "\n",
    "\n",
    "def get_file_type(file_name):\n",
    "    \"\"\"Determine file type from filename\"\"\"\n",
    "    if file_name.startswith('fhvhv_'):\n",
    "        return 'fhvhv'\n",
    "    elif file_name.startswith('fhv_'):\n",
    "        return 'fhv'\n",
    "    elif file_name.startswith('green_'):\n",
    "        return 'green'\n",
    "    elif file_name.startswith('yellow_'):\n",
    "        return 'yellow'\n",
    "    return 'unknown'\n",
    "\n",
    "\n",
    "def preview_cleaning(df, file_name, file_type):\n",
    "    \"\"\"Preview what cleaning would do WITHOUT modifying data\"\"\"\n",
    "    original_rows = len(df)\n",
    "    original_cols = len(df.columns)\n",
    "    \n",
    "    stats = {\n",
    "        'file_name': file_name,\n",
    "        'file_type': file_type,\n",
    "        'original_rows': original_rows,\n",
    "        'original_cols': original_cols,\n",
    "        'actions': []\n",
    "    }\n",
    "    \n",
    "    # 100% empty columns\n",
    "    if CLEANING_CONFIG['drop_completely_empty_columns']:\n",
    "        empty_cols = [col for col in df.columns if df[col].isnull().sum() == original_rows]\n",
    "        if empty_cols:\n",
    "            stats['actions'].append({\n",
    "                'action': 'DROP_COLUMNS',\n",
    "                'columns': empty_cols,\n",
    "                'reason': '100% missing',\n",
    "                'impact': f\"Remove {len(empty_cols)} columns\"\n",
    "            })\n",
    "    \n",
    "    # Location columns\n",
    "    location_cols = [col for col in df.columns if 'location' in col.lower()]\n",
    "    for col in location_cols:\n",
    "        missing = df[col].isnull().sum()\n",
    "        if missing > 0:\n",
    "            stats['actions'].append({\n",
    "                'action': 'FILL_NULL',\n",
    "                'column': col,\n",
    "                'missing_count': int(missing),\n",
    "                'missing_pct': round(missing/original_rows*100, 2),\n",
    "                'fill_value': -1,\n",
    "                'reason': 'Preserve trips with unknown location'\n",
    "            })\n",
    "    \n",
    "    # passenger_count\n",
    "    if 'passenger_count' in df.columns:\n",
    "        missing = df['passenger_count'].isnull().sum()\n",
    "        if missing > 0:\n",
    "            stats['actions'].append({\n",
    "                'action': 'FILL_NULL',\n",
    "                'column': 'passenger_count',\n",
    "                'missing_count': int(missing),\n",
    "                'missing_pct': round(missing/original_rows*100, 2),\n",
    "                'fill_value': 1,\n",
    "                'reason': 'Most trips have 1 passenger'\n",
    "            })\n",
    "    \n",
    "    # RatecodeID\n",
    "    if 'RatecodeID' in df.columns:\n",
    "        missing = df['RatecodeID'].isnull().sum()\n",
    "        if missing > 0:\n",
    "            stats['actions'].append({\n",
    "                'action': 'FILL_NULL',\n",
    "                'column': 'RatecodeID',\n",
    "                'missing_count': int(missing),\n",
    "                'missing_pct': round(missing/original_rows*100, 2),\n",
    "                'fill_value': 1,\n",
    "                'reason': 'Standard rate is default'\n",
    "            })\n",
    "    \n",
    "    # store_and_fwd_flag\n",
    "    if 'store_and_fwd_flag' in df.columns:\n",
    "        missing = df['store_and_fwd_flag'].isnull().sum()\n",
    "        if missing > 0:\n",
    "            stats['actions'].append({\n",
    "                'action': 'FILL_NULL',\n",
    "                'column': 'store_and_fwd_flag',\n",
    "                'missing_count': int(missing),\n",
    "                'missing_pct': round(missing/original_rows*100, 2),\n",
    "                'fill_value': 'N',\n",
    "                'reason': 'Most trips are not stored and forwarded'\n",
    "            })\n",
    "    \n",
    "    # payment_type\n",
    "    if 'payment_type' in df.columns:\n",
    "        missing = df['payment_type'].isnull().sum()\n",
    "        if missing > 0:\n",
    "            stats['actions'].append({\n",
    "                'action': 'FILL_NULL',\n",
    "                'column': 'payment_type',\n",
    "                'missing_count': int(missing),\n",
    "                'missing_pct': round(missing/original_rows*100, 2),\n",
    "                'fill_value': 1,\n",
    "                'reason': 'Credit card is most common'\n",
    "            })\n",
    "    \n",
    "    # fee columns\n",
    "    fee_cols = [col for col in df.columns if any(x in col.lower() for x in ['surcharge', 'fee', 'tax'])]\n",
    "    for col in fee_cols:\n",
    "        if col in df.columns:\n",
    "            missing = df[col].isnull().sum()\n",
    "            if missing > 0:\n",
    "                stats['actions'].append({\n",
    "                    'action': 'FILL_NULL',\n",
    "                    'column': col,\n",
    "                    'missing_count': int(missing),\n",
    "                    'missing_pct': round(missing/original_rows*100, 2),\n",
    "                    'fill_value': 0,\n",
    "                    'reason': 'Assume no fee if not recorded'\n",
    "                })\n",
    "    \n",
    "    # trip_type\n",
    "    if 'trip_type' in df.columns:\n",
    "        missing = df['trip_type'].isnull().sum()\n",
    "        if missing > 0:\n",
    "            mode_val = df['trip_type'].mode()\n",
    "            mode_val = mode_val[0] if len(mode_val) > 0 else 1\n",
    "            stats['actions'].append({\n",
    "                'action': 'FILL_NULL',\n",
    "                'column': 'trip_type',\n",
    "                'missing_count': int(missing),\n",
    "                'missing_pct': round(missing/original_rows*100, 2),\n",
    "                'fill_value': int(mode_val),\n",
    "                'reason': f'Use most common trip type ({mode_val})'\n",
    "            })\n",
    "    \n",
    "    # invalid records\n",
    "    if CLEANING_CONFIG['remove_invalid_records']:\n",
    "        invalid_count = 0\n",
    "        \n",
    "        if 'fare_amount' in df.columns:\n",
    "            invalid = ((df['fare_amount'] < 0) | (df['fare_amount'] > CLEANING_CONFIG['max_fare_amount'])).sum()\n",
    "            invalid_count += invalid\n",
    "        \n",
    "        if 'trip_distance' in df.columns:\n",
    "            invalid = ((df['trip_distance'] < 0) | (df['trip_distance'] > CLEANING_CONFIG['max_trip_distance'])).sum()\n",
    "            invalid_count += invalid\n",
    "        \n",
    "        if 'passenger_count' in df.columns:\n",
    "            invalid = ((df['passenger_count'].notna()) & \n",
    "                      ((df['passenger_count'] <= 0) | (df['passenger_count'] > CLEANING_CONFIG['max_passenger_count']))).sum()\n",
    "            invalid_count += invalid\n",
    "        \n",
    "        if invalid_count > 0:\n",
    "            stats['actions'].append({\n",
    "                'action': 'DROP_ROWS',\n",
    "                'rows_affected': int(invalid_count),\n",
    "                'rows_affected_pct': round(invalid_count/original_rows*100, 2),\n",
    "                'reason': 'Invalid data (negative values, outliers)'\n",
    "            })\n",
    "    \n",
    "    rows_after = original_rows - sum(a.get('rows_affected', 0) for a in stats['actions'])\n",
    "    cols_after = original_cols - sum(len(a.get('columns', [])) for a in stats['actions'] if a['action'] == 'DROP_COLUMNS')\n",
    "    \n",
    "    stats['final_rows'] = rows_after\n",
    "    stats['final_cols'] = cols_after\n",
    "    stats['retention_rate'] = round(rows_after/original_rows*100, 2) if original_rows > 0 else 0\n",
    "    \n",
    "    return stats\n",
    "\n",
    "print(\"\u2705 Functions for analysis & preview loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "\ud83d\udd0d ANALYZING NYC TRIP DATA (DRY RUN - NO FILES MODIFIED)\n",
      "Directory: /Users/gouravdhama/Documents/bubu/Raw_data/raw_data\n",
      "Found 100 parquet files\n",
      "====================================================================================================\n",
      "\n",
      "[1/100] fhv_tripdata_2015-02.parquet\n",
      "  Rows: 3,053,183 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "  Cleaning preview: 3 actions\n",
      "  After cleaning: 3,053,183 rows (100.0% retained)\n",
      "\n",
      "[2/100] fhv_tripdata_2015-12.parquet\n",
      "  Rows: 8,888,809 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "  Cleaning preview: 3 actions\n",
      "  After cleaning: 8,888,809 rows (100.0% retained)\n",
      "\n",
      "[3/100] fhv_tripdata_2019-02.parquet\n",
      "    \u26a0\ufe0f  Timestamp overflow detected, using safe mode...\n",
      "  Rows: 1,707,650 | Cols: 7\n",
      "  Missing data in 5 columns\n",
      "  Cleaning preview: 2 actions\n",
      "  After cleaning: 1,707,650 rows (100.0% retained)\n",
      "\n",
      "[4/100] fhv_tripdata_2019-12.parquet\n",
      "  Rows: 2,044,196 | Cols: 7\n",
      "  Missing data in 4 columns\n",
      "  Cleaning preview: 3 actions\n",
      "  After cleaning: 2,044,196 rows (100.0% retained)\n",
      "\n",
      "[5/100] fhv_tripdata_2024-01.parquet\n",
      "  Rows: 1,290,116 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "  Cleaning preview: 3 actions\n",
      "  After cleaning: 1,290,116 rows (100.0% retained)\n",
      "\n",
      "[6/100] fhv_tripdata_2024-02.parquet\n",
      "  Rows: 1,176,093 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "  Cleaning preview: 3 actions\n",
      "  After cleaning: 1,176,093 rows (100.0% retained)\n",
      "\n",
      "[7/100] fhv_tripdata_2024-03.parquet\n",
      "  Rows: 1,469,352 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "  Cleaning preview: 3 actions\n",
      "  After cleaning: 1,469,352 rows (100.0% retained)\n",
      "\n",
      "[8/100] fhv_tripdata_2024-04.parquet\n",
      "  Rows: 1,444,626 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "  Cleaning preview: 3 actions\n",
      "  After cleaning: 1,444,626 rows (100.0% retained)\n",
      "\n",
      "[9/100] fhv_tripdata_2024-05.parquet\n",
      "  Rows: 1,352,502 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "  Cleaning preview: 3 actions\n",
      "  After cleaning: 1,352,502 rows (100.0% retained)\n",
      "\n",
      "[10/100] fhv_tripdata_2024-06.parquet\n",
      "  Rows: 1,386,539 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "  Cleaning preview: 3 actions\n",
      "  After cleaning: 1,386,539 rows (100.0% retained)\n",
      "\n",
      "[11/100] fhv_tripdata_2024-07.parquet\n",
      "  Rows: 1,382,739 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "  Cleaning preview: 3 actions\n",
      "  After cleaning: 1,382,739 rows (100.0% retained)\n",
      "\n",
      "[12/100] fhv_tripdata_2024-08.parquet\n",
      "  Rows: 1,484,471 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "  Cleaning preview: 3 actions\n",
      "  After cleaning: 1,484,471 rows (100.0% retained)\n",
      "\n",
      "[13/100] fhv_tripdata_2024-09.parquet\n",
      "  Rows: 1,718,375 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "  Cleaning preview: 3 actions\n",
      "  After cleaning: 1,718,375 rows (100.0% retained)\n",
      "\n",
      "[14/100] fhv_tripdata_2024-10.parquet\n",
      "  Rows: 1,421,231 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "  Cleaning preview: 3 actions\n",
      "  After cleaning: 1,421,231 rows (100.0% retained)\n",
      "\n",
      "[15/100] fhv_tripdata_2024-11.parquet\n",
      "  Rows: 1,591,082 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "  Cleaning preview: 3 actions\n",
      "  After cleaning: 1,591,082 rows (100.0% retained)\n",
      "\n",
      "[16/100] fhv_tripdata_2024-12.parquet\n",
      "  Rows: 1,913,200 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "  Cleaning preview: 3 actions\n",
      "  After cleaning: 1,913,200 rows (100.0% retained)\n",
      "\n",
      "[17/100] fhv_tripdata_2025-01.parquet\n",
      "  Rows: 1,898,108 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "  Cleaning preview: 3 actions\n",
      "  After cleaning: 1,898,108 rows (100.0% retained)\n",
      "\n",
      "[18/100] fhv_tripdata_2025-02.parquet\n",
      "  Rows: 1,578,722 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "  Cleaning preview: 3 actions\n",
      "  After cleaning: 1,578,722 rows (100.0% retained)\n",
      "\n",
      "[19/100] fhv_tripdata_2025-03.parquet\n",
      "  Rows: 2,182,992 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "  Cleaning preview: 3 actions\n",
      "  After cleaning: 2,182,992 rows (100.0% retained)\n",
      "\n",
      "[20/100] fhv_tripdata_2025-04.parquet\n",
      "  Rows: 1,699,478 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "  Cleaning preview: 3 actions\n",
      "  After cleaning: 1,699,478 rows (100.0% retained)\n",
      "\n",
      "[21/100] fhv_tripdata_2025-05.parquet\n",
      "  Rows: 2,210,721 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "  Cleaning preview: 3 actions\n",
      "  After cleaning: 2,210,721 rows (100.0% retained)\n",
      "\n",
      "[22/100] fhv_tripdata_2025-06.parquet\n",
      "  Rows: 2,231,731 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "  Cleaning preview: 3 actions\n",
      "  After cleaning: 2,231,731 rows (100.0% retained)\n",
      "\n",
      "[23/100] fhv_tripdata_2025-07.parquet\n",
      "  Rows: 2,187,536 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "  Cleaning preview: 3 actions\n",
      "  After cleaning: 2,187,536 rows (100.0% retained)\n",
      "\n",
      "[24/100] fhv_tripdata_2025-08.parquet\n",
      "  Rows: 2,256,854 | Cols: 7\n",
      "  Missing data in 3 columns\n",
      "  Cleaning preview: 3 actions\n",
      "  After cleaning: 2,256,854 rows (100.0% retained)\n",
      "\n",
      "[25/100] fhvhv_tripdata_2019-02.parquet\n"
     ]
    }
   ],
   "source": [
    "# ---- Run analysis + preview across all files (DRY RUN) ----\n",
    "\n",
    "raw_path = Path(RAW_DATA_DIR)\n",
    "parquet_files = sorted(raw_path.glob(\"*.parquet\"))\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"\ud83d\udd0d ANALYZING NYC TRIP DATA (DRY RUN - NO FILES MODIFIED)\")\n",
    "print(f\"Directory: {RAW_DATA_DIR}\")\n",
    "print(f\"Found {len(parquet_files)} parquet files\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "all_analysis = []\n",
    "all_cleaning_preview = []\n",
    "by_type = {'fhv': [], 'fhvhv': [], 'green': [], 'yellow': []}\n",
    "\n",
    "for i, file_path in enumerate(parquet_files, 1):\n",
    "    file_name = file_path.name\n",
    "    file_type = get_file_type(file_name)\n",
    "\n",
    "    print(f\"\\n[{i}/{len(parquet_files)}] {file_name}\")\n",
    "    try:\n",
    "        df = safe_read_parquet(file_path)\n",
    "\n",
    "        analysis = analyze_missing_data(df, file_name)\n",
    "        all_analysis.append(analysis)\n",
    "\n",
    "        cleaning_preview = preview_cleaning(df, file_name, file_type)\n",
    "        all_cleaning_preview.append(cleaning_preview)\n",
    "\n",
    "        by_type.setdefault(file_type, []).append({\n",
    "            'analysis': analysis,\n",
    "            'cleaning': cleaning_preview,\n",
    "        })\n",
    "\n",
    "        print(f\"  Rows: {analysis['total_rows']:,} | Cols: {analysis['total_columns']}\")\n",
    "        print(f\"  Missing data in {len(analysis['columns_with_missing'])} columns\")\n",
    "        print(f\"  Cleaning preview: {len(cleaning_preview['actions'])} actions\")\n",
    "        print(f\"  After cleaning: {cleaning_preview['final_rows']:,} rows ({cleaning_preview['retention_rate']}% retained)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  \u274c ERROR: {e}\")\n",
    "\n",
    "# Save results\n",
    "output_file = \"/Users/gouravdhama/Documents/bubu/cleaning_code/analysis_results.json\"\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump({'analysis': all_analysis, 'cleaning_preview': all_cleaning_preview}, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"\ud83d\udcca SUMMARY BY FILE TYPE\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "for file_type, files in by_type.items():\n",
    "    if not files:\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n{file_type.upper()} FILES ({len(files)} files)\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    total_rows_before = sum(f['analysis']['total_rows'] for f in files)\n",
    "    total_rows_after = sum(f['cleaning']['final_rows'] for f in files)\n",
    "    retention = (total_rows_after / total_rows_before * 100) if total_rows_before > 0 else 0\n",
    "\n",
    "    print(f\"  Total rows before: {total_rows_before:,}\")\n",
    "    print(f\"  Total rows after: {total_rows_after:,}\")\n",
    "    print(f\"  Overall retention: {retention:.2f}%\")\n",
    "    print(f\"  Rows to remove: {total_rows_before - total_rows_after:,}\")\n",
    "\n",
    "    all_missing = {}\n",
    "    for f in files:\n",
    "        for col, info in f['analysis']['columns_with_missing'].items():\n",
    "            all_missing.setdefault(col, []).append(info['null_percentage'])\n",
    "\n",
    "    if all_missing:\n",
    "        print(\"\\n  Columns with missing data:\")\n",
    "        for col, percentages in sorted(all_missing.items()):\n",
    "            avg_pct = sum(percentages) / len(percentages)\n",
    "            print(f\"    \u2022 {col}: avg {avg_pct:.1f}% missing (in {len(percentages)} files)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"\ud83d\udcbe RESULTS SAVED\")\n",
    "print(\"=\"*100)\n",
    "print(f\"Detailed analysis: {output_file}\")\n",
    "print(\"\\nDone.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 File loaded successfully!\n",
      "Rows: 1,707,650\n",
      "Columns: ['dispatching_base_num', 'pickup_datetime', 'dropOff_datetime', 'PUlocationID', 'DOlocationID', 'SR_Flag', 'Affiliated_base_number']\n",
      "\n",
      "First few rows:\n",
      "  dispatching_base_num     pickup_datetime    dropOff_datetime  PUlocationID  \\\n",
      "0               B00037 2019-02-01 00:08:44 2019-02-01 00:23:35         264.0   \n",
      "1               B00037 2019-02-01 00:27:51 2019-02-01 00:32:54         264.0   \n",
      "2               B00037 2019-02-01 00:18:30 2019-02-01 00:25:45         264.0   \n",
      "3               B00037 2019-02-01 00:43:15 2019-02-01 00:48:29         264.0   \n",
      "4               B00037 2019-02-01 00:01:45 2019-02-01 00:09:13         264.0   \n",
      "\n",
      "   DOlocationID  SR_Flag Affiliated_base_number  \n",
      "0         265.0      NaN                 B00037  \n",
      "1         265.0      NaN                 B00037  \n",
      "2         265.0      NaN                 B00037  \n",
      "3         265.0      NaN                 B00037  \n",
      "4         265.0      NaN                 B00037  \n",
      "\n",
      "\u2705 Saved to: /Users/gouravdhama/Documents/bubu/cleaned_data/fhv_tripdata_2019-02.parquet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from pathlib import Path\n",
    "\n",
    "# Read the problematic file\n",
    "file_path = \"/Users/gouravdhama/Documents/bubu/Raw_data/raw_data/fhv_tripdata_2019-02.parquet\"\n",
    "\n",
    "# Read with PyArrow to avoid timestamp overflow\n",
    "table = pq.read_table(file_path)\n",
    "df = table.to_pandas(timestamp_as_object=True)\n",
    "\n",
    "# Try to fix timestamp columns\n",
    "for col in df.columns:\n",
    "    if any(x in col.lower() for x in ['time', 'datetime', 'date']):\n",
    "        try:\n",
    "            # Convert back to datetime, coercing invalid timestamps to NaT (Not a Time)\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# Check the result\n",
    "print(f\"\u2705 File loaded successfully!\")\n",
    "print(f\"Rows: {len(df):,}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Optional: Save the fixed version\n",
    "output_path = \"/Users/gouravdhama/Documents/bubu/cleaned_data/fhv_tripdata_2019-02.parquet\"\n",
    "df.to_parquet(output_path, index=False)\n",
    "print(f\"\\n\u2705 Saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Cleaning (Write Cleaned Parquet Files)\n",
    "\n",
    "This section performs the **actual cleaning** and writes cleaned parquet files to:\n",
    "\n",
    "`/Users/gouravdhama/Documents/bubu/cleaned_data`\n",
    "\n",
    "Run this **after** you are happy with the DRY RUN preview above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Cleaning function ready (for execute mode).\n"
     ]
    }
   ],
   "source": [
    "# Cleaning function (applies the same logic as the preview, but actually modifies the data)\n",
    "\n",
    "def clean_dataframe(df, file_name, file_type):\n",
    "    \"\"\"Clean a dataframe according to CLEANING_CONFIG and return cleaned df + log.\"\"\"\n",
    "    df_clean = df.copy()\n",
    "    original_rows = len(df_clean)\n",
    "    original_cols = len(df_clean.columns)\n",
    "\n",
    "    cleaning_log = {\n",
    "        'file_name': file_name,\n",
    "        'file_type': file_type,\n",
    "        'original_rows': original_rows,\n",
    "        'original_cols': original_cols,\n",
    "        'actions': []\n",
    "    }\n",
    "\n",
    "    # 1. Drop 100% empty columns\n",
    "    if CLEANING_CONFIG['drop_completely_empty_columns']:\n",
    "        empty_cols = [col for col in df_clean.columns if df_clean[col].isnull().sum() == original_rows]\n",
    "        if empty_cols:\n",
    "            df_clean = df_clean.drop(columns=empty_cols)\n",
    "            cleaning_log['actions'].append({\n",
    "                'action': 'DROP_COLUMNS',\n",
    "                'columns': empty_cols,\n",
    "                'reason': '100% missing'\n",
    "            })\n",
    "\n",
    "    # 2. Fill location fields with -1 (unknown)\n",
    "    if CLEANING_CONFIG['impute_locations_with_unknown']:\n",
    "        location_cols = [col for col in df_clean.columns if 'location' in col.lower()]\n",
    "        for col in location_cols:\n",
    "            missing = df_clean[col].isnull().sum()\n",
    "            if missing > 0:\n",
    "                df_clean[col] = df_clean[col].fillna(-1)\n",
    "                cleaning_log['actions'].append({\n",
    "                    'action': 'FILL_NULL',\n",
    "                    'column': col,\n",
    "                    'missing_count': int(missing),\n",
    "                    'fill_value': -1\n",
    "                })\n",
    "\n",
    "    # 3. Fill passenger_count with 1 (most common)\n",
    "    if 'passenger_count' in df_clean.columns:\n",
    "        missing = df_clean['passenger_count'].isnull().sum()\n",
    "        if missing > 0:\n",
    "            df_clean['passenger_count'] = df_clean['passenger_count'].fillna(1)\n",
    "            cleaning_log['actions'].append({\n",
    "                'action': 'FILL_NULL',\n",
    "                'column': 'passenger_count',\n",
    "                'missing_count': int(missing),\n",
    "                'fill_value': 1\n",
    "            })\n",
    "\n",
    "    # 4. Fill RatecodeID with 1 (standard rate)\n",
    "    if 'RatecodeID' in df_clean.columns:\n",
    "        missing = df_clean['RatecodeID'].isnull().sum()\n",
    "        if missing > 0:\n",
    "            df_clean['RatecodeID'] = df_clean['RatecodeID'].fillna(1)\n",
    "            cleaning_log['actions'].append({\n",
    "                'action': 'FILL_NULL',\n",
    "                'column': 'RatecodeID',\n",
    "                'missing_count': int(missing),\n",
    "                'fill_value': 1\n",
    "            })\n",
    "\n",
    "    # 5. Fill store_and_fwd_flag with 'N'\n",
    "    if 'store_and_fwd_flag' in df_clean.columns:\n",
    "        missing = df_clean['store_and_fwd_flag'].isnull().sum()\n",
    "        if missing > 0:\n",
    "            df_clean['store_and_fwd_flag'] = df_clean['store_and_fwd_flag'].fillna('N')\n",
    "            cleaning_log['actions'].append({\n",
    "                'action': 'FILL_NULL',\n",
    "                'column': 'store_and_fwd_flag',\n",
    "                'missing_count': int(missing),\n",
    "                'fill_value': 'N'\n",
    "            })\n",
    "\n",
    "    # 6. Fill payment_type with 1 (credit card)\n",
    "    if 'payment_type' in df_clean.columns:\n",
    "        missing = df_clean['payment_type'].isnull().sum()\n",
    "        if missing > 0:\n",
    "            df_clean['payment_type'] = df_clean['payment_type'].fillna(1)\n",
    "            cleaning_log['actions'].append({\n",
    "                'action': 'FILL_NULL',\n",
    "                'column': 'payment_type',\n",
    "                'missing_count': int(missing),\n",
    "                'fill_value': 1\n",
    "            })\n",
    "\n",
    "    # 7. Fill fee-related columns with 0\n",
    "    fee_cols = [col for col in df_clean.columns if any(x in col.lower() for x in ['surcharge', 'fee', 'tax'])]\n",
    "    for col in fee_cols:\n",
    "        missing = df_clean[col].isnull().sum()\n",
    "        if missing > 0:\n",
    "            df_clean[col] = df_clean[col].fillna(0)\n",
    "            cleaning_log['actions'].append({\n",
    "                'action': 'FILL_NULL',\n",
    "                'column': col,\n",
    "                'missing_count': int(missing),\n",
    "                'fill_value': 0\n",
    "            })\n",
    "\n",
    "    # 8. Fill trip_type with mode\n",
    "    if 'trip_type' in df_clean.columns:\n",
    "        missing = df_clean['trip_type'].isnull().sum()\n",
    "        if missing > 0:\n",
    "            mode_val = df_clean['trip_type'].mode()\n",
    "            mode_val = mode_val[0] if len(mode_val) > 0 else 1\n",
    "            df_clean['trip_type'] = df_clean['trip_type'].fillna(mode_val)\n",
    "            cleaning_log['actions'].append({\n",
    "                'action': 'FILL_NULL',\n",
    "                'column': 'trip_type',\n",
    "                'missing_count': int(missing),\n",
    "                'fill_value': int(mode_val)\n",
    "            })\n",
    "\n",
    "    # 9. Remove invalid records (negative/out-of-range values)\n",
    "    if CLEANING_CONFIG['remove_invalid_records']:\n",
    "        rows_before = len(df_clean)\n",
    "\n",
    "        if 'fare_amount' in df_clean.columns:\n",
    "            df_clean = df_clean[\n",
    "                (df_clean['fare_amount'] >= 0) &\n",
    "                (df_clean['fare_amount'] <= CLEANING_CONFIG['max_fare_amount'])\n",
    "            ]\n",
    "\n",
    "        if 'trip_distance' in df_clean.columns:\n",
    "            df_clean = df_clean[\n",
    "                (df_clean['trip_distance'] >= 0) &\n",
    "                (df_clean['trip_distance'] <= CLEANING_CONFIG['max_trip_distance'])\n",
    "            ]\n",
    "\n",
    "        if 'passenger_count' in df_clean.columns:\n",
    "            df_clean = df_clean[\n",
    "                (df_clean['passenger_count'] > 0) &\n",
    "                (df_clean['passenger_count'] <= CLEANING_CONFIG['max_passenger_count'])\n",
    "            ]\n",
    "\n",
    "        rows_removed = rows_before - len(df_clean)\n",
    "        if rows_removed > 0:\n",
    "            cleaning_log['actions'].append({\n",
    "                'action': 'DROP_ROWS',\n",
    "                'rows_affected': int(rows_removed),\n",
    "                'reason': 'Invalid data (negative values / outliers)'\n",
    "            })\n",
    "\n",
    "    # Final stats\n",
    "    cleaning_log['final_rows'] = len(df_clean)\n",
    "    cleaning_log['final_cols'] = len(df_clean.columns)\n",
    "    cleaning_log['retention_rate'] = round(len(df_clean) / original_rows * 100, 2) if original_rows > 0 else 0\n",
    "\n",
    "    return df_clean, cleaning_log\n",
    "\n",
    "print(\"\u2705 Cleaning function ready (for execute mode).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}